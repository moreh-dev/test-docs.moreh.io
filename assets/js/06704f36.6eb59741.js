"use strict";(self.webpackChunkmif_docs=self.webpackChunkmif_docs||[]).push([[445],{608(e,t,s){s.r(t),s.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"getting_started/overview","title":"Overview","description":"MoAI Inference Framework is designed to enable efficient and automated distributed inference on cluster systems and Kubernetes environments. It supports a wide range of distributed inference techniques &mdash; such as prefill-decode disaggregation, expert parallelism, and prefix-cache-aware routing. Leveraging its unique cost model, it automatically identifies, applies, and dynamically adjusts the optimal way to utilize various accelerators so as to meet the defined service level objectives (SLOs). All of these capabilities are seamlessly integrated not only for NVIDIA GPUs but also for other accelerators, especially AMD GPUs.","source":"@site/versioned_docs/version-v0.0.0/getting_started/overview.md","sourceDirName":"getting_started","slug":"/getting_started/overview","permalink":"/getting_started/overview","draft":false,"unlisted":false,"tags":[],"version":"v0.0.0","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Overview"},"sidebar":"docs","previous":{"title":"Getting Started","permalink":"/getting-started"},"next":{"title":"Prerequisites","permalink":"/getting_started/prerequisites"}}');var n=s(4848),i=s(8453);const o={sidebar_position:1,title:"Overview"},a=void 0,l={},d=[{value:"Components",id:"components",level:2},{value:"Supported accelerators",id:"supported-accelerators",level:2},{value:"Supported models",id:"supported-models",level:2}];function c(e){const t={a:"a",h2:"h2",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.p,{children:"MoAI Inference Framework is designed to enable efficient and automated distributed inference on cluster systems and Kubernetes environments. It supports a wide range of distributed inference techniques \u2014 such as prefill-decode disaggregation, expert parallelism, and prefix-cache-aware routing. Leveraging its unique cost model, it automatically identifies, applies, and dynamically adjusts the optimal way to utilize various accelerators so as to meet the defined service level objectives (SLOs). All of these capabilities are seamlessly integrated not only for NVIDIA GPUs but also for other accelerators, especially AMD GPUs."}),"\n",(0,n.jsx)(t.h2,{id:"components",children:"Components"}),"\n",(0,n.jsx)(t.p,{children:"MoAI Inference Framework consists of four main components:"}),"\n",(0,n.jsxs)(t.ol,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:'"Heimdall" scheduler'}),": responsible for deciding optimal routing and scheduling across different inference services. It is based on the ",(0,n.jsx)(t.a,{href:"https://gateway-api-inference-extension.sigs.k8s.io/",children:"Gateway API Inference Extension"}),", allowing various gateway controllers such as ",(0,n.jsx)(t.a,{href:"https://istio.io/",children:"Istio"})," or ",(0,n.jsx)(t.a,{href:"https://kgateway.dev/",children:"Kgateway"})," to be used as frontends."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:'"Norns" autoscaler'}),": responsible for determining the optimal amount of GPU resources for each model or disaggregated model."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:'"Odin" inference service'}),": runs individual inference instances on Kubernetes at scale."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Moreh vLLM"}),": an optimized version of vLLM designed to deliver superior inference performance on AMD GPUs. It supports the same models and features as the original vLLM, while applying end-to-end optimization across GPU kernels, libraries, model implementations, and individual parallelization/disaggregation techniques specifically tuned for AMD GPU architectures. ",(0,n.jsx)(t.a,{href:"https://moreh.io/moreh-vllm/",children:"(Learn more)"})]}),"\n"]}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.h2,{id:"supported-accelerators",children:"Supported accelerators"}),"\n",(0,n.jsx)(t.p,{children:"MoAI Inference Framework is compatible with NVIDIA GPUs, AMD GPUs, Tenstorrent AI accelerators, and various other devices supported by vLLM. Its official support currently covers the following accelerators:"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Vendor"}),(0,n.jsx)(t.th,{children:"Models"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"AMD"}),(0,n.jsx)(t.td,{children:"MI250, MI250X, MI300X, MI308X, MI325X, and MI355X"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"NVIDIA"}),(0,n.jsx)(t.td,{children:"A100, H100, H200, H20, and B200"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Tenstorrent"}),(0,n.jsx)(t.td,{children:"Wormhole and Blackhole"})]})]})]}),"\n",(0,n.jsx)(t.p,{children:"Supported features and compatibility may vary across accelerators. Please contact Moreh for detailed information."}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.h2,{id:"supported-models",children:"Supported models"}),"\n",(0,n.jsxs)(t.p,{children:["MoAI Inference Framework natively supports all open-source LLMs available in ",(0,n.jsx)(t.a,{href:"https://docs.vllm.ai/en/latest/models/supported_models.html",children:"vLLM"}),", including (but not limited to) Llama 2/3/4, DeepSeek V3/3.1/3.2/R1, GPT-OSS, Qwen 1.5/2/2.5/3, Step3, Baichuan2, Gemma2, and Mistral."]})]})}function p(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},8453(e,t,s){s.d(t,{R:()=>o,x:()=>a});var r=s(6540);const n={},i=r.createContext(n);function o(e){const t=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),r.createElement(i.Provider,{value:t},e.children)}}}]);