"use strict";(self.webpackChunkmif_docs=self.webpackChunkmif_docs||[]).push([[9089],{168(e,n,r){r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>m,frontMatter:()=>l,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"features/preset","title":"Presets","description":"The MoAI Inference Framework provides a set of pre-configured InferenceServiceTemplates, known as presets. These presets encapsulate standard configurations for various models and hardware setups, simplifying the deployment of inference services.","source":"@site/docs/features/preset.mdx","sourceDirName":"features","slug":"/features/preset","permalink":"/dev/features/preset","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"sidebar_label":"Presets","title":"Presets"},"sidebar":"docs","previous":{"title":"Quickstart","permalink":"/dev/getting-started/quickstart"},"next":{"title":"HF Model Management (PV)","permalink":"/dev/operations/hf-model-management-with-pv"}}');var i=r(4848),a=r(8453);const l={sidebar_position:1,sidebar_label:"Presets",title:"Presets"},t=void 0,c={},o=[{value:"Using a complete preset",id:"using-a-complete-preset",level:2},{value:"Overriding preset configuration",id:"overriding-preset-configuration",level:2},{value:"Using a runtime-base",id:"using-a-runtime-base",level:2},{value:"Creating a reusable preset",id:"creating-a-reusable-preset",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["The MoAI Inference Framework provides a set of pre-configured ",(0,i.jsx)(n.code,{children:"InferenceServiceTemplate"}),"s, known as presets. These presets encapsulate standard configurations for various models and hardware setups, simplifying the deployment of inference services."]}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:["This feature requires cluster-level configuration. Please confirm with your cluster administrator that the ",(0,i.jsx)(n.a,{href:"/dev/getting-started/prerequisites",children:"prerequisites"})," have been met."]})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"using-a-complete-preset",children:"Using a complete preset"}),"\n",(0,i.jsxs)(n.p,{children:["To use a preset, you reference it in the ",(0,i.jsx)(n.code,{children:"spec.templateRefs"})," field of your ",(0,i.jsx)(n.code,{children:"InferenceService"}),". You can specify multiple templates; they will be merged in the order listed, with later templates overriding earlier ones."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"templateRefs"})," searches for templates in the following order:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["The namespace where the ",(0,i.jsx)(n.code,{children:"InferenceService"})," is created."]}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"mif"})," namespace, where the Odin operator is typically installed."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"You can view the available presets in your cluster using the following command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"kubectl get inferenceservicetemplate -n mif -l mif.moreh.io/template.type=preset\n"})}),"\n",(0,i.jsxs)(n.p,{children:["For example, to deploy a vLLM service for the Llama 3.2 1B Instruct model on AMD MI250 GPUs, you can combine the base ",(0,i.jsx)(n.code,{children:"vllm"})," template with the model-specific ",(0,i.jsx)(n.code,{children:"vllm-meta-llama-llama-3.2-1b-instruct-amd-mi250-tp2"})," template:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",metastring:"{18}",children:"apiVersion: odin.moreh.io/v1alpha1\nkind: InferenceService\nmetadata:\n  name: vllm-llama3-1b-instruct-tp2\nspec:\n  replicas: 2\n  inferencePoolRefs:\n    - name: heimdall\n  templateRefs:\n    - name: vllm\n    - name: vllm-meta-llama-llama-3.2-1b-instruct-amd-mi250-tp2\n  template:\n    spec:\n      containers:\n        - name: main\n          env:\n            - name: HF_TOKEN\n              value: <huggingFaceToken>\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"overriding-preset-configuration",children:"Overriding preset configuration"}),"\n",(0,i.jsxs)(n.p,{children:["You can customize or override the configuration defined in the presets by providing a ",(0,i.jsx)(n.code,{children:"spec.template"})," in your ",(0,i.jsx)(n.code,{children:"InferenceService"}),". The fields in ",(0,i.jsx)(n.code,{children:"spec.template"})," take precedence over those in the referenced templates."]}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:["When using certain runtime-bases (e.g., ",(0,i.jsx)(n.code,{children:"vllm-decode-dp"}),"), ",(0,i.jsx)(n.code,{children:"workerTemplate"})," is used instead of ",(0,i.jsx)(n.code,{children:"template"})," to define the pod configuration. Therefore, you must use ",(0,i.jsx)(n.code,{children:"spec.workerTemplate"})," instead of ",(0,i.jsx)(n.code,{children:"spec.template"})," when overriding values."]})}),"\n",(0,i.jsxs)(n.p,{children:["To identify which values to override, you can inspect the contents of the ",(0,i.jsx)(n.code,{children:"InferenceServiceTemplate"})," resources. For example, to check the runtime-base configuration (",(0,i.jsx)(n.code,{children:"vllm"}),") and the model-specific configuration (",(0,i.jsx)(n.code,{children:"vllm-meta-llama-llama-3.2-1b-instruct-amd-mi250-tp2"}),"):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"kubectl get inferenceservicetemplate vllm -n mif -o yaml\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"kubectl get inferenceservicetemplate -n mif \\\n    vllm-meta-llama-llama-3.2-1b-instruct-amd-mi250-tp2 -o yaml\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",metastring:'title="Expected output"',children:'apiVersion: odin.moreh.io/v1alpha1\nkind: InferenceServiceTemplate\nmetadata:\n  name: vllm-meta-llama-llama-3.2-1b-instruct-amd-mi250-tp2\n  namespace: mif\n  # ... (other fields)\nspec:\n  # ... (other fields)\n  template:\n    spec:\n      # ... (other fields)\n      containers:\n        # ... (other fields)\n        - name: main\n          # ... (other fields)\n          env:\n            # ... (other fields)\n            - name: ISVC_MODEL_NAME\n              value: meta-llama/Llama-3.2-1B-Instruct\n            - name: ISVC_EXTRA_ARGS\n              value: --disable-uvicorn-access-log --no-enable-log-requests\n                --quantization None --max-model-len 8192 --max-num-batched-tokens 32768\n                --no-enable-prefix-caching --kv-transfer-config \'{"kv_connector":"NixlConnector","kv_role":"kv_both"}\'\n'})}),"\n",(0,i.jsxs)(n.p,{children:["This command reveals the default configuration, including containers, environment variables, and resource limits. You can then reference this output to determine the correct structure and values to include in your ",(0,i.jsx)(n.code,{children:"spec.template"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["A common use case is modifying the model execution arguments. For instance, the ",(0,i.jsx)(n.code,{children:"vllm-meta-llama-llama-3.2-1b-instruct-amd-mi250-tp2"})," preset disables prefix caching by default (",(0,i.jsx)(n.code,{children:"--no-enable-prefix-caching"}),") in ",(0,i.jsx)(n.code,{children:"ISVC_EXTRA_ARGS"}),". You can enable it by overriding the environment variable in your ",(0,i.jsx)(n.code,{children:"InferenceService"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",metastring:"{22,25}",children:'apiVersion: odin.moreh.io/v1alpha1\nkind: InferenceService\nmetadata:\n  name: vllm-llama3-1b-instruct-tp2\nspec:\n  # ... (other fields)\n  templateRefs:\n    - name: vllm\n    - name: vllm-meta-llama-llama-3.2-1b-instruct-amd-mi250-tp2\n  template:\n    spec:\n      containers:\n        - name: main\n          env:\n            - name: ISVC_EXTRA_ARGS\n              value: >-\n                --disable-uvicorn-access-log\n                --no-enable-log-requests\n                --quantization None\n                --max-model-len 8192\n                --max-num-batched-tokens 32768\n                --enable-prefix-caching\n                --kv-transfer-config \'{"kv_connector":"NixlConnector","kv_role":"kv_both"}\'\n            - name: HF_TOKEN\n              value: <huggingFaceToken>\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"using-a-runtime-base",children:"Using a runtime-base"}),"\n",(0,i.jsxs)(n.p,{children:["If a preset for your specific model or hardware configuration is not available, you can use only the runtime-base (e.g., ",(0,i.jsx)(n.code,{children:"vllm-decode-dp"}),") and manually specify environment variables, resources, scheduler requirements (node selector and tolerations), etc."]}),"\n",(0,i.jsx)(n.p,{children:"You can view the available runtime-bases in your cluster using the following command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"kubectl get inferenceservicetemplate -n mif -l mif.moreh.io/template.type=runtime-base\n"})}),"\n",(0,i.jsx)(n.p,{children:"To identify which values to override, you can inspect the contents of the runtime-bases:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"kubectl get inferenceservicetemplate -n mif vllm-decode-dp -o yaml\n"})}),"\n",(0,i.jsx)(n.p,{children:"The following environment variables are frequently overridden to customize the behavior of the runtime-base."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"ISVC_MODEL_NAME"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["The name of the model to serve (e.g., ",(0,i.jsx)(n.code,{children:"meta-llama/Llama-3.2-1B-Instruct"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"ISVC_MODEL_PATH"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The Hugging Face model ID or the local path to the model weights."}),"\n",(0,i.jsxs)(n.li,{children:["Defaults to ",(0,i.jsx)(n.code,{children:"$ISVC_MODEL_NAME"})," to use the Hugging Face model ID. Set this only when using a locally downloaded model."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"ISVC_EXTRA_ARGS"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Additional arguments passed to the inference engine (e.g., vLLM). Since parallelism configurations are handled by the runtime-base, use this variable to add other model-specific arguments."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"ISVC_PRE_PROCESS_SCRIPT"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A script to run before the inference server starts."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["For example, the following ",(0,i.jsx)(n.code,{children:"InferenceService"})," uses ",(0,i.jsx)(n.code,{children:"vllm-decode-dp"})," as a runtime-base and serves ",(0,i.jsx)(n.code,{children:"meta-llama/Llama-3.2-1B-Instruct"}),"."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",metastring:"{28}",children:"apiVersion: odin.moreh.io/v1alpha1\nkind: InferenceService\nmetadata:\n  name: my-custom-model\nspec:\n  replicas: 1\n  inferencePoolRefs:\n    - name: heimdall\n  templateRefs:\n    - name: vllm-decode-dp # runtime-base only\n  parallelism:\n    data: 2\n    tensor: 1\n  workerTemplate: # Use workerTemplate for vllm-decode-dp\n    spec:\n      containers:\n        - name: main\n          env:\n            - name: ISVC_MODEL_NAME\n              value: meta-llama/Llama-3.2-1B-Instruct\n            - name: ISVC_EXTRA_ARGS\n              value: >-\n                --disable-uvicorn-access-log\n                --no-enable-log-requests\n                --quantization None\n                --max-model-len 4096\n            - name: HF_TOKEN\n              value: <huggingFaceToken>\n          resources:\n            limits:\n              amd.com/gpu: 1\n            requests:\n              amd.com/gpu: 1\n      nodeSelector:\n        moai.moreh.io/accelerator.vendor: amd\n        moai.moreh.io/accelerator.model: mi300x\n      tolerations:\n        - key: amd.com/gpu\n          operator: Exists\n          effect: NoSchedule\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"creating-a-reusable-preset",children:"Creating a reusable preset"}),"\n",(0,i.jsxs)(n.p,{children:["You can turn the configuration above into a reusable preset (",(0,i.jsx)(n.code,{children:"InferenceServiceTemplate"}),") by removing the ",(0,i.jsx)(n.code,{children:"replicas"}),", ",(0,i.jsx)(n.code,{children:"inferencePoolRefs"}),", and ",(0,i.jsx)(n.code,{children:"templateRefs"})," fields and changing the ",(0,i.jsx)(n.code,{children:"kind"})," to ",(0,i.jsx)(n.code,{children:"InferenceServiceTemplate"}),". Also, remove the configurations that users need to provide in the ",(0,i.jsx)(n.code,{children:"InferenceService"})," (e.g., ",(0,i.jsx)(n.code,{children:"HF_TOKEN"}),")."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"For example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",metastring:'{10} title="custom-prefill-dp16ep.yaml"',children:"apiVersion: odin.moreh.io/v1alpha1\nkind: InferenceServiceTemplate\nmetadata:\n  name: custom-prefill-dp16ep\nspec:\n  parallelism:\n    data: 16\n    dataLocal: 8\n    expert: true\n  workerTemplate: # Use workerTemplate for vllm-prefill-dp runtime-base.\n    spec:\n      containers:\n        - name: main\n          env:\n            - name: ISVC_MODEL_NAME\n              value: deepseek-ai/DeepSeek-R1\n            - name: ISVC_EXTRA_ARGS\n              value: >-\n                --disable-uvicorn-access-log\n                --no-enable-log-requests\n                # ... (other args)\n            # ... (other envs)\n          resources:\n            limits:\n              amd.com/gpu: '8'\n            requests:\n              amd.com/gpu: '8'\n      nodeSelector:\n        moai.moreh.io/accelerator.vendor: amd\n        moai.moreh.io/accelerator.model: mi300x\n      tolerations:\n        - key: amd.com/gpu\n          operator: Exists\n          effect: NoSchedule\n"})}),"\n",(0,i.jsx)(n.p,{children:"Register this custom preset to your namespace:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"kubectl apply -n <yourNamespace> -f custom-prefill-dp16ep.yaml\n"})}),"\n",(0,i.jsxs)(n.p,{children:["To use this custom preset, you can reference it alongside the runtime-base in your ",(0,i.jsx)(n.code,{children:"InferenceService"}),"."]}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"InferenceServiceTemplate"}),"s installed in a non-system namespace are available only within that namespace."]})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",metastring:'title="custom-prefill.yaml"',children:"apiVersion: odin.moreh.io/v1alpha1\nkind: InferenceService\n# ... (other fields)\nspec:\n  # ... (other fields)\n  templateRefs:\n    - name: vllm-prefill-dp\n    - name: custom-prefill-dp16ep\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"kubectl apply -n <yourNamespace> -f custom-prefill.yaml\n"})})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,r){r.d(n,{R:()=>l,x:()=>t});var s=r(6540);const i={},a=s.createContext(i);function l(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);