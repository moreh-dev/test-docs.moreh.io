"use strict";(self.webpackChunkmif_docs=self.webpackChunkmif_docs||[]).push([[3409],{3134(e,n,r){r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>m,frontMatter:()=>t,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"features/preset","title":"Presets","description":"The MoAI Inference Framework provides a set of pre-configured InferenceServiceTemplates, known as presets. These presets encapsulate standard configurations for various models and hardware setups, simplifying the deployment of inference services.","source":"@site/versioned_docs/version-v0.0.0/features/preset.md","sourceDirName":"features","slug":"/features/preset","permalink":"/features/preset","draft":false,"unlisted":false,"tags":[],"version":"v0.0.0","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Presets"},"sidebar":"docs","previous":{"title":"Features","permalink":"/features"},"next":{"title":"Prefill-decode disaggregation","permalink":"/features/prefill_decode_disaggregation"}}');var s=r(4848),i=r(8453);const t={sidebar_position:1,title:"Presets"},l="Presets",o={},c=[{value:"Installation",id:"installation",level:2},{value:"Using a complete preset",id:"using-a-complete-preset",level:2},{value:"Overriding preset configuration",id:"overriding-preset-configuration",level:2},{value:"Using a runtime base preset",id:"using-a-runtime-base-preset",level:2},{value:"Creating a reusable preset",id:"creating-a-reusable-preset",level:2}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"presets",children:"Presets"})}),"\n",(0,s.jsxs)(n.p,{children:["The MoAI Inference Framework provides a set of pre-configured ",(0,s.jsx)(n.code,{children:"InferenceServiceTemplate"}),"s, known as presets. These presets encapsulate standard configurations for various models and hardware setups, simplifying the deployment of inference services."]}),"\n",(0,s.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,s.jsxs)(n.p,{children:["The presets are installed via the ",(0,s.jsx)(n.code,{children:"moai-inference-preset"})," Helm chart."]}),"\n",(0,s.jsx)(n.p,{children:"First, add the Moreh Helm chart repository:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"helm repo add moreh https://moreh-dev.github.io/helm-charts\nhelm repo update moreh\n"})}),"\n",(0,s.jsx)(n.p,{children:"You can check the available versions of the preset chart:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"helm search repo moreh/moai-inference-preset -l\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Then, install the chart. This will create ",(0,s.jsx)(n.code,{children:"InferenceServiceTemplate"})," resources in your cluster."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"helm upgrade -i moai-inference-preset moreh/moai-inference-preset \\\n    --version v0.3.0 \\\n    -n mif\n"})}),"\n",(0,s.jsx)(n.p,{children:"You can view the available presets in your cluster using the following command:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"kubectl get inferenceservicetemplates -n mif\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"using-a-complete-preset",children:"Using a complete preset"}),"\n",(0,s.jsxs)(n.p,{children:["To use a preset, you reference it in the ",(0,s.jsx)(n.code,{children:"spec.templateRefs"})," field of your ",(0,s.jsx)(n.code,{children:"InferenceService"}),". You can specify multiple templates; they will be merged in the order listed, with later templates overriding earlier ones."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"templateRefs"})," searches for templates in the following order:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["The namespace where the ",(0,s.jsx)(n.code,{children:"InferenceService"})," is created."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"mif"})," namespace, where the Odin operator is typically installed."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["For example, to deploy a vLLM service for the Llama 3.2 1B Instruct model on AMD MI250 GPUs, you can combine the base ",(0,s.jsx)(n.code,{children:"vllm"})," template with the model-specific ",(0,s.jsx)(n.code,{children:"vllm-meta-llama-llama-3.2-1b-instruct-amd-mi250-tp2"})," template:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:"{20}",children:"apiVersion: odin.moreh.io/v1alpha1\nkind: InferenceService\nmetadata:\n  name: vllm-llama3-1b-instruct-tp2\nspec:\n  replicas: 2\n  inferencePoolRefs:\n    - name: heimdall\n  templateRefs:\n    - name: vllm\n    - name: vllm-meta-llama-llama-3.2-1b-instruct-amd-mi250-tp2\n  parallelism:\n    tensor: 2\n  template:\n    spec:\n      containers:\n        - name: main\n          env:\n            - name: HF_TOKEN\n              value: <huggingFaceToken>\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"overriding-preset-configuration",children:"Overriding preset configuration"}),"\n",(0,s.jsxs)(n.p,{children:["You can customize or override the configuration defined in the presets by providing a ",(0,s.jsx)(n.code,{children:"spec.template"})," in your ",(0,s.jsx)(n.code,{children:"InferenceService"}),". The fields in ",(0,s.jsx)(n.code,{children:"spec.template"})," take precedence over those in the referenced templates."]}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["When using certain runtime bases (e.g., ",(0,s.jsx)(n.code,{children:"vllm-decode-dp"}),"), ",(0,s.jsx)(n.code,{children:"workerTemplate"})," is used instead of ",(0,s.jsx)(n.code,{children:"template"})," to define the pod configuration. Therefore, you must use ",(0,s.jsx)(n.code,{children:"spec.workerTemplate"})," instead of ",(0,s.jsx)(n.code,{children:"spec.template"})," when overriding values."]})}),"\n",(0,s.jsxs)(n.p,{children:["To identify which values to override, you can inspect the contents of the ",(0,s.jsx)(n.code,{children:"InferenceServiceTemplate"})," resources. For example, to check the runtime base configuration (",(0,s.jsx)(n.code,{children:"vllm"}),") and the model-specific configuration (",(0,s.jsx)(n.code,{children:"vllm-meta-llama-llama-3.2-1b-instruct-amd-mi250-tp2"}),"):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"kubectl get inferenceservicetemplate vllm -n mif -o yaml\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"kubectl get inferenceservicetemplate vllm-meta-llama-llama-3.2-1b-instruct-amd-mi250-tp2 -n mif -o yaml\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",metastring:"Expected output",children:'apiVersion: odin.moreh.io/v1alpha1\nkind: InferenceServiceTemplate\nmetadata:\n  name: vllm-meta-llama-llama-3.2-1b-instruct-amd-mi250-tp2\n  namespace: mif\n  # ... (other fields)\nspec:\n  # ... (other fields)\n  template:\n    spec:\n      # ... (other fields)\n      containers:\n        # ... (other fields)\n        - name: main\n          # ... (other fields)\n          env:\n            # ... (other fields)\n            - name: ISVC_MODEL_NAME\n              value: meta-llama/Llama-3.2-1B-Instruct\n            - name: ISVC_EXTRA_ARGS\n              value: $(ISVC_MODEL_NAME) --disable-uvicorn-access-log --no-enable-log-requests\n                --quantization None --max-model-len 8192 --max-num-batched-tokens 32768\n                --no-enable-prefix-caching --kv-transfer-config \'{"kv_connector":"NixlConnector","kv_role":"kv_both"}\'\n'})}),"\n",(0,s.jsxs)(n.p,{children:["This command reveals the default configuration, including containers, environment variables, and resource limits. You can then reference this output to determine the correct structure and values to include in your ",(0,s.jsx)(n.code,{children:"spec.template"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["A common use case is modifying the model execution arguments. For instance, the ",(0,s.jsx)(n.code,{children:"vllm-meta-llama-llama-3.2-1b-instruct-amd-mi250-tp2"})," preset disables prefix caching by default (",(0,s.jsx)(n.code,{children:"--no-enable-prefix-caching"}),") in ",(0,s.jsx)(n.code,{children:"ISVC_EXTRA_ARGS"}),". You can enable it by overriding the environment variable in your ",(0,s.jsx)(n.code,{children:"InferenceService"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:"{23,26}",children:'apiVersion: odin.moreh.io/v1alpha1\nkind: InferenceService\nmetadata:\n  name: vllm-llama3-1b-instruct-tp2\nspec:\n  # ... (other fields)\n  templateRefs:\n    - name: vllm\n    - name: vllm-meta-llama-llama-3.2-1b-instruct-amd-mi250-tp2\n  template:\n    spec:\n      containers:\n        - name: main\n          env:\n            - name: ISVC_EXTRA_ARGS\n              value: >-\n                $(ISVC_MODEL_NAME)\n                --disable-uvicorn-access-log\n                --no-enable-log-requests\n                --quantization None\n                --max-model-len 8192\n                --max-num-batched-tokens 32768\n                --enable-prefix-caching\n                --kv-transfer-config \'{"kv_connector":"NixlConnector","kv_role":"kv_both"}\'\n            - name: HF_TOKEN\n              value: <huggingFaceToken>\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"using-a-runtime-base-preset",children:"Using a runtime base preset"}),"\n",(0,s.jsxs)(n.p,{children:["If a preset for your specific model or hardware configuration is not available, you can use only the runtime base preset (e.g., ",(0,s.jsx)(n.code,{children:"vllm-decode-dp"}),") and manually define the model-specific configurations in ",(0,s.jsx)(n.code,{children:"spec.workerTemplate"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"In this case, you need to manually specify the model name, extra arguments, resources, and scheduler requirements (node selector and tolerations)."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:"{29}",children:"apiVersion: odin.moreh.io/v1alpha1\nkind: InferenceService\nmetadata:\n  name: my-custom-model\nspec:\n  replicas: 1\n  inferencePoolRefs:\n    - name: heimdall\n  templateRefs:\n    - name: vllm-decode-dp # Runtime base only\n  parallelism:\n    data: 2\n    tensor: 1\n  workerTemplate: # Use workerTemplate for vllm-decode-dp\n    spec:\n      containers:\n        - name: main\n          env:\n            - name: ISVC_MODEL_NAME\n              value: meta-llama/Llama-3.2-1B-Instruct\n            - name: ISVC_EXTRA_ARGS\n              value: >-\n                $(ISVC_MODEL_NAME)\n                --disable-uvicorn-access-log\n                --no-enable-log-requests\n                --quantization None\n                --max-model-len 4096\n            - name: HF_TOKEN\n              value: <huggingFaceToken>\n          resources:\n            limits:\n              amd.com/gpu: 1\n            requests:\n              amd.com/gpu: 1\n      nodeSelector:\n        moai.moreh.io/accelerator.vendor: amd\n        moai.moreh.io/accelerator.model: mi300x\n      tolerations:\n        - key: amd.com/gpu\n          operator: Exists\n          effect: NoSchedule\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"creating-a-reusable-preset",children:"Creating a reusable preset"}),"\n",(0,s.jsxs)(n.p,{children:["You can turn the configuration above into a reusable preset (",(0,s.jsx)(n.code,{children:"InferenceServiceTemplate"}),") by removing the ",(0,s.jsx)(n.code,{children:"replicas"}),", ",(0,s.jsx)(n.code,{children:"inferencePoolRefs"}),", ",(0,s.jsx)(n.code,{children:"templateRefs"}),", and ",(0,s.jsx)(n.code,{children:"parallelism"})," fields and changing the ",(0,s.jsx)(n.code,{children:"kind"})," to ",(0,s.jsx)(n.code,{children:"InferenceServiceTemplate"}),". Also, remove the configurations that users need to provide in the ",(0,s.jsx)(n.code,{children:"InferenceService"})," (e.g., ",(0,s.jsx)(n.code,{children:"HF_TOKEN"}),")."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"apiVersion: odin.moreh.io/v1alpha1\nkind: InferenceServiceTemplate\nmetadata:\n  name: my-custom-preset\nspec:\n  workerTemplate:\n    spec:\n      containers:\n        - name: main\n          env:\n            - name: ISVC_MODEL_NAME\n              value: meta-llama/Llama-3.2-1B-Instruct\n            - name: ISVC_EXTRA_ARGS\n              value: >-\n                $(ISVC_MODEL_NAME)\n                --disable-uvicorn-access-log\n                --no-enable-log-requests\n                --quantization None\n                --max-model-len 4096\n          resources:\n            limits:\n              amd.com/gpu: 1\n            requests:\n              amd.com/gpu: 1\n      nodeSelector:\n        moai.moreh.io/accelerator.vendor: amd\n        moai.moreh.io/accelerator.model: mi300x\n      tolerations:\n        - key: amd.com/gpu\n          operator: Exists\n          effect: NoSchedule\n"})})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,r){r.d(n,{R:()=>t,x:()=>l});var a=r(6540);const s={},i=a.createContext(s);function t(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);