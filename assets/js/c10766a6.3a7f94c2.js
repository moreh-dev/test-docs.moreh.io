"use strict";(self.webpackChunkmif_docs=self.webpackChunkmif_docs||[]).push([[653],{9239(e,n,r){r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>m,frontMatter:()=>l,metadata:()=>s,toc:()=>u});const s=JSON.parse('{"id":"best_practices/resource_allocation","title":"Resource allocation","description":"This document describes how to allocate resources (accelerators and NICs) to your inference containers, select specific nodes using node selectors and node affinity, and handle taints. When an InferenceService is created, it generates a Deployment or a LeaderWorkerSet, which ultimately results in the creation of Pods. Therefore, placing a Pod is synonymous with placing an inference container in this context.","source":"@site/versioned_docs/version-v0.0.0/best_practices/resource_allocation.md","sourceDirName":"best_practices","slug":"/best_practices/resource_allocation","permalink":"/best_practices/resource_allocation","draft":false,"unlisted":false,"tags":[],"version":"v0.0.0","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Resource allocation"},"sidebar":"docs","previous":{"title":"Best practices","permalink":"/best-practices"},"next":{"title":"Hugging Face model management with persistent volume","permalink":"/best_practices/hf_model_management_with_pv"}}');var a=r(4848),o=r(8453),t=r(9489),i=r(7227);const l={sidebar_position:1,title:"Resource allocation"},c="Resource allocation",d={},u=[{value:"Allocation examples",id:"allocation-examples",level:2},{value:"Resource requests and limits",id:"resource-requests-and-limits",level:2},{value:"Node selection",id:"node-selection",level:2},{value:"Accelerator labels",id:"accelerator-labels",level:3},{value:"Example: Selecting MI300X nodes",id:"example-selecting-mi300x-nodes",level:3},{value:"Taints and tolerations",id:"taints-and-tolerations",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"resource-allocation",children:"Resource allocation"})}),"\n",(0,a.jsxs)(n.p,{children:["This document describes how to allocate resources (accelerators and NICs) to your inference containers, select specific nodes using ",(0,a.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector",children:"node selectors"})," and ",(0,a.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity",children:"node affinity"}),", and handle ",(0,a.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/",children:"taints"}),". When an InferenceService is created, it generates a Deployment or a LeaderWorkerSet, which ultimately results in the creation of Pods. Therefore, placing a Pod is synonymous with placing an inference container in this context."]}),"\n",(0,a.jsxs)(n.p,{children:["Before starting this guide, make sure to install all ",(0,a.jsx)(n.a,{href:"/getting_started/prerequisites",children:"prerequisites"}),"."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"allocation-examples",children:"Allocation examples"}),"\n",(0,a.jsxs)(n.p,{children:["Before guiding you through individual settings, let's look at a complete example of allocating the desired resources. This example allocates ",(0,a.jsx)(n.strong,{children:"8x AMD MI300X GPUs"})," and ",(0,a.jsx)(n.strong,{children:"all RDMA NIC devices"})," to the InferenceService:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'apiVersion: odin.moreh.io/v1alpha1\nkind: InferenceService\nspec:\n  template:\n    spec:\n      containers:\n        - name: main\n          resources:\n            limits:\n              amd.com/gpu: "8"\n              mellanox/hca: "1"\n            requests:\n              amd.com/gpu: "8"\n              mellanox/hca: "1"\n      nodeSelector:\n        moai.moreh.io/accelerator.vendor: amd\n        moai.moreh.io/accelerator.model: mi300x\n      tolerations:\n        - key: "amd.com/gpu"\n          operator: "Exists"\n          effect: "NoSchedule"\n'})}),"\n",(0,a.jsx)(n.p,{children:"If you use a LeaderWorkerSet, you can allocate resources to the worker containers by specifying them in the workerTemplate as well like this:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'apiVersion: odin.moreh.io/v1alpha1\nkind: InferenceService\nspec:\n  workerTemplate:\n    spec:\n      containers:\n        - name: main\n          resources:\n            limits:\n              amd.com/gpu: "8"\n              mellanox/hca: "1"\n            requests:\n              amd.com/gpu: "8"\n              mellanox/hca: "1"\n      nodeSelector:\n        moai.moreh.io/accelerator.vendor: amd\n        moai.moreh.io/accelerator.model: mi300x\n      tolerations:\n        - key: "amd.com/gpu"\n          operator: "Exists"\n          effect: "NoSchedule"\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"resource-requests-and-limits",children:"Resource requests and limits"}),"\n",(0,a.jsxs)(n.p,{children:["You can request the resources to use by setting ",(0,a.jsx)(n.code,{children:"requests"})," and ",(0,a.jsx)(n.code,{children:"limits"})," in the ",(0,a.jsx)(n.code,{children:"resources"})," section. Each resource uses the following resource names."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["AMD GPU: ",(0,a.jsx)(n.code,{children:"amd.com/gpu"})]}),"\n",(0,a.jsxs)(n.li,{children:["NVIDIA GPU: ",(0,a.jsx)(n.code,{children:"nvidia.com/gpu"})]}),"\n",(0,a.jsxs)(n.li,{children:["RDMA NIC: ",(0,a.jsx)(n.code,{children:"mellanox/hca"})]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"For example, you can specify resources as follows. Note that, for RDMA NICs, you do not specify the number to use; instead, once requested, the Pod is granted access to all RDMA NICs on the node."}),"\n",(0,a.jsxs)(t.A,{children:[(0,a.jsx)(i.A,{value:"amd-gpus",label:"4x AMD GPUs",default:!0,children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'resources:\n  limits:\n    amd.com/gpu: "4"\n  requests:\n    amd.com/gpu: "4"\n'})})}),(0,a.jsx)(i.A,{value:"nvidia-gpus",label:"4x NVIDIA GPUs",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'resources:\n  limits:\n    nvidia.com/gpu: "4"\n  requests:\n    nvidia.com/gpu: "4"\n'})})}),(0,a.jsx)(i.A,{value:"rdma-nics",label:"RDMA NICs",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'resources:\n  limits:\n    mellanox/hca: "1"\n  requests:\n    mellanox/hca: "1"\n'})})})]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"node-selection",children:"Node selection"}),"\n",(0,a.jsxs)(n.p,{children:["Resource names alone cannot distinguish GPU types. If you require a specific GPU type for inference containers, you must ",(0,a.jsx)(n.strong,{children:"additionally"})," use either ",(0,a.jsx)(n.code,{children:"nodeSelector"})," or ",(0,a.jsx)(n.code,{children:"affinity"}),". You only need to use one of the two options. Node selectors are suitable if you simply want to select GPU types, while node affinity allows you to define more complex conditions."]}),"\n",(0,a.jsx)(n.h3,{id:"accelerator-labels",children:"Accelerator labels"}),"\n",(0,a.jsx)(n.p,{children:"MoAI Inference Framework automatically detects the vendors and models of accelerators in the cluster and assigns the following labels accordingly, so they can be used with node selectors or node affinity."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"moai.moreh.io/accelerator.vendor"}),": The vendor of the accelerator (e.g., ",(0,a.jsx)(n.code,{children:"amd"}),")."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"moai.moreh.io/accelerator.model"}),": The specific model of the accelerator."]}),"\n"]}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Vendor"}),(0,a.jsx)(n.th,{children:"Models"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"amd"})}),(0,a.jsxs)(n.td,{children:[(0,a.jsx)(n.code,{children:"mi355x"}),", ",(0,a.jsx)(n.code,{children:"mi350x"}),", ",(0,a.jsx)(n.code,{children:"mi325x"}),", ",(0,a.jsx)(n.code,{children:"mi300x"}),", ",(0,a.jsx)(n.code,{children:"mi308x"}),", ",(0,a.jsx)(n.code,{children:"mi250x"}),", ",(0,a.jsx)(n.code,{children:"mi250"}),", ",(0,a.jsx)(n.code,{children:"mi210"}),", and ",(0,a.jsx)(n.code,{children:"mi100"})]})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"nvidia"})}),(0,a.jsxs)(n.td,{children:[(0,a.jsx)(n.code,{children:"h200-sxm"}),", ",(0,a.jsx)(n.code,{children:"h100-sxm"}),", ",(0,a.jsx)(n.code,{children:"a100-80gb-sxm"}),", ",(0,a.jsx)(n.code,{children:"a100-80gb-pcie"}),", ",(0,a.jsx)(n.code,{children:"a100-40gb-sxm"}),", and ",(0,a.jsx)(n.code,{children:"a100-40gb-pcie"})]})]})]})]}),"\n",(0,a.jsxs)(n.p,{children:["For more detailed information, please refer to the ",(0,a.jsx)(n.a,{href:"/getting_started/supported_devices",children:"supported devices"}),"."]}),"\n",(0,a.jsx)(n.h3,{id:"example-selecting-mi300x-nodes",children:"Example: Selecting MI300X nodes"}),"\n",(0,a.jsxs)(n.p,{children:["To schedule your workload specifically on nodes with AMD MI300X GPUs, use the ",(0,a.jsx)(n.code,{children:"nodeSelector"})," or ",(0,a.jsx)(n.code,{children:"affinity"})," fields as follows."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsxs)(n.strong,{children:["Using ",(0,a.jsx)(n.code,{children:"nodeSelector"}),":"]})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"nodeSelector:\n  moai.moreh.io/accelerator.vendor: amd\n  moai.moreh.io/accelerator.model: mi300x\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsxs)(n.strong,{children:["Using ",(0,a.jsx)(n.code,{children:"affinity"}),":"]})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",metastring:"{17-18}",children:"affinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n        - matchExpressions:\n            - key: moai.moreh.io/accelerator.vendor\n              operator: In\n              values:\n                - amd\n            - key: moai.moreh.io/accelerator.model\n              operator: In\n              values:\n                - mi300x\n            - key: kubernetes.io/hostname\n              operator: In\n              values:\n                - <nodeName>\n                - <nodeName>\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"taints-and-tolerations",children:"Taints and tolerations"}),"\n",(0,a.jsx)(n.p,{children:"Nodes equipped with GPUs are tainted to prevent accidental scheduling of non-GPU workloads. To schedule a pod on these nodes, you must include a matching toleration in your pod specification. Please note that while a toleration is required to schedule a pod on a tainted node, the presence of a toleration does not imply that the node must be tainted."}),"\n",(0,a.jsxs)(t.A,{children:[(0,a.jsxs)(i.A,{value:"amd-gpus",label:"AMD GPUs",default:!0,children:[(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Node taint:"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'taints:\n  - key: "amd.com/gpu"\n    effect: "NoSchedule"\n'})}),(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Pod toleration:"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'tolerations:\n  - key: "amd.com/gpu"\n    operator: "Exists"\n    effect: "NoSchedule"\n'})})]}),(0,a.jsxs)(i.A,{value:"nvidia-gpus",label:"NVIDIA GPUs",children:[(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Node taint:"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'taints:\n  - key: "nvidia.com/gpu"\n    effect: "NoSchedule"\n'})}),(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Pod toleration:"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'tolerations:\n  - key: "nvidia.com/gpu"\n    operator: "Exists"\n    effect: "NoSchedule"\n'})})]})]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},7227(e,n,r){r.d(n,{A:()=>t});r(6540);var s=r(4164);const a="tabItem_Ymn6";var o=r(4848);function t(e){var n=e.children,r=e.hidden,t=e.className;return(0,o.jsx)("div",{role:"tabpanel",className:(0,s.A)(a,t),hidden:r,children:n})}},9489(e,n,r){r.d(n,{A:()=>I});var s=r(6540),a=r(4164),o=r(8630),t=r(4245),i=r(6347),l=r(6494),c=r(2814),d=r(5167),u=r(9900);function h(e){var n,r;return null!=(n=null==(r=s.Children.toArray(e).filter(function(e){return"\n"!==e}).map(function(e){if(!e||(0,s.isValidElement)(e)&&((n=e.props)&&"object"==typeof n&&"value"in n))return e;var n;throw new Error("Docusaurus error: Bad <Tabs> child <"+("string"==typeof e.type?e.type:e.type.name)+'>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.')}))?void 0:r.filter(Boolean))?n:[]}function m(e){var n=e.values,r=e.children;return(0,s.useMemo)(function(){var e=null!=n?n:function(e){return h(e).map(function(e){var n=e.props;return{value:n.value,label:n.label,attributes:n.attributes,default:n.default}})}(r);return function(e){var n=(0,d.XI)(e,function(e,n){return e.value===n.value});if(n.length>0)throw new Error('Docusaurus error: Duplicate values "'+n.map(function(e){return e.value}).join(", ")+'" found in <Tabs>. Every value needs to be unique.')}(e),e},[n,r])}function p(e){var n=e.value;return e.tabValues.some(function(e){return e.value===n})}function x(e){var n=e.queryString,r=void 0!==n&&n,a=e.groupId,o=(0,i.W6)(),t=function(e){var n=e.queryString,r=void 0!==n&&n,s=e.groupId;if("string"==typeof r)return r;if(!1===r)return null;if(!0===r&&!s)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return null!=s?s:null}({queryString:r,groupId:a});return[(0,c.aZ)(t),(0,s.useCallback)(function(e){if(t){var n=new URLSearchParams(o.location.search);n.set(t,e),o.replace(Object.assign({},o.location,{search:n.toString()}))}},[t,o])]}function f(e){var n,r,a,o,t=e.defaultValue,i=e.queryString,c=void 0!==i&&i,d=e.groupId,h=m(e),f=(0,s.useState)(function(){return function(e){var n,r=e.defaultValue,s=e.tabValues;if(0===s.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(r){if(!p({value:r,tabValues:s}))throw new Error('Docusaurus error: The <Tabs> has a defaultValue "'+r+'" but none of its children has the corresponding value. Available values are: '+s.map(function(e){return e.value}).join(", ")+". If you intend to show no default tab, use defaultValue={null} instead.");return r}var a=null!=(n=s.find(function(e){return e.default}))?n:s[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:h})}),g=f[0],j=f[1],v=x({queryString:c,groupId:d}),b=v[0],y=v[1],w=(n=function(e){return e?"docusaurus.tab."+e:null}({groupId:d}.groupId),r=(0,u.Dv)(n),a=r[0],o=r[1],[a,(0,s.useCallback)(function(e){n&&o.set(e)},[n,o])]),N=w[0],I=w[1],k=function(){var e=null!=b?b:N;return p({value:e,tabValues:h})?e:null}();return(0,l.A)(function(){k&&j(k)},[k]),{selectedValue:g,selectValue:(0,s.useCallback)(function(e){if(!p({value:e,tabValues:h}))throw new Error("Can't select invalid tab value="+e);j(e),y(e),I(e)},[y,I,h]),tabValues:h}}var g=r(1062);const j="tabList__CuJ",v="tabItem_LNqP";var b=r(4848);function y(e){var n=e.className,r=e.block,s=e.selectedValue,o=e.selectValue,i=e.tabValues,l=[],c=(0,t.a_)().blockElementScrollPositionUntilNextRender,d=function(e){var n=e.currentTarget,r=l.indexOf(n),a=i[r].value;a!==s&&(c(n),o(a))},u=function(e){var n,r=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":var s,a=l.indexOf(e.currentTarget)+1;r=null!=(s=l[a])?s:l[0];break;case"ArrowLeft":var o,t=l.indexOf(e.currentTarget)-1;r=null!=(o=l[t])?o:l[l.length-1]}null==(n=r)||n.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":r},n),children:i.map(function(e){var n=e.value,r=e.label,o=e.attributes;return(0,b.jsx)("li",Object.assign({role:"tab",tabIndex:s===n?0:-1,"aria-selected":s===n,ref:function(e){l.push(e)},onKeyDown:u,onClick:d},o,{className:(0,a.A)("tabs__item",v,null==o?void 0:o.className,{"tabs__item--active":s===n}),children:null!=r?r:n}),n)})})}function w(e){var n=e.lazy,r=e.children,o=e.selectedValue,t=(Array.isArray(r)?r:[r]).filter(Boolean);if(n){var i=t.find(function(e){return e.props.value===o});return i?(0,s.cloneElement)(i,{className:(0,a.A)("margin-top--md",i.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:t.map(function(e,n){return(0,s.cloneElement)(e,{key:n,hidden:e.props.value!==o})})})}function N(e){var n=f(e);return(0,b.jsxs)("div",{className:(0,a.A)(o.G.tabs.container,"tabs-container",j),children:[(0,b.jsx)(y,Object.assign({},n,e)),(0,b.jsx)(w,Object.assign({},n,e))]})}function I(e){var n=(0,g.A)();return(0,b.jsx)(N,Object.assign({},e,{children:h(e.children)}),String(n))}},8453(e,n,r){r.d(n,{R:()=>t,x:()=>i});var s=r(6540);const a={},o=s.createContext(a);function t(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);