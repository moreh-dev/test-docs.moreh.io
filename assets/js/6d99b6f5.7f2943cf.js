"use strict";(self.webpackChunkmif_docs=self.webpackChunkmif_docs||[]).push([[8684],{6415(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"features/load_aware_routing","title":"Load-aware routing","description":"Load-aware routing monitors the number of assigned requests and real-time utilization metrics of each inference instance (pod) to determine where the next request should be routed. Since individual requests have different workload characteristics and processing times, applying load-aware routing can achieve higher system-level efficiency than round-robin routing and especially help reduce latency variance across requests. Similar to other routing strategies such as prefix cache-aware routing, load-aware routing cannot serve as the sole routing criterion and should be combined with other metrics for optimal decision-making.","source":"@site/versioned_docs/version-v0.0.0/features/load_aware_routing.md","sourceDirName":"features","slug":"/features/load_aware_routing","permalink":"/features/load_aware_routing","draft":false,"unlisted":false,"tags":[],"version":"v0.0.0","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Load-aware routing"},"sidebar":"docs","previous":{"title":"Prefix cache-aware routing","permalink":"/features/prefix_cache_aware_routing"},"next":{"title":"Auto-scaling","permalink":"/features/auto_scaling"}}');var s=t(4848),i=t(8453);const a={sidebar_position:5,title:"Load-aware routing"},o="Load-aware routing",l={},d=[{value:"Key features",id:"key-features",level:2},{value:"Scorer",id:"scorer",level:2},{value:"queue-scorer",id:"queue-scorer",level:3},{value:"load-aware-scorer",id:"load-aware-scorer",level:3},{value:"active-request-scorer",id:"active-request-scorer",level:3},{value:"session-affinity-scorer",id:"session-affinity-scorer",level:3},{value:"no-hit-lru-scorer",id:"no-hit-lru-scorer",level:3},{value:"Checking active scorers",id:"checking-active-scorers",level:3},{value:"Example: round-robin routing vs load-aware routing",id:"example-round-robin-routing-vs-load-aware-routing",level:2},{value:"Benchmarking environment and configuration",id:"benchmarking-environment-and-configuration",level:3},{value:"Workload generator",id:"workload-generator",level:3},{value:"Experimental results",id:"experimental-results",level:3}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components},{Details:r}=n;return r||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"load-aware-routing",children:"Load-aware routing"})}),"\n",(0,s.jsx)(n.p,{children:"Load-aware routing monitors the number of assigned requests and real-time utilization metrics of each inference instance (pod) to determine where the next request should be routed. Since individual requests have different workload characteristics and processing times, applying load-aware routing can achieve higher system-level efficiency than round-robin routing and especially help reduce latency variance across requests. Similar to other routing strategies such as prefix cache-aware routing, load-aware routing cannot serve as the sole routing criterion and should be combined with other metrics for optimal decision-making."}),"\n",(0,s.jsx)(n.h2,{id:"key-features",children:"Key features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.strong,{children:"Heimdall"})," scheduler supports various scoring methods for load-aware routing."]}),"\n",(0,s.jsx)(n.li,{children:"The framework can dynamically adjust the importance of load-aware routing based on defined service level objectives (SLOs) and the current traffic volume."}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"scorer",children:"Scorer"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"Heimdall"})," scheduler currently supports five scoring methods that can be manually enabled, disabled, or weighted to adjust their influence. All scores are normalized to values between 0 and 1, and a higher score indicates a lighter load \u2014 meaning the pod is more preferred for routing. The following configuration file shows an example of manully enabling all scorers and assigning them equal weights."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:"heimdall-values.yaml",children:'...\nconfig:\n  apiVersion: inference.networking.x-k8s.io/v1alpha1\n  kind: EndpointPickerConfig\n  plugins:\n    ...\n    - type: queue-scorer\n    - type: load-aware-scorer\n      parameters:\n        threshold: 128\n    - type: active-request-scorer\n      parameters:\n        requestTimeout: "2m"\n    - type: session-affinity-scorer\n    - type: no-hit-lru-scorer\n    - type: max-score-picker\n      parameters:\n        maxNumOfEndpoints: 2\n  schedulingProfiles:\n    - name: default\n      plugins:\n        ...\n        - pluginRef: queue-scorer\n        - pluginRef: load-aware-scorer\n        - pluginRef: active-request-scorer\n        - pluginRef: session-affinity-scorer\n        - pluginRef: no-hit-lru-scorer\n        - pluginRef: max-score-picker\n        ...\n...\n'})}),"\n",(0,s.jsx)(n.h3,{id:"queue-scorer",children:"queue-scorer"}),"\n",(0,s.jsx)(n.p,{children:"It assigns scores based on the number of queued requests. The pod with the fewest queued requests receives a score of 1.0, and the one with the most receives 0.0. The others are assigned proportionally based on their relative queue lengths."}),"\n",(0,s.jsx)(n.h3,{id:"load-aware-scorer",children:"load-aware-scorer"}),"\n",(0,s.jsxs)(n.p,{children:["It assigns scores also based on the number of queued requests. A pod with no queued requests receives a score of 0.5. If the number of queued requests exceeds the threshold, it receives a score of 1.0. For values in between, the score is proportional to how many requests are waiting relative to the threshold (i.e., ",(0,s.jsx)(n.code,{children:"0.5 + (waitingRequests / threshold)"}),")."]}),"\n",(0,s.jsx)(n.p,{children:"Unlike the queue-scorer, this method prevents excessive score gaps between pods when there are not many pending requests or little variation in their numbers across pods."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"threshold"}),": the threshold that serves as the criterion for overload"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"active-request-scorer",children:"active-request-scorer"}),"\n",(0,s.jsx)(n.p,{children:"It assigns scores based on the number of active request. A pod with no active requests receives a score of 1.0. while the pod with the most active requests receives a score of 0.0. All other pods are assigned scores proportional to their relative number of active requests."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"requestTimeout"}),": If a response is not received within this time, the request is assumed to have been timed out by the inference engine (vLLM). Since the scorer does not have visibility into individual request timeouts, this assumption is necessary \u2014 otherwise, timed-out requests would remain counted as active indefinitely."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"session-affinity-scorer",children:"session-affinity-scorer"}),"\n",(0,s.jsxs)(n.p,{children:["It assigns a higher score if a pod has previously handled a request from the same session (with the same ",(0,s.jsx)(n.code,{children:"x-session-token"})," value in the HTTP header). This indirectly produces a similar effect to prefix cache-aware routing."]}),"\n",(0,s.jsx)(n.h3,{id:"no-hit-lru-scorer",children:"no-hit-lru-scorer"}),"\n",(0,s.jsx)(n.p,{children:"To ensure that cold requests (those without prefix cache hits) are evenly distributed across pods, scores from 0.0 to 1.0 are assigned in order from the pod that most recently received a cold request to the one that received it the longest time ago. This helps ensure that the size of the KV cache stored in either GPU memory or main memory increases evenly across pods."}),"\n",(0,s.jsxs)(n.p,{children:["On the other hand, for hot requests (those with (partial) prefix cache hits), a score of 0.5 is assigned to all pods. That means, unlike other scorers, the ",(0,s.jsx)(n.code,{children:"no-hit-lru-scorer"})," is influenced not only by the state of the pods but also by the input prompts of incoming requests."]}),"\n",(0,s.jsx)(n.p,{children:"To determine whether each request has a prefix cache hit, it operates in integration with the prefix caching plugin."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"prefixPluginName"}),": the name of the prefix caching plugin used to determine whether a cache hit occurs. The default value is ",(0,s.jsx)(n.code,{children:"prefix-cache-scorer"}),". You must specify the actual plugin that is currently enabled."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"lruSize"}),": the maximum number of pods to track for least recently used (LRU) status. The default value is 1024, meaning that only the most recent 1024 pods that received cold requests are tracked and assigned scores between 0.0 and 1.0. All other pods beyond this range are just assigned a score of 1.0."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"checking-active-scorers",children:"Checking active scorers"}),"\n",(0,s.jsxs)(n.p,{children:["If you set the log level of the Heimdall scheduler to 4 (by adding ",(0,s.jsx)(n.code,{children:"-v=4"})," to ",(0,s.jsx)(n.code,{children:"extraArgs"}),"), Heimdall will print logs like the following each time it receives a request. The scorers listed in the logs indicate that they are active and functioning."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"kubectl logs -n mif -f -l app=heimdall | jq -r 'select(.msg | test(\"Running scorer\")) | [.scorer, .msg]'\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",metastring:"Expected output",children:'Defaulted container "main" out of: main, traffic-agent\n[\n  "queue-scorer",\n  "Running scorer"\n]\n[\n  "load-aware-scorer",\n  "Running scorer"\n]\n[\n  "active-request-scorer",\n  "Running scorer"\n]\n[\n  "session-affinity-scorer",\n  "Running scorer"\n]\n...\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"example-round-robin-routing-vs-load-aware-routing",children:"Example: round-robin routing vs load-aware routing"}),"\n",(0,s.jsx)(n.p,{children:"This example shows how load-aware routing can shorten request processing time compared to simple round-robin routing that does not account for imbalance among pods. To emulate real-world scenarios where diverse request patterns coexist, we prepare a workload consisting of requests with varying input and output sequence lengths. Adjusting these parameters and the request order, combined with round-robin routing, allows us to closely mirror real-world conditions where utilization imbalance between pods occurs. By introducing load-aware routing, we can prevent overload on any single pod, resulting in higher overall efficiency and reduced total wall-clock time for processing all requests."}),"\n",(0,s.jsx)(n.h3,{id:"benchmarking-environment-and-configuration",children:"Benchmarking environment and configuration"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Item"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Servers"}),(0,s.jsx)(n.td,{children:"2x servers, each equipped with 4x AMD MI250 GPUs"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Networking"}),(0,s.jsx)(n.td,{children:"InfiniBand HDR"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Inference Engine"}),(0,s.jsx)(n.td,{children:"vLLM"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"meta-llama/Llama-3.3-70B-Instruct"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Pods"}),(0,s.jsx)(n.td,{children:"4x, each using 2x AMD MI250 GPUs"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Scorers used"}),(0,s.jsx)(n.td,{children:"queue-scorer and no-hit-lru-scorer, both with a weight of 1"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"workload-generator",children:"Workload generator"}),"\n",(0,s.jsx)(n.p,{children:"The following is a Python script used to send requests with varying input and output sequence lengths."}),"\n",(0,s.jsxs)(r,{children:[(0,s.jsxs)("summary",{children:["Source code of the workload generator (",(0,s.jsx)(n.code,{children:"workload.py"}),")"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"workload.py",children:'#!/usr/bin/env python3\nimport asyncio, aiohttp, time, os, argparse, statistics as stats, random, json\nfrom typing import Dict, Any, List, Tuple\nimport string\n\n# -------------------- CLI --------------------\n\ndef parse_args(): p = argparse.ArgumentParser(description="Generate a JSQ-favorable workload with inline heavy requests") p.add_argument("--api-base", default=os.environ.get("API_BASE", "http:127.0.0.1:8000"), help="Base URL (e.g., https://host)") p.add_argument("--api-key", default=os.environ.get("API_KEY", ""), help="Bearer token if your gateway requires it") p.add_argument("--model", default=os.environ.get("MODEL", "meta-llama/Llama-3.3-70B-Instruct"), help="Model name served by your endpoint") p.add_argument("--workers", "-N", type=int, default=int(os.environ.get("WORKERS", 4)), help="Logical workers behind the router (for sizing rationale only)") p.add_argument("--total", type=int, default=int(os.environ.get("TOTAL", 1000)), help="Total number of requests to send (including seeds + inline heavies)") p.add_argument("--heavy-seeds", type=int, default=int(os.environ.get("HEAVY_SEEDS", 2)), help="Heavy requests sent first to pin some workers") p.add_argument("--inline-every", type=int, default=int(os.environ.get("INLINE_EVERY", 50)), help="Insert 1 inline heavy after every N non-heavy burst items (ignored if --inline-heavy-count set)") p.add_argument("--inline-heavy-count", type=int, default=int(os.environ.get("INLINE_HEAVY_COUNT", 0)), help="Override: explicit number of inline heavy requests to insert within the burst (0 = auto from --inline-every)") p.add_argument("--seed-interval", type=float, default=float(os.environ.get("SEED_INTERVAL", 0.05)), help="Seconds between heavy seeds") p.add_argument("--burst-start", type=float, default=float(os.environ.get("BURST_START", 0.30)), help="When the burst begins (seconds after start)") p.add_argument("--burst-interval", type=float, default=float(os.environ.get("BURST_INTERVAL", 0.010)), help="Base spacing between burst requests (seconds)") p.add_argument("--jitter", type=float, default=float(os.environ.get("JITTER", 0.003)), help="Uniform \xb1jitter added to each burst spacing (seconds)") p.add_argument("--inline-extra-gap", type=float, default=float(os.environ.get("INLINE_EXTRA_GAP", 0.06)), help="Extra gap AFTER any inline heavy to help it pin a worker (seconds)") p.add_argument("--timeout", type=float, default=600.0, help="Per-request timeout in seconds") p.add_argument("--print-output", action="store_true", help="Print model output text for each request") p.add_argument("--save-json", default="", help="Optional path to save per-request results as JSON") return p.parse_args()\n\n# -------------------- Helpers --------------------\n\ndef make_prompt(words: int, tag: str) -> str: """ Generate deterministic but unique prompt text per request. - Each run produces the same prompts (fixed random seed). - Each request gets different text content. - \'words\' roughly controls input sequence length (ISL). """ # Use a fixed global seed for reproducibility across runs base_seed = 12345 # Derive a stable per-tag seed so same tag \u2192 same text random.seed(base_seed + sum(ord(c) for c in tag))\n\n    vocab = [\n        "quantum", "neural", "matrix", "optimization", "graph", "tensor",\n        "latency", "throughput", "bandwidth", "kernel", "scheduler", "routing",\n        "prefill", "decode", "scaling", "gradient", "cache", "expert",\n        "activation", "pipeline", "distributed", "dynamic", "cluster", "epoch",\n        "inference", "token", "sampling", "prefetch", "adapter", "mixture",\n        "routing", "context", "parallelism", "load", "dispatch", "bandwidth",\n        "coherence", "gradient", "topology", "fabric", "kernel"\n    ]\n\n    # Pick random words deterministically\n    text_words = [random.choice(vocab) for _ in range(words)]\n    # Add a little random noise (letters) to vary structure\n    noise = \'\'.join(random.choices(string.ascii_lowercase + " ", k=min(words * 5, 2000)))\n\n    text = f"[{tag}] " + \' \'.join(text_words) + " " + noise\n    # Keep safely under context limits\n    return text[:min(len(text), 65000)]\n\ndef req(payload_id: str, isl_words: int, osl_tokens: int, model: str) -> Dict[str, Any]: return { "id": payload_id, "json": { "model": model, "prompt": make_prompt(isl_words, payload_id), "max_tokens": osl_tokens, "temperature": 0, "top_p": 1, "stream": False, }, }\n\ndef mk_id(prefix: str, i: int, width: int) -> str: return f"{prefix}{i:0{width}d}"\n\n# -------------------- Workload builders --------------------\n\nHEAVY_SPECS: List[Tuple[int, int]] = [ (1300, 320), (1500, 350), (2700, 780), (4000, 1050), ]\n\ndef build_workload(args) -> List[Dict[str, Any]]: """ Build list of request dicts: - \'heavy-seeds\' at the very beginning - a burst mixing short/medium requests - inline heavy requests sprinkled inside the burst Total count ~= args.total """ random.seed(42) workload: List[Dict[str, Any]] = []\n\n    # 1) Seed heavies\n    for i in range(args.heavy_seeds):\n        isl, osl = HEAVY_SPECS[i % len(HEAVY_SPECS)]\n        workload.append(req(mk_id("S", i+1, 3) + "_HEAVY", isl, osl, args.model))\n\n    remaining = max(0, args.total - args.heavy_seeds)\n\n    # Determine inline heavy count\n    if args.inline_heavy_count and args.inline_heavy_count > 0:\n        inline_heavy_count = min(args.inline_heavy_count, remaining)\n    else:\n        # auto: ~1 inline heavy after every \'inline_every\' non-heavy\n        inline_heavy_count = max(0, remaining // (args.inline_every + 1))\n\n    non_heavy_burst = max(0, remaining - inline_heavy_count)\n    short_cnt = non_heavy_burst // 2\n    medium_cnt = non_heavy_burst - short_cnt\n\n    # Build burst pool and shuffle\n    burst_pool: List[Tuple[str, int, int]] = []\n    for _ in range(short_cnt):\n        isl = random.randint(10, 20)\n        osl = random.randint(25, 40)\n        burst_pool.append(("SHORT", isl, osl))\n    for _ in range(medium_cnt):\n        isl = random.randint(120, 180)\n        osl = random.randint(60, 90)\n        burst_pool.append(("MED", isl, osl))\n    random.shuffle(burst_pool)\n\n    # Compute insertion positions to spread inline heavies across the burst\n    inline_pos = []\n    if inline_heavy_count > 0 and len(burst_pool) > 0:\n        step = max(1, len(burst_pool) // inline_heavy_count)\n        inline_pos = [min(len(burst_pool), (k+1)*step) for k in range(inline_heavy_count)]\n\n    # Assemble burst + inline heavies\n    inline_used = 0\n    heavy_cursor = 0\n    built_burst: List[Tuple[str, int, int]] = []\n    for idx, item in enumerate(burst_pool, start=1):\n        built_burst.append(item)\n        if inline_used < len(inline_pos) and idx == inline_pos[inline_used]:\n            isl, osl = HEAVY_SPECS[heavy_cursor % len(HEAVY_SPECS)]\n            built_burst.append(("HEAVY_INLINE", isl, osl))\n            heavy_cursor += 1\n            inline_used += 1\n\n    # Convert to request dicts with IDs\n    b_short = b_med = b_heavy_inline = 0\n    for kind, isl, osl in built_burst:\n        if kind == "SHORT":\n            b_short += 1\n            workload.append(req(mk_id("B", b_short, 4) + "_SHORT", isl, osl, args.model))\n        elif kind == "MED":\n            b_med += 1\n            workload.append(req(mk_id("M", b_med, 4) + "_MED", isl, osl, args.model))\n        else:\n            b_heavy_inline += 1\n            workload.append(req(mk_id("I", b_heavy_inline, 3) + "_HEAVY", isl, osl, args.model))\n\n    # Quick summary\n    print(f"[Workload] workers={args.workers}  total={len(workload)} | "\n          f"seed_heavy={args.heavy_seeds}, inline_heavy={inline_heavy_count}, "\n          f"nonheavy={non_heavy_burst} (short={short_cnt}, med={medium_cnt})")\n\n    return workload\n\ndef build_schedule(workload: List[Dict[str, Any]], args) -> Dict[str, float]: """ Assign a launch time to every request: - seeds: 0, seed_interval, 2\\*seed_interval, ... - burst: starts at burst_start, spaced by burst_interval \xb1 jitter add inline_extra_gap AFTER any inline heavy """ # Map id -> time sched: Dict[str, float] = {}\n\n    # Seeds first\n    for i in range(args.heavy_seeds):\n        sched[workload[i]["id"]] = i * args.seed_interval\n\n    # Burst\n    t = args.burst_start\n    for w in workload[args.heavy_seeds:]:\n        rid = w["id"]\n        sched[rid] = t\n        # base spacing with jitter\n        dt = args.burst_interval + (random.uniform(-args.jitter, args.jitter) if args.jitter > 0 else 0.0)\n        # inline heavy \u2192 add an extra gap AFTER it\n        if rid.endswith("_HEAVY") and rid.startswith("I"):\n            dt += args.inline_extra_gap\n        t += max(0.0, dt)\n\n    print(f"[Timing] seed_interval={args.seed_interval:.3f}s  burst_start={args.burst_start:.3f}s  "\n          f"burst_interval={args.burst_interval:.3f}s  jitter=\xb1{args.jitter:.3f}s  "\n          f"inline_extra_gap={args.inline_extra_gap:.3f}s")\n    return sched\n\n# -------------------- HTTP runner --------------------\n\nasync def post_one(session: aiohttp.ClientSession, url: str, headers: Dict[str, str], r: Dict[str, Any], timeout: float, print_output: bool): t0 = time.perf_counter() async with session.post(url, json=r["json"], headers=headers, timeout=timeout) as resp: if resp.status == 200: data = await resp.json() else: # capture some text for debugging on error data = {"error_text": (await resp.text())[:1000]} t1 = time.perf_counter()\n\n    text = ""\n    if resp.status == 200 and isinstance(data, dict) and "choices" in data and data["choices"]:\n        text = data["choices"][0].get("text", "").strip()\n\n    if print_output:\n        print(f"\\n=== {r[\'id\']} ===")\n        print(f"Status: {resp.status}  Latency: {t1 - t0:.3f}s")\n        print("=" * 60)\n\n    return {\n        "id": r["id"],\n        "status": resp.status,\n        "latency_s": t1 - t0,\n        "resp_text": text,\n        "error": None if resp.status == 200 else data,\n    }\n\n# -------------------- Main --------------------\n\nasync def main(): args = parse_args() random.seed(42) # deterministic timing/jitter\n\n    # Build workload & schedule\n    workload = build_workload(args)\n    schedule = build_schedule(workload, args)\n\n    url = f"{args.api_base.rstrip(\'/\')}/v1/completions"\n    headers = {"Content-Type": "application/json"}\n    if args.api_key:\n        headers["Authorization"] = f"Bearer {args.api_key}"\n\n    start = time.perf_counter()\n    results: List[Dict[str, Any]] = []\n\n    conn = aiohttp.TCPConnector(\n        limit=0,  # 0 = no limit\n        limit_per_host=0,   # no per-host limit\n        ttl_dns_cache=300\n    )\n\n    async with aiohttp.ClientSession(connector=conn) as session:\n        tasks = []\n\n        async def launcher(r):\n            launch_at = schedule[r["id"]]\n            # sleep until its scheduled time relative to \'start\'\n            await asyncio.sleep(max(0.0, launch_at - (time.perf_counter() - start)))\n\n            now = time.perf_counter() - start\n            print(f"[SEND] {r[\'id\']:<15}  scheduled={launch_at:.3f}s  actual={now:.3f}s")\n\n            return await post_one(session, url, headers, r, args.timeout, args.print_output)\n\n        for r in workload:\n            tasks.append(asyncio.create_task(launcher(r)))\n\n        # Gather as they finish\n        for t in tasks:\n            res = await t\n            results.append(res)\n            # quick per-request line:\n            print(f"[{res[\'id\']}] status={res[\'status\']} latency={res[\'latency_s\']:.3f}s")\n\n    # Summaries\n    lat_ok = [r["latency_s"] for r in results if r["status"] == 200]\n    summary = {}\n    if lat_ok:\n        avg = sum(lat_ok) / len(lat_ok)\n        p50 = stats.median(lat_ok)\n        p95 = sorted(lat_ok)[max(0, int(len(lat_ok) * 0.95) - 1)]\n        print("\\n=== Latency Summary (successful requests) ===")\n        print(f"count={len(lat_ok)}  avg={avg:.3f}s  p50={p50:.3f}s  p95={p95:.3f}s")\n        summary.update({"count": len(lat_ok), "avg": avg, "p50": p50, "p95": p95})\n\n    # Total E2E wall-clock\n    end = time.perf_counter()\n    total_e2e = end - start\n    print(f"\\n=== Total End-to-End (E2E) Latency ===")\n    print(f"All requests completed in {total_e2e:.3f} seconds")\n    summary["total_e2e"] = total_e2e\n\n    # Optional save\n    if args.save_json:\n        try:\n            payload = {"requests": results, "summary": summary}\n            with open(args.save_json, "w", encoding="utf-8") as f:\n                json.dump(payload, f, ensure_ascii=False, indent=2)\n            print(f"\\nSaved results to {args.save_json}")\n        except Exception as e:\n            print(f"\\nFailed to save JSON: {e}")\n\nif **name** == "**main**": asyncio.run(main())\n\n'})})]}),"\n",(0,s.jsxs)(n.p,{children:["You can execute the workload generator as follows. Please replace ",(0,s.jsx)(n.code,{children:"{SERVER_ENDPOINT}"})," and ",(0,s.jsx)(n.code,{children:"{SAVE_DIRECTORY}"})," with your own values."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"python3 workload.py --api-base {SERVER_ENDPOINT} --save-json {SAVE_DIRECTORY} --heavy-seeds 400 --seed-interval 0 --total 800 --burst-start 5 --burst-interval 0.5 --jitter 0.5 --inline-extra-gap 0.5 --inline-every 24 --timeout 10000\n"})}),"\n",(0,s.jsx)(n.p,{children:"It sends a total of 800 requests, with the first 400 designated as heavy workloads. Among these heavy requests, one super-heavy request is sent after every three normal ones. The purpose of sending 400 heavy requests first is to quickly saturate the pods, causing subsequent requests to be queued. Afterwards, shorter requests are used to evaluate the effect of the routing mechanisms. To introduce additional stress to the system, a heavy request is injected after every 24 requests."}),"\n",(0,s.jsx)(n.h3,{id:"experimental-results",children:"Experimental results"}),"\n",(0,s.jsx)(n.p,{children:"The following table shows the total wall-clock time elapsed to process all requests when applying round-robin routing versus load-aware routing. When load-aware routing was applied, the total wall-clock time was reduced by ~18%."}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Metric"}),(0,s.jsx)(n.th,{children:"Round-robin routing"}),(0,s.jsx)(n.th,{children:"Load-aware routing"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Total wall-clock time"}),(0,s.jsx)(n.td,{children:"560.02 s"}),(0,s.jsxs)(n.td,{children:["458.87 s ",(0,s.jsx)(n.strong,{children:"(-18.06%)"})]})]})})]}),"\n",(0,s.jsx)(n.p,{children:"A deeper analysis can be conducted by examining the queuing behavior under both routing mechanisms. The following images illustrate the queue size (the number of pending requests) of each pod while processing incoming requests."}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Routing"}),(0,s.jsx)(n.th,{children:"Queue size trend of each pod over time"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Round-robin"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.img,{alt:"Round Robin",src:t(294).A+"",width:"1888",height:"628"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Load-aware"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.img,{alt:"Load-aware",src:t(7972).A+"",width:"1896",height:"632"})})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"Under round-robin routing, after one minute has passed, the pod represented by the blue line begins to lag behind, taking longer to complete its requests and accumulating a larger queue. This imbalance continues to widen, and even after all other pods finish processing their requests, the blue pod continues handling the requests assigned to it for roughly four minutes."}),"\n",(0,s.jsx)(n.p,{children:"In contrast, load-aware routing does not suffer from such imbalance \u2014 it maintains balanced queues across the pods. The pod represented by the green line takes more time to complete, but the difference from other pods is only about two minutes. This indicates that load-aware routing achieves a much more balanced workload distribution and improves overall processing efficiency."})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},7972(e,n,t){t.d(n,{A:()=>r});const r=t.p+"assets/images/load_aware_routing__load_aware_queue-451a30316c4e865a2542fdc439f1e913.png"},294(e,n,t){t.d(n,{A:()=>r});const r=t.p+"assets/images/load_aware_routing__round_robin_queue-e38fb9cf7b742282c8dd89a900e1b422.png"},8453(e,n,t){t.d(n,{R:()=>a,x:()=>o});var r=t(6540);const s={},i=r.createContext(s);function a(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);