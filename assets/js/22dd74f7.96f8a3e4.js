"use strict";(self.webpackChunkmif_docs=self.webpackChunkmif_docs||[]).push([[567],{5226(e){e.exports=JSON.parse('{"version":{"pluginId":"default","version":"v0.0.0","label":"v0.0.0","banner":null,"badge":true,"noIndex":false,"className":"docs-version-v0.0.0","isLast":true,"docsSidebars":{"docs":[{"type":"link","key":"home","href":"/","label":"Home","docId":"home","unlisted":false},{"type":"category","label":"Getting Started","collapsible":true,"collapsed":false,"items":[{"type":"link","href":"/getting_started/overview","label":"Overview","docId":"getting_started/overview","unlisted":false},{"type":"link","href":"/getting_started/prerequisites","label":"Prerequisites","docId":"getting_started/prerequisites","unlisted":false},{"type":"link","href":"/getting_started/quickstart","label":"Quickstart","docId":"getting_started/quickstart","unlisted":false},{"type":"link","href":"/getting_started/monitoring","label":"Monitoring","docId":"getting_started/monitoring","unlisted":false},{"type":"link","href":"/getting_started/supported_devices","label":"Supported devices","docId":"getting_started/supported_devices","unlisted":false}],"key":"getting-started","href":"/getting-started"},{"type":"category","label":"Benchmarking","collapsible":true,"collapsed":false,"items":[{"type":"link","href":"/benchmarking/deepseek_r1_671b_on_amd_mi300x_gpus_maximum_throughput","label":"DeepSeek R1 671B on AMD MI300X GPUs: Maximum throughput","docId":"benchmarking/deepseek_r1_671b_on_amd_mi300x_gpus_maximum_throughput","unlisted":false},{"type":"category","label":"More benchmarking for DeepSeek R1 671B on AMD MI300X GPUs","collapsible":true,"collapsed":false,"items":[{"type":"link","href":"/benchmarking/more_benchmarking_for_deepseek_r1_671b_on_amd_mi300x_gpus/performance_with_prefix_cache_and_load_aware_routing","label":"Performance with prefix cache- and load-aware routing","docId":"benchmarking/more_benchmarking_for_deepseek_r1_671b_on_amd_mi300x_gpus/performance_with_prefix_cache_and_load_aware_routing","unlisted":false}],"key":"more-benchmarking-deepseek-r1-671b-mi300x"}],"key":"benchmarking","href":"/benchmarking"},{"type":"category","label":"Features","collapsible":true,"collapsed":false,"items":[{"type":"link","href":"/features/preset","label":"Presets","docId":"features/preset","unlisted":false},{"type":"link","href":"/features/prefill_decode_disaggregation","label":"Prefill-decode disaggregation","docId":"features/prefill_decode_disaggregation","unlisted":false},{"type":"link","href":"/features/expert_parallelism","label":"Expert parallelism","docId":"features/expert_parallelism","unlisted":false},{"type":"link","href":"/features/prefix_cache_aware_routing","label":"Prefix cache-aware routing","docId":"features/prefix_cache_aware_routing","unlisted":false},{"type":"link","href":"/features/load_aware_routing","label":"Load-aware routing","docId":"features/load_aware_routing","unlisted":false},{"type":"link","href":"/features/auto_scaling","label":"Auto-scaling","docId":"features/auto_scaling","unlisted":false}],"key":"features","href":"/features"},{"type":"category","label":"Best practices","collapsible":true,"collapsed":false,"items":[{"type":"link","href":"/best_practices/resource_allocation","label":"Resource allocation","docId":"best_practices/resource_allocation","unlisted":false},{"type":"link","href":"/best_practices/hf_model_management_with_pv","label":"Hugging Face model management with persistent volume","docId":"best_practices/hf_model_management_with_pv","unlisted":false}],"key":"best-practices","href":"/best-practices"},{"type":"category","label":"Reference","collapsible":true,"collapsed":false,"items":[{"type":"link","href":"/reference/heimdall_scheduler","label":"Heimdall scheduler","docId":"reference/heimdall_scheduler","unlisted":false},{"type":"link","href":"/reference/odin_inference_service","label":"Odin inference service","docId":"reference/odin_inference_service","unlisted":false}],"key":"reference","href":"/reference"}]},"docs":{"benchmarking/deepseek_r1_671b_on_amd_mi300x_gpus_maximum_throughput":{"id":"benchmarking/deepseek_r1_671b_on_amd_mi300x_gpus_maximum_throughput","title":"DeepSeek R1 671B on AMD MI300X GPUs: Maximum throughput","description":"This article presents the performance evaluation method and results of DeepSeek R1 671B inference on 5x AMD MI300X servers (40 GPUs in total).","sidebar":"docs"},"benchmarking/more_benchmarking_for_deepseek_r1_671b_on_amd_mi300x_gpus/performance_with_prefix_cache_and_load_aware_routing":{"id":"benchmarking/more_benchmarking_for_deepseek_r1_671b_on_amd_mi300x_gpus/performance_with_prefix_cache_and_load_aware_routing","title":"Performance with prefix cache- and load-aware routing","description":"This article demonstrates how applying prefix cache-aware routing and load-aware routing when running the DeepSeek R1 671B model on an AMD MI300X GPU cluster can reduce both prefill computation and overall infrastructure cost.","sidebar":"docs"},"best_practices/hf_model_management_with_pv":{"id":"best_practices/hf_model_management_with_pv","title":"Hugging Face model management with persistent volume","description":"Efficient management of large language models (LLMs) is crucial for optimizing storage usage and reducing startup times. Instead of downloading models repeatedly for each pod, using a shared Persistent Volume (PV) allows multiple pods to access the same model files.","sidebar":"docs"},"best_practices/resource_allocation":{"id":"best_practices/resource_allocation","title":"Resource allocation","description":"This document describes how to allocate resources (accelerators and NICs) to your inference containers, select specific nodes using node selectors and node affinity, and handle taints. When an InferenceService is created, it generates a Deployment or a LeaderWorkerSet, which ultimately results in the creation of Pods. Therefore, placing a Pod is synonymous with placing an inference container in this context.","sidebar":"docs"},"features/auto_scaling":{"id":"features/auto_scaling","title":"Auto-scaling","description":"The model inference endpoints provided by the MoAI Inference Framework are often just one of many functions running on the overall AI compute infrastructure. Therefore, it is essential to allocate the appropriate amount of GPU resources (to run the appropriate number of Pods) so that GPUs are not under-utilized while still handling all incoming traffic and meeting the defined service level objectives (SLOs).","sidebar":"docs"},"features/expert_parallelism":{"id":"features/expert_parallelism","title":"Expert parallelism","description":"Expert parallelism (EP) refers to a parallelization method that assigns and executes multiple experts of a Mixture-of-Experts (MoE) model across different GPUs. While applying EP is not mandatory for MoE models, it is generally helpful for increasing overall inference throughput (total output tokens per second).","sidebar":"docs"},"features/load_aware_routing":{"id":"features/load_aware_routing","title":"Load-aware routing","description":"Load-aware routing monitors the number of assigned requests and real-time utilization metrics of each inference instance (pod) to determine where the next request should be routed. Since individual requests have different workload characteristics and processing times, applying load-aware routing can achieve higher system-level efficiency than round-robin routing and especially help reduce latency variance across requests. Similar to other routing strategies such as prefix cache-aware routing, load-aware routing cannot serve as the sole routing criterion and should be combined with other metrics for optimal decision-making.","sidebar":"docs"},"features/prefill_decode_disaggregation":{"id":"features/prefill_decode_disaggregation","title":"Prefill-decode disaggregation","description":"During LLM inference, computation occurs in two stages: prefill and decode. In the prefill phase, the model processes the entire input prompt to generate the first token &mdash; a highly parallel, compute-bound process. The decode phase then predicts one token at a time, reusing the growing KV cache, and is memory-bound.","sidebar":"docs"},"features/prefix_cache_aware_routing":{"id":"features/prefix_cache_aware_routing","title":"Prefix cache-aware routing","description":"Prefix caching refers to a technique that stores the KV cache from previous queries, allowing subsequent queries with an identical prefix to reuse it, thereby eliminating redundant computation and improving performance. Since multiple queries often share common prefixes &mdash; such as system prompts, conversation history, or contextual documents, &mdash; recomputing the KV cache for every request would be highly inefficient.","sidebar":"docs"},"features/preset":{"id":"features/preset","title":"Presets","description":"The MoAI Inference Framework provides a set of pre-configured InferenceServiceTemplates, known as presets. These presets encapsulate standard configurations for various models and hardware setups, simplifying the deployment of inference services.","sidebar":"docs"},"getting_started/monitoring":{"id":"getting_started/monitoring","title":"Monitoring","description":"This document describes how to access Grafana dashboards to monitor the MoAI Inference Framework and provides an overview of the available metrics. Please make sure to install all prerequisites before starting this monitoring guide.","sidebar":"docs"},"getting_started/overview":{"id":"getting_started/overview","title":"Overview","description":"MoAI Inference Framework is designed to enable efficient and automated distributed inference on cluster systems and Kubernetes environments. It supports a wide range of distributed inference techniques &mdash; such as prefill-decode disaggregation, expert parallelism, and prefix-cache-aware routing. Leveraging its unique cost model, it automatically identifies, applies, and dynamically adjusts the optimal way to utilize various accelerators so as to meet the defined service level objectives (SLOs). All of these capabilities are seamlessly integrated not only for NVIDIA GPUs but also for other accelerators, especially AMD GPUs.","sidebar":"docs"},"getting_started/prerequisites":{"id":"getting_started/prerequisites","title":"Prerequisites","description":"This document introduces the prerequisites for the MoAI Inference Framework and provides instructions on how to install them.","sidebar":"docs"},"getting_started/quickstart":{"id":"getting_started/quickstart","title":"Quickstart","description":"In this quickstart, we will launch two vLLM instances (pods) of the Llama 3.2 1B Instruct model and serve them through a single endpoint as an example. Please make sure to install all prerequisites, including the following versions of the components, before starting this quickstart guide.","sidebar":"docs"},"getting_started/supported_devices":{"id":"getting_started/supported_devices","title":"Supported devices","description":"Accelerator labels","sidebar":"docs"},"home":{"id":"home","title":"Home","description":"MoAI Inference Framework is a distributed inference framework that optimizes LLM inference at data center scale.","sidebar":"docs"},"reference/heimdall_scheduler":{"id":"reference/heimdall_scheduler","title":"Heimdall scheduler","description":"Heimdall is the component that performs smart routing and scheduling across multiple inference pods, deciding which pod each request should be sent to. It is implemented according to the Kubernetes Gateway API Inference Extension and can operate together with various gateway controllers.","sidebar":"docs"},"reference/odin_inference_service":{"id":"reference/odin_inference_service","title":"Odin inference service","description":"Odin is the component that launches individual inference pods at scale. These inference pods run Moreh vLLM by default, but they can also use open-source vLLM or SGLang when needed.","sidebar":"docs"}}}}')}}]);