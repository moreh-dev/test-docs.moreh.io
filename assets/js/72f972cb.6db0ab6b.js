"use strict";(self.webpackChunkmif_docs=self.webpackChunkmif_docs||[]).push([[2391],{7028(e,n,a){a.r(n),a.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>d,frontMatter:()=>i,metadata:()=>t,toc:()=>m});const t=JSON.parse('{"id":"best_practices/hf_model_management_with_pv","title":"Hugging Face model management with persistent volume","description":"Efficient management of large language models (LLMs) is crucial for optimizing storage usage and reducing startup times. Instead of downloading models repeatedly for each pod, using a shared Persistent Volume (PV) allows multiple pods to access the same model files.","source":"@site/docs/best_practices/hf_model_management_with_pv.mdx","sourceDirName":"best_practices","slug":"/best_practices/hf_model_management_with_pv","permalink":"/dev/best_practices/hf_model_management_with_pv","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Hugging Face model management with persistent volume"},"sidebar":"docs","previous":{"title":"Resource allocation","permalink":"/dev/best_practices/resource_allocation"},"next":{"title":"Reference","permalink":"/dev/reference"}}');var s=a(4848),o=a(8453);const i={sidebar_position:2,title:"Hugging Face model management with persistent volume"},l="Hugging Face model management with persistent volume",r={},m=[{value:"Create a ReadWriteMany (RWX) PVC",id:"create-a-readwritemany-rwx-pvc",level:2},{value:"Deploy a model manager pod",id:"deploy-a-model-manager-pod",level:2},{value:"Download and manage models",id:"download-and-manage-models",level:2},{value:"Use models in InferenceService",id:"use-models-in-inferenceservice",level:2},{value:"Create an offline template",id:"create-an-offline-template",level:3},{value:"Create the InferenceService",id:"create-the-inferenceservice",level:3},{value:"Create an InferenceService without Template",id:"create-an-inferenceservice-without-template",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"hugging-face-model-management-with-persistent-volume",children:"Hugging Face model management with persistent volume"})}),"\n",(0,s.jsx)(n.p,{children:"Efficient management of large language models (LLMs) is crucial for optimizing storage usage and reducing startup times. Instead of downloading models repeatedly for each pod, using a shared Persistent Volume (PV) allows multiple pods to access the same model files."}),"\n",(0,s.jsx)(n.p,{children:"This workflow optimizes your LLM deployment pipeline by:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Centralizing storage: Creating a ReadWriteMany (RWX) volume named ",(0,s.jsx)(n.code,{children:"models"})," to store multiple large models."]}),"\n",(0,s.jsx)(n.li,{children:"Decoupling management: Utilizing a dedicated model manager pod to download model parameters into the volume."}),"\n",(0,s.jsx)(n.li,{children:"Accelerating deployment: Enabling vLLM or other engines to load the parameters directly from the local mount when an InferenceService is created."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"create-a-readwritemany-rwx-pvc",children:"Create a ReadWriteMany (RWX) PVC"}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["Ensure your Kubernetes cluster supports a storage class that provides ",(0,s.jsx)(n.code,{children:"ReadWriteMany"})," access mode (e.g., NFS, CephFS, or specific cloud provider storage)."]})}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["Values enclosed in ",(0,s.jsx)(n.code,{children:"<>"})," (e.g., ",(0,s.jsx)(n.code,{children:"<namespace>"}),", ",(0,s.jsx)(n.code,{children:"<huggingFaceToken>"}),") are placeholders. You must replace them with your actual values before applying the YAML files. Failure to do so will result in deployment errors."]})}),"\n",(0,s.jsxs)(n.p,{children:["First, create a PersistentVolumeClaim (PVC) with ",(0,s.jsx)(n.code,{children:"ReadWriteMany"})," access mode. This allows multiple nodes to mount the same volume simultaneously, enabling concurrent access for distributed inference."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:"{5,12}",children:"apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: models\n  namespace: <namespace>\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 10Ti\n  storageClassName: <storageClassForReadWriteMany>\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"deploy-a-model-manager-pod",children:"Deploy a model manager pod"}),"\n",(0,s.jsxs)(n.p,{children:["Create a lightweight deployment to serve as a workspace for managing model files. This pod will be used to download models using the ",(0,s.jsx)(n.code,{children:"hf"})," command-line tool."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:"{5,31}",children:"apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: huggingface-cli\n  namespace: <namespace>\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: huggingface-cli\n  template:\n    metadata:\n      labels:\n        app: huggingface-cli\n    spec:\n      containers:\n        - name: main\n          image: ubuntu:24.04\n          command:\n            - /bin/bash\n            - -c\n          args:\n            - |\n              apt-get update \\\n              && apt-get install -y python3 python3-pip \\\n              && pip install huggingface_hub[cli] --break-system-packages\n\n              sleep infinity\n          env:\n            - name: HF_TOKEN\n              value: <huggingFaceToken>\n            - name: HF_HOME\n              value: /mnt/models\n          volumeMounts:\n            - name: models\n              mountPath: /mnt/models\n      volumes:\n        - name: models\n          persistentVolumeClaim:\n            claimName: models\n"})}),"\n",(0,s.jsx)(n.p,{children:"Apply this manifest and wait for the pod to be ready."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"download-and-manage-models",children:"Download and manage models"}),"\n",(0,s.jsxs)(n.p,{children:["Once the management pod is running, you can access it to manage your models. For more details about the ",(0,s.jsx)(n.code,{children:"hf"})," command, refer to the ",(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/huggingface_hub/en/guides/cli",children:"Hugging Face CLI guides"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"Access the pod:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"kubectl exec -it deployment/huggingface-cli -n <namespace> -- bash\n"})}),"\n",(0,s.jsx)(n.p,{children:"Download models:"}),"\n",(0,s.jsx)(n.p,{children:"Download models directly to the mounted volume. This example downloads the meta-llama/Llama-3.2-1B-Instruct model."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"hf download meta-llama/Llama-3.2-1B-Instruct\n"})}),"\n",(0,s.jsx)(n.p,{children:"List downloaded models:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"hf cache ls\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",metastring:"Expected output",children:"ID                                      SIZE     LAST_ACCESSED LAST_MODIFIED REFS\n--------------------------------------- -------- ------------- ------------- ----\nmodel/meta-llama/Llama-3.2-1B-Instruct      7.4G 1 minute ago  1 minute ago  main\nmodel/meta-llama/Llama-3.3-70B-Instruct   405.7G 1 days ago    1 days ago    main\nmodel/openai/gpt-oss-120b                 195.8G 1 days ago    1 days ago    main\nmodel/openai/gpt-oss-20b                   82.6G 1 days ago    1 days ago    main\n...\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"use-models-in-inferenceservice",children:"Use models in InferenceService"}),"\n",(0,s.jsx)(n.p,{children:"To use the pre-downloaded models in an InferenceService, define an InferenceServiceTemplate that mounts the PVC and sets the environment to offline mode."}),"\n",(0,s.jsx)(n.h3,{id:"create-an-offline-template",children:"Create an offline template"}),"\n",(0,s.jsxs)(n.p,{children:["This template configures the worker pods to mount the shared storage (",(0,s.jsx)(n.code,{children:"/mnt/models"}),") and tells the Hugging Face library to operate in offline mode."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:"{5}",children:"apiVersion: odin.moreh.io/v1alpha1\nkind: InferenceServiceTemplate\nmetadata:\n  name: template-hf-hub-offline\n  namespace: <namespace>\nspec:\n  template:\n    spec:\n      containers:\n        - name: main\n          env:\n            - name: HF_HOME\n              value: /mnt/models\n            - name: HF_HUB_OFFLINE\n              value: '1'\n          volumeMounts:\n            - name: models\n              mountPath: /mnt/models\n      volumes:\n        - name: models\n          persistentVolumeClaim:\n            claimName: models\n"})}),"\n",(0,s.jsx)(n.h3,{id:"create-the-inferenceservice",children:"Create the InferenceService"}),"\n",(0,s.jsxs)(n.p,{children:["Reference the offline template in your InferenceService. Specify the model ID (e.g., meta-llama/Llama-3.2-1B-Instruct) in the ",(0,s.jsx)(n.code,{children:"args"})," field to load the model from the offline cache."]}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["The container name in the InferenceService (e.g., ",(0,s.jsx)(n.code,{children:"main"}),") must match the container name defined in the InferenceServiceTemplate for the configurations to merge correctly."]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:"{5,12}",children:"apiVersion: odin.moreh.io/v1alpha1\nkind: InferenceService\nmetadata:\n  name: llama3-1b-instruct\n  namespace: <namespace>\nspec:\n  templateRefs:\n    - name: template-hf-hub-offline\n  template:\n    spec:\n      containers:\n        - name: main\n          image: 255250787067.dkr.ecr.ap-northeast-2.amazonaws.com/quickstart/moreh-vllm:20250915.1\n          command:\n            - vllm\n            - serve\n          args:\n            - meta-llama/Llama-3.2-1B-Instruct\n            - --port\n            - '8000'\n          resources:\n            requests:\n              amd.com/gpu: '1'\n            limits:\n              amd.com/gpu: '1'\n          ports:\n            - name: http\n              containerPort: 8000\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 8000\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 3\n      tolerations:\n        - key: amd.com/gpu\n          operator: Exists\n          effect: NoSchedule\n"})}),"\n",(0,s.jsx)(n.h3,{id:"create-an-inferenceservice-without-template",children:"Create an InferenceService without Template"}),"\n",(0,s.jsx)(n.p,{children:"Alternatively, you can configure the volume mount and environment variables directly within the InferenceService without using a template. However, as the number of InferenceService and models grows, managing individual configurations can become complex. In such cases, using templates is recommended to simplify management and reduce duplication."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:"{5,19-23,42-48}",children:"apiVersion: odin.moreh.io/v1alpha1\nkind: InferenceService\nmetadata:\n  name: llama3-1b-instruct-no-template\n  namespace: <namespace>\nspec:\n  template:\n    spec:\n      containers:\n        - name: main\n          image: 255250787067.dkr.ecr.ap-northeast-2.amazonaws.com/quickstart/moreh-vllm:20250915.1\n          command:\n            - vllm\n            - serve\n          args:\n            - meta-llama/Llama-3.2-1B-Instruct\n            - --port\n            - '8000'\n          env:\n            - name: HF_HOME\n              value: /mnt/models\n            - name: HF_HUB_OFFLINE\n              value: '1'\n          resources:\n            requests:\n              amd.com/gpu: '1'\n            limits:\n              amd.com/gpu: '1'\n          ports:\n            - name: http\n              containerPort: 8000\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 8000\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 3\n          volumeMounts:\n            - name: models\n              mountPath: /mnt/models\n      volumes:\n        - name: models\n          persistentVolumeClaim:\n            claimName: models\n      tolerations:\n        - key: amd.com/gpu\n          operator: Exists\n          effect: NoSchedule\n"})})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453(e,n,a){a.d(n,{R:()=>i,x:()=>l});var t=a(6540);const s={},o=t.createContext(s);function i(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);