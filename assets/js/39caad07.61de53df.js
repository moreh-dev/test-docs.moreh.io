"use strict";(self.webpackChunkmif_docs=self.webpackChunkmif_docs||[]).push([[4551],{9274(e,n,s){s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>o});const i=JSON.parse('{"id":"features/auto_scaling","title":"Auto-scaling","description":"The model inference endpoints provided by the MoAI Inference Framework are often just one of many functions running on the overall AI compute infrastructure. Therefore, it is essential to allocate the appropriate amount of GPU resources (to run the appropriate number of Pods) so that GPUs are not under-utilized while still handling all incoming traffic and meeting the defined service level objectives (SLOs).","source":"@site/docs/features/auto_scaling.mdx","sourceDirName":"features","slug":"/features/auto_scaling","permalink":"/dev/features/auto_scaling","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"title":"Auto-scaling"},"sidebar":"docs","previous":{"title":"Load-aware routing","permalink":"/dev/features/load_aware_routing"},"next":{"title":"Best practices","permalink":"/dev/best-practices"}}');var r=s(4848),l=s(8453);const t={sidebar_position:6,title:"Auto-scaling"},c="Auto-scaling",a={},o=[{value:"Key features",id:"key-features",level:2},{value:"Manual configuration of auto-scaling rules",id:"manual-configuration-of-auto-scaling-rules",level:2},{value:"Installing KEDA",id:"installing-keda",level:3},{value:"Enabling auto-scaling in the Odin inference service",id:"enabling-auto-scaling-in-the-odin-inference-service",level:3},{value:"Configuration parameters",id:"configuration-parameters",level:3},{value:"Scaling behavior",id:"scaling-behavior",level:4},{value:"Prometheus metric triggers",id:"prometheus-metric-triggers",level:4},{value:"Available metrics for auto-scaling",id:"available-metrics-for-auto-scaling",level:3},{value:"Latency metrics",id:"latency-metrics",level:4},{value:"Queue and load-related metrics",id:"queue-and-load-related-metrics",level:4},{value:"Resource-related metrics",id:"resource-related-metrics",level:4}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"auto-scaling",children:"Auto-scaling"})}),"\n",(0,r.jsx)(n.p,{children:"The model inference endpoints provided by the MoAI Inference Framework are often just one of many functions running on the overall AI compute infrastructure. Therefore, it is essential to allocate the appropriate amount of GPU resources (to run the appropriate number of Pods) so that GPUs are not under-utilized while still handling all incoming traffic and meeting the defined service level objectives (SLOs)."}),"\n",(0,r.jsx)(n.p,{children:"This is where auto-scaling comes into play. Instead of allocating all GPU resources from the start, the system begins with a small number of Pods, and adds more only when traffic increases or SLOs are at risk. Additionally, if traffic decreases, the number of Pods is reduced accordingly. It is also necessary to adjust not only the total number of Pods but also the number of Pods assigned to each disaggregated part (prefill, decode, a set of experts, etc.)."}),"\n",(0,r.jsx)(n.h2,{id:"key-features",children:"Key features"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The framework can dynamically adjust the number of GPU resources (the number of Pods) according to the given SLOs and the current amount of traffic."}),"\n",(0,r.jsxs)(n.li,{children:["Users can manually configure auto-scaling rules using ",(0,r.jsx)(n.a,{href:"https://keda.sh/",children:"KEDA"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"manual-configuration-of-auto-scaling-rules",children:"Manual configuration of auto-scaling rules"}),"\n",(0,r.jsx)(n.h3,{id:"installing-keda",children:"Installing KEDA"}),"\n",(0,r.jsxs)(n.p,{children:["You can install KEDA as follows. See ",(0,r.jsx)(n.a,{href:"https://keda.sh/docs/deploy/",children:"KEDA / Deploying KEDA"})," for more details."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"helm repo add kedacore https://kedacore.github.io/charts\nhelm repo update kedacore\nhelm upgrade -i keda kedacore/keda \\\n    --version 2.18.0 \\\n    -n keda \\\n    --create-namespace\n"})}),"\n",(0,r.jsx)(n.h3,{id:"enabling-auto-scaling-in-the-odin-inference-service",children:"Enabling auto-scaling in the Odin inference service"}),"\n",(0,r.jsxs)(n.p,{children:["To enable auto-scaling, set the ",(0,r.jsx)(n.code,{children:"autoscaling.enabled"})," to ",(0,r.jsx)(n.code,{children:"true"})," under each profile (",(0,r.jsx)(n.code,{children:"decode"})," and ",(0,r.jsx)(n.code,{children:"prefill"}),") of the ",(0,r.jsx)(n.strong,{children:"Odin"})," inference service. If this is set to ",(0,r.jsx)(n.code,{children:"false"}),", the number of replicas remains fixed as specified in the ",(0,r.jsx)(n.code,{children:"replicas"})," field. However, if it is set to ",(0,r.jsx)(n.code,{children:"true"}),", the number of replicas dynamically changes between ",(0,r.jsx)(n.code,{children:"minReplicaCount"})," and ",(0,r.jsx)(n.code,{children:"maxReplicaCount"}),". The following is an example of configuring auto-scaling for the prefill phase (and not for the decode phase)."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",metastring:"inference-service-values.yaml",children:'...\ndecode:\n  replicas: 4\n  autoscaling:\n    enabled: false\n  ...\n\nprefill:\n  autoscaling:\n    enabled: true\n    minReplicaCount: 1\n    maxReplicaCount: 6\n    behavior:\n      scaleUp:\n        stabilizationWindowSeconds: 100\n        policies:\n          - type: Pods\n            value: 1\n            periodSeconds: 200\n      scaleDown:\n        stabilizationWindowSeconds: 200\n        policies:\n          - type: Pods\n            value: 1\n            periodSeconds: 60\n    triggers:\n      - type: prometheus\n        metricType: Value\n        metadata:\n          serverAddress: http://prometheus-operated.prometheus-stack:9090\n          query: histogram_quantile(0.9, sum by(le) (rate(llm_time_to_first_token_seconds_bucket{job="{{ include "common.names.namespace" . }}/{{ include "inferenceService.prefill.fullname" . }}"}[2m])))\n          threshold: "1"  # Scale up if the P90 TTFT exceeds 1 second\n  ...\n'})}),"\n",(0,r.jsx)(n.h3,{id:"configuration-parameters",children:"Configuration parameters"}),"\n",(0,r.jsx)(n.h4,{id:"scaling-behavior",children:"Scaling behavior"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"behavior.scaleUp"})," and ",(0,r.jsx)(n.code,{children:"behavior.scaleDown"})," section control how fast the system scales up and down, respectively. For more details, see the ",(0,r.jsx)(n.a,{href:"https://github.com/kubernetes/enhancements/blob/master/keps/sig-autoscaling/853-configurable-hpa-scale-velocity/README.md",children:"Kubernetes HPA Scale Velocity KEP"}),"."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"stabilizationWindowSeconds"}),": specifies the duration of the time window the autoscaler considers when determining the target replica count.","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["When scaling up, the system picks the ",(0,r.jsx)(n.strong,{children:"smallest"})," replica count recommended during the window.","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Set to 0 to respond immediately to load increases."}),"\n",(0,r.jsx)(n.li,{children:"Use a non-zero value (e.g., 300 seconds) to prevent rapid scaling due to temporary traffic spikes."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["When scaling down, the system picks the ",(0,r.jsx)(n.strong,{children:"largest"})," replica count recommended during the window.","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Set to 0 for immediate scale-down (not recommended for production)."}),"\n",(0,r.jsx)(n.li,{children:"Use a non-zero value (e.g., 300 seconds) to prevent rapid scaling due to temporary traffic drops."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"policies"}),": define the maximum rate at which replicas can be added or removed. Each item includes the following fields.","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"type"}),": either ",(0,r.jsx)(n.code,{children:"Pods"})," (absolute replica count) or ",(0,r.jsx)(n.code,{children:"Percent"})," (percentage relative to the current replica count)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"value"}),": maximum number of pods or percentage that can be added or removed."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"periodSeconds"}),": the time period over which the policy applies."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"selectPolicy"}),": defines how the system determines which policy to apply when multiple policies are specified.","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Set to ",(0,r.jsx)(n.code,{children:"Max"})," (default) to select the policy that allows the maximum change."]}),"\n",(0,r.jsxs)(n.li,{children:["Set to ",(0,r.jsx)(n.code,{children:"Min"})," to select the policy that allows the minimum change."]}),"\n",(0,r.jsxs)(n.li,{children:["Set to ",(0,r.jsx)(n.code,{children:"Disabled"})," to selectively disable scaling up or scaling down."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"prometheus-metric-triggers",children:"Prometheus metric triggers"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"triggers"})," section defines the metrics that will be monitored to determine when to scale. Specifically, the MoAI Inference Framework uses KEDA's Prometheus scaler. For more details, see ",(0,r.jsx)(n.a,{href:"https://keda.sh/docs/scalers/prometheus/",children:"KEDA / Prometheus Scaler"}),". Each trigger has the following fields."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"type"}),": ",(0,r.jsx)(n.code,{children:"prometheus"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"metricType"}),": specifies how the metric value is interpreted. It can be either ",(0,r.jsx)(n.code,{children:"Value"})," or ",(0,r.jsx)(n.code,{children:"AverageValue"}),".","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Set to ",(0,r.jsx)(n.code,{children:"Value"})," to adjust the replica count so that ",(0,r.jsx)(n.code,{children:"currentMetricValue"})," equals ",(0,r.jsx)(n.code,{children:"threshold"}),".","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"desiredReplicaCount = currentReplicaCount * (currentMetricValue / threshold)"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Set to ",(0,r.jsx)(n.code,{children:"AverageValue"})," to set the number of replicas to ",(0,r.jsx)(n.code,{children:"currentMetricValue / threshold"}),".","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"desiredReplicaCount = currentMetricValue / threshold"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"metadata"}),": defines the metric value and the threshold.","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"serverAddress"}),": Prometheus server endpoint."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"query"}),": a PromQL query to calculate the metric value."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"threshold"}),": the target value that triggers scaling."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"available-metrics-for-auto-scaling",children:"Available metrics for auto-scaling"}),"\n",(0,r.jsxs)(n.p,{children:["The following metrics can be used to configure auto-scaling triggers. These metrics are originally exposed by vLLM with the ",(0,r.jsx)(n.code,{children:"vllm:"})," prefix, but are relabeled with the ",(0,r.jsx)(n.code,{children:"llm_"})," prefix in Prometheus through the PodMonitor configuration (for example, ",(0,r.jsx)(n.code,{children:"vllm:time_to_first_token_seconds"})," becomes ",(0,r.jsx)(n.code,{children:"llm_time_to_first_token_seconds"}),") to maintain compatibility with other inference engines such as SGLang."]}),"\n",(0,r.jsx)(n.h4,{id:"latency-metrics",children:"Latency metrics"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"llm_time_to_first_token_seconds_bucket"}),": a histogram of time to first token (TTFT) in seconds, used to scale based on how quickly users receive the first token.","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Example: scale up when the P90 TTFT exceeds 1 second."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'metricType: Value\nmetadata:\n  query: histogram_quantile(0.9, sum by(le) (rate(llm_time_to_first_token_seconds_bucket{job="{{ include "common.names.namespace" . }}/{{ include "inferenceService.prefill.fullname" . }}"}[2m])))\n  threshold: \'1\'\n'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"llm_inter_token_latency_seconds_bucket"}),": a histogram of inter-token latency (ITL) in seconds, used to scale based on token generation speed.","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Example: scale up when the P90 ITL exceeds 200 ms."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'metricType: Value\nmetadata:\n  query: histogram_quantile(0.9, sum by(le) (rate(llm_inter_token_latency_seconds_bucket{job="{{ include "common.names.namespace" . }}/{{ include "inferenceService.prefill.fullname" . }}"}[2m])))\n  threshold: \'0.2\'\n'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"llm_e2e_request_latency_seconds_bucket"}),": a histogram of end-to-end request latency (E2EL) in seconds, used to scale based on total request processing time.","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Example: scale up when the P95 E2EL exceeds 5 seconds."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'metricType: Value\nmetadata:\n  query: histogram_quantile(0.95, sum by(le) (rate(llm_e2e_request_latency_seconds_bucket{job="{{ include "common.names.namespace" . }}/{{ include "inferenceService.prefill.fullname" . }}"}[2m])))\n  threshold: \'5\'\n'})}),"\n",(0,r.jsx)(n.h4,{id:"queue-and-load-related-metrics",children:"Queue and load-related metrics"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"llm_num_requests_waiting"}),": the number of requests waiting to be processed, used to scale up when too many requests are queued."]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"resource-related-metrics",children:"Resource-related metrics"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"llm_kv_cache_usage_perc"}),": the KV cache usage (1 = 100% utilization), used to scale up when the cache is nearly full."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453(e,n,s){s.d(n,{R:()=>t,x:()=>c});var i=s(6540);const r={},l=i.createContext(r);function t(e){const n=i.useContext(l);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),i.createElement(l.Provider,{value:n},e.children)}}}]);