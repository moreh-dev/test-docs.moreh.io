"use strict";(self.webpackChunkmif_docs=self.webpackChunkmif_docs||[]).push([[4967],{9168(e,n,r){r.r(n),r.d(n,{assets:()=>h,contentTitle:()=>l,default:()=>p,frontMatter:()=>c,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"features/prefix_cache_aware_routing","title":"Prefix cache-aware routing","description":"Prefix caching refers to a technique that stores the KV cache from previous queries, allowing subsequent queries with an identical prefix to reuse it, thereby eliminating redundant computation and improving performance. Since multiple queries often share common prefixes &mdash; such as system prompts, conversation history, or contextual documents, &mdash; recomputing the KV cache for every request would be highly inefficient.","source":"@site/docs/features/prefix_cache_aware_routing.mdx","sourceDirName":"features","slug":"/features/prefix_cache_aware_routing","permalink":"/dev/features/prefix_cache_aware_routing","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Prefix cache-aware routing"},"sidebar":"docs","previous":{"title":"Expert parallelism","permalink":"/dev/features/expert_parallelism"},"next":{"title":"Load-aware routing","permalink":"/dev/features/load_aware_routing"}}');var t=r(4848),s=r(8453),a=r(9489),o=r(7227);const c={sidebar_position:4,title:"Prefix cache-aware routing"},l="Prefix cache-aware routing",h={},d=[{value:"Key features",id:"key-features",level:2},{value:"Scorer",id:"scorer",level:2},{value:"Tokenizer and prefix store",id:"tokenizer-and-prefix-store",level:3},{value:"Token processor",id:"token-processor",level:3},{value:"KV block index",id:"kv-block-index",level:3},{value:"Example: random routing vs prefix cache-aware routing",id:"example-random-routing-vs-prefix-cache-aware-routing",level:2},{value:"Benchmarking environment and configuration",id:"benchmarking-environment-and-configuration",level:3},{value:"Deployment",id:"deployment",level:3},{value:"Experimental result",id:"experimental-result",level:3}];function u(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"prefix-cache-aware-routing",children:"Prefix cache-aware routing"})}),"\n",(0,t.jsx)(n.p,{children:"Prefix caching refers to a technique that stores the KV cache from previous queries, allowing subsequent queries with an identical prefix to reuse it, thereby eliminating redundant computation and improving performance. Since multiple queries often share common prefixes \u2014 such as system prompts, conversation history, or contextual documents, \u2014 recomputing the KV cache for every request would be highly inefficient."}),"\n",(0,t.jsx)(n.p,{children:"In a system composed of multiple inference instances (Pods), each instance maintains its own (L1) prefix cache in GPU memory. As a result, the cache hit rate (the length of the cached prefix) can vary depending on which instance a request is routed to. Prefix cache-aware routing calculates the cache hit rate of the given request for each Pod and prioritizes routing to the Pod with the highest cache coverage. This reduces redundant KV computation and improves both time to first token (TTFT) and overall throughput."}),"\n",(0,t.jsx)(n.p,{children:"However, in real-world inference systems, the cache hit rate alone cannot serve as the sole routing criterion. It must be considered alongside other factors \u2014 such as the workload characteristics of the requests and the current state of each Pod \u2014 to make optimal routing decisions."}),"\n",(0,t.jsx)(n.h2,{id:"key-features",children:"Key features"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.strong,{children:"Heimdall"})," scheduler tokenizes the request prompt, calculates the cache hit rate for each Pod, and assigns a normalized score to each Pod so that it can be used as a routing decision criterion. It continuously receives updates on each Pod's cache status through ZMQ events."]}),"\n",(0,t.jsx)(n.li,{children:"The framework can determine how much importance to assign to prefix cache-aware routing based on the given service level objectives (SLOs) and the computation characteristics of the GPUs (the penalty of KV cache recomputation)."}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"scorer",children:"Scorer"}),"\n",(0,t.jsxs)(n.p,{children:["Prefix cache-aware routing is applied by enabling and configuring ",(0,t.jsx)(n.strong,{children:"precise-prefix-cache-scorer"})," in the ",(0,t.jsx)(n.strong,{children:"Heimdall"})," scheduler. The following configuration file shows an example of setting up the scorer, including each pod's prefix cache information and the model tokenizer details."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",metastring:"heimdall-values.yaml {24}",children:'...\nconfig:\n  apiVersion: inference.networking.x-k8s.io/v1alpha1\n  kind: EndpointPickerConfig\n  plugins:\n    ...\n    - type: precise-prefix-cache-scorer\n      parameters:\n        indexerConfig:\n          prefixStoreConfig:\n            cacheSize: 500000\n            blockSize: 256\n          tokenProcessorConfig:\n            blockSize: 16\n            hashSeed: "12345"\n          kvBlockIndexConfig:\n            inMemoryConfig:\n              size: 100000000\n              podCacheSize: 10\n            enableMetrics: true\n          tokenizersPoolConfig:\n            workersCount: 8\n            minPrefixOverlapRatio: 0.8\n            huggingFaceToken: <huggingFaceToken>\n            tokenizersCacheDir: "/tmp"\n        kvEventsConfig:\n          zmqEndpoint: "tcp://*:5557"\n          topicFilter: "kv@"\n          concurrency: 16\n    - type: max-score-picker\n      parameters:\n        maxNumOfEndpoints: 2\n  schedulingProfiles:\n    - name: default\n      plugins:\n        ...\n        - pluginRef: precise-prefix-cache-scorer\n        - pluginRef: max-score-picker\n        ...\n...\n'})}),"\n",(0,t.jsx)(n.h3,{id:"tokenizer-and-prefix-store",children:"Tokenizer and prefix store"}),"\n",(0,t.jsx)(n.p,{children:"Each vLLM instance (pod) manages its own prefix cache, which uses tokenized sequences rather than raw prompts as cache keys. Therefore, to determine prefix cache hit rates, the scorer must first tokenize each incoming prompt using the model's tokenizer."}),"\n",(0,t.jsxs)(n.p,{children:["Although multiple worker processes are used to increase concurrency, tokenizing every request can still introduce significant overhead. To reduce this cost, the scorer maintains a cache called the ",(0,t.jsx)(n.strong,{children:"prefix store"}),", which holds previously computed tokenization results. It uses a combination of model (to identify the tokenizer) and prompt as the key, and the tokenized sequence as the value."]}),"\n",(0,t.jsx)(n.p,{children:"If a large portion of a new request's prompt prefix \u2014 for example, more than 80% \u2014 has already been tokenized, the scorer simply reuses cached results to estimate prefix cache hit rates. It even skips tokenizing the remaining unmatched suffix, since calculating the rate using only the first >80% of the prompt yields results nearly identical to those obtained with the full prompt. This is especially reasonable because the goal is not to compute an exact hit rate value, but to identify pods with more prefix cache hits."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"indexerConfig.prefixStoreConfig"}),": configuration for the prefix store. ",(0,t.jsx)(n.code,{children:"cacheSize * blockSize"})," will be the capacity of the prefix store.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"cacheSize"}),": the maximum number of blocks."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"blockSize"}),": the number of characters in each block."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"indexerConfig.tokenizersPoolConfig"}),": configuration for the tokenizer worker processes.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"workersCount"}),": the number of workers."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"minPrefixOverlapRatio"}),": threshold for reusing cached results in the prefix store. A value between 0 and 1."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"huggingFaceToken"}),": Hugging Face token required to download the tokenizer."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"tokenizersCacheDir"}),": Tokenizer download path."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"token-processor",children:"Token processor"}),"\n",(0,t.jsxs)(n.p,{children:["vLLM uses block hashes to efficiently look up all possible prefixes of the current input sequence in the prefix cache. For a given sequence, the first block (the first ",(0,t.jsx)(n.em,{children:"B"})," tokens) has one hash value, the first and second blocks together (the first ",(0,t.jsx)(n.em,{children:"2B"})," tokens) have another, ..., and finally the entire sequence has its own last hash value. These hash values serve as actual keys for the prefix cache. For detailed behavior, please refer to the ",(0,t.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/design/prefix_caching/",children:"Automatic Prefix Caching"})," page."]}),"\n",(0,t.jsx)(n.p,{children:"To emulate vLLM's prefix cache access, the scorer must also compute the block hashes for a given request in the same way as vLLM does."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"indexerConfig.tokenProcessorConfig"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"blockSize"}),": hash block size, must be identical to the vLLM configuration of each pod."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"hashSeed"}),": must be identical to vLLM's ",(0,t.jsx)(n.code,{children:"PYTHONHASHSEED"})," of each pod."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"kv-block-index",children:"KV block index"}),"\n",(0,t.jsxs)(n.p,{children:["Each vLLM instance (pod) publishes events (",(0,t.jsx)(n.code,{children:"BlockStored"}),", ",(0,t.jsx)(n.code,{children:"BlockRemoved"}),", and ",(0,t.jsx)(n.code,{children:"AllCacheCleared"}),") via ZMQ whenever its prefix cache is updated. The scorer subscribes to the ZMQ channels of all pods to receive these events, thereby maintaining a complete view of the overall prefix cache status. This information is stored in a data structure called the ",(0,t.jsx)(n.strong,{children:"KV block index"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"The KV block index uses a prefix hash value as the key and stores a list of pods that hold the corresponding KV cache as the value. KV cache values for the same prefix may exist on multiple pods (since previous requests with that prefix could have been routed to different pods), so the value in the KV block index must be a list of pods."}),"\n",(0,t.jsx)(n.p,{children:"For each incoming request, the scorer tokenizes the prompt into blocks and hashes each block to query the KV block index. The pod that holds the largest number of matching blocks is considered to have the highest cache hit rate."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"indexerConfig.kvBlockIndexConfig"}),": configurations for the KV block index.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"inMemoryConfig.size"}),": the maximum number of entries (hash keys)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"inMemoryConfig.podCacheSize"}),": the maximum length of the pod list for each hash key. If the KV cache for a given prefix is actually stored in more than ",(0,t.jsx)(n.code,{children:"podCacheSize"})," pods, only ",(0,t.jsx)(n.code,{children:"podCacheSize"})," of them are selectively recorded. However, if this value is larger than the total number of vLLM worker pods, it does not affect the result of selecting the pod with the highest cache hit rate."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"enableMetrics"}),": enables Prometheus to collect metrics related to the KV block index."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"kvEventsConfig"}),": ZMQ subscription configuration.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"zmqEndpoint"}),": ZMQ endpoint for communication with vLLM pods."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"topicFilter"}),": a string used to filter prefix cache events only."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"concurrency"}),": the number of workers for receiving ZMQ events and maintaining the KV block index."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"example-random-routing-vs-prefix-cache-aware-routing",children:"Example: random routing vs prefix cache-aware routing"}),"\n",(0,t.jsx)(n.p,{children:"This example shows how prefix cache-aware routing can improve time to first token (TTFT) and end-to-end latency compared with random routing which selects one of the pods at random for each request."}),"\n",(0,t.jsx)(n.h3,{id:"benchmarking-environment-and-configuration",children:"Benchmarking environment and configuration"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Item"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Servers"}),(0,t.jsx)(n.td,{children:"4x servers, each equipped with 4x AMD MI250 GPUs"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Networking"}),(0,t.jsx)(n.td,{children:"InfiniBand HDR"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Inference Engine"}),(0,t.jsx)(n.td,{children:"vLLM"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Model"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"Qwen/Qwen3-32B"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Pods"}),(0,t.jsx)(n.td,{children:"8x, each using 2x AMD MI250 GPUs"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"deployment",children:"Deployment"}),"\n",(0,t.jsxs)(n.p,{children:["The following configuration file shows how to set up the ",(0,t.jsx)(n.strong,{children:"precise-prefix-cache-scorer"})," on the ",(0,t.jsx)(n.strong,{children:"Heimdall"})," scheduler. Each of the eight instances runs on two AMD MI250 GPUs and maintains its own separate prefix cache."]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["In the ",(0,t.jsx)(n.code,{children:"inference-service-values.yaml"})," file, the number of ",(0,t.jsx)(n.code,{children:"amd.com/gpu"})," is set to 4 because each MI250 GPU is recognized as two logical devices at the device driver level. Therefore, four logical devices correspond to two physical GPUs. This behavior is specific to the MI250 model."]})}),"\n",(0,t.jsxs)(a.A,{children:[(0,t.jsx)(o.A,{value:"heimdall",label:"Heimdall scheduler configuration",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",metastring:"heimdall-values.yaml {55}",children:"global:\n  imagePullSecrets:\n    - name: moreh-registry\n\nconfig:\n  apiVersion: inference.networking.x-k8s.io/v1alpha1\n  kind: EndpointPickerConfig\n  plugins:\n    - type: single-profile-handler\n    - type: queue-scorer\n    - type: kv-cache-utilization-scorer\n    - type: max-score-picker\n    - type: precise-prefix-cache-scorer\n      parameters:\n        indexerConfig:\n          prefixStoreConfig:\n            cacheSize: 500000\n            blockSize: 256\n          tokenProcessorConfig:\n            blockSize: 32\n            hashSeed: '12345'\n          kvBlockIndexConfig:\n            inMemoryConfig:\n              size: 100000000\n              podCacheSize: 10\n            enableMetrics: true\n          tokenizersPoolConfig:\n            workersCount: 8\n            minPrefixOverlapRatio: 0.8\n            tokenizersCacheDir: '/tmp'\n        kvEventsConfig:\n          zmqEndpoint: 'tcp://*:5557'\n          topicFilter: 'kv@'\n          concurrency: 16\n  schedulingProfiles:\n    - name: default\n      plugins:\n        - pluginRef: queue-scorer\n          weight: 2\n        - pluginRef: kv-cache-utilization-scorer\n          weight: 2\n        - pluginRef: precise-prefix-cache-scorer\n          weight: 3\n        - pluginRef: max-score-picker\ngateway:\n  name: mif\n  gatewayClassName: istio\n\nserviceMonitor:\n  labels:\n    release: prometheus-stack\n\nextraEnvVars:\n  - name: HF_TOKEN\n    value: <huggingFaceToken>\n"})})}),(0,t.jsx)(o.A,{value:"odin",label:"Odin inference service configuration",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",metastring:"inference-service-values.yaml {41}",children:"global:\n  imagePullSecrets:\n    - name: moreh-registry\n\nextraArgs:\n  - Qwen/Qwen3-32B\n  - --no-enable-log-requests\n  - --disable-uvicorn-access-log\n  - --quantization\n  - 'None'\n  - --kv-transfer-config\n  - '{\"kv_connector\":\"NixlConnector\", \"kv_role\":\"kv_both\"}'\n  - --max-num-batched-tokens\n  - '65536'\n  - --max-num-seqs\n  - '512'\n  - --prefix-caching-hash-algo\n  - sha256_cbor\n  - --block-size\n  - '32'\n  - --kv-events-config\n  - |\n    {\n      \"enable_kv_cache_events\": true,\n      \"publisher\": \"zmq\",\n      \"endpoint\": \"tcp://heimdall.mif.svc.cluster.local:5557\",\n      \"topic\": \"kv@$(POD_IP)@Qwen/Qwen3-32B\",\n      \"buffer_steps\": 1,\n      \"hwm\": 10000,\n      \"max_queue_size\": 10000\n    }\n\nextraEnvVars:\n  - name: VLLM_NIXL_SIDE_CHANNEL_HOST\n    valueFrom:\n      fieldRef:\n        fieldPath: status.podIP\n  - name: UCX_TLS\n    value: rocm_copy,rocm_ipc,self,sm,rc_x\n  - name: HF_TOKEN\n    value: '<huggingFaceToken>'\n  - name: PYTHONHASHSEED\n    value: '12345'\n  - name: VLLM_PORT\n    value: '8000'\n\n_common: &common\n  image:\n    repository: '255250787067.dkr.ecr.ap-northeast-2.amazonaws.com/quickstart/moreh-vllm'\n    tag: '20250915.1'\n\n  podMonitor:\n    labels:\n      release: prometheus-stack\n\ndecode:\n  replicas: 8\n\n  <<: *common\n\n  extraArgs:\n    - --tensor-parallel-size\n    - '4'\n\n  resources:\n    limits:\n      amd.com/gpu: '4'\n      mellanox/hca: '1'\n    requests:\n      amd.com/gpu: '4'\n      mellanox/hca: '1'\n\nprefill:\n  enabled: false\n"})})})]}),"\n",(0,t.jsx)(n.h3,{id:"experimental-result",children:"Experimental result"}),"\n",(0,t.jsx)(n.p,{children:"We alternated among 230 different prompt groups, each containing 5 unique user prompts. Each request consisted of an 8,000-token system prompt and a 1,000-token user prompt (9,000 tokens total), generating 1,000 output tokens. Starting with 46 requests per second (RPS) for warmup and gradually increasing from 3 to 100 RPS across multiple stages, we measured time to first token (TTFT) across different load levels."}),"\n",(0,t.jsx)(n.p,{children:"As a result, when prefix cache-aware routing was applied, the median TTFT was dramatically reduced from 4.46 seconds to 0.22 seconds under random routing \u2014 approximately 20 times faster. The P75 TTFT improved from 9.29 seconds to 0.72 seconds, and the P90 TTFT improved from 17.96 seconds to 1.11 seconds. These results demonstrate that prefix cache-aware routing significantly reduces TTFT by effectively reusing cached prefixes across multiple inference instances, particularly in scenarios with shared system prompts."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"TTFT (time to first token):"})}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Routing"}),(0,t.jsx)(n.th,{children:"P50 (ms)"}),(0,t.jsx)(n.th,{children:"P75 (ms)"}),(0,t.jsx)(n.th,{children:"P90 (ms)"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Random routing"}),(0,t.jsx)(n.td,{children:"4464.611"}),(0,t.jsx)(n.td,{children:"9292.547"}),(0,t.jsx)(n.td,{children:"17961.767"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Prefix cache-aware routing"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"217.383"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"718.785"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"1106.871"})})]})]})]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(u,{...e})}):u(e)}},7227(e,n,r){r.d(n,{A:()=>a});r(6540);var i=r(4164);const t="tabItem_Ymn6";var s=r(4848);function a(e){var n=e.children,r=e.hidden,a=e.className;return(0,s.jsx)("div",{role:"tabpanel",className:(0,i.A)(t,a),hidden:r,children:n})}},9489(e,n,r){r.d(n,{A:()=>T});var i=r(6540),t=r(4164),s=r(8630),a=r(4245),o=r(6347),c=r(6494),l=r(2814),h=r(5167),d=r(9900);function u(e){var n,r;return null!=(n=null==(r=i.Children.toArray(e).filter(function(e){return"\n"!==e}).map(function(e){if(!e||(0,i.isValidElement)(e)&&((n=e.props)&&"object"==typeof n&&"value"in n))return e;var n;throw new Error("Docusaurus error: Bad <Tabs> child <"+("string"==typeof e.type?e.type:e.type.name)+'>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.')}))?void 0:r.filter(Boolean))?n:[]}function p(e){var n=e.values,r=e.children;return(0,i.useMemo)(function(){var e=null!=n?n:function(e){return u(e).map(function(e){var n=e.props;return{value:n.value,label:n.label,attributes:n.attributes,default:n.default}})}(r);return function(e){var n=(0,h.XI)(e,function(e,n){return e.value===n.value});if(n.length>0)throw new Error('Docusaurus error: Duplicate values "'+n.map(function(e){return e.value}).join(", ")+'" found in <Tabs>. Every value needs to be unique.')}(e),e},[n,r])}function f(e){var n=e.value;return e.tabValues.some(function(e){return e.value===n})}function m(e){var n=e.queryString,r=void 0!==n&&n,t=e.groupId,s=(0,o.W6)(),a=function(e){var n=e.queryString,r=void 0!==n&&n,i=e.groupId;if("string"==typeof r)return r;if(!1===r)return null;if(!0===r&&!i)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return null!=i?i:null}({queryString:r,groupId:t});return[(0,l.aZ)(a),(0,i.useCallback)(function(e){if(a){var n=new URLSearchParams(s.location.search);n.set(a,e),s.replace(Object.assign({},s.location,{search:n.toString()}))}},[a,s])]}function x(e){var n,r,t,s,a=e.defaultValue,o=e.queryString,l=void 0!==o&&o,h=e.groupId,u=p(e),x=(0,i.useState)(function(){return function(e){var n,r=e.defaultValue,i=e.tabValues;if(0===i.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(r){if(!f({value:r,tabValues:i}))throw new Error('Docusaurus error: The <Tabs> has a defaultValue "'+r+'" but none of its children has the corresponding value. Available values are: '+i.map(function(e){return e.value}).join(", ")+". If you intend to show no default tab, use defaultValue={null} instead.");return r}var t=null!=(n=i.find(function(e){return e.default}))?n:i[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:a,tabValues:u})}),g=x[0],v=x[1],j=m({queryString:l,groupId:h}),b=j[0],k=j[1],w=(n=function(e){return e?"docusaurus.tab."+e:null}({groupId:h}.groupId),r=(0,d.Dv)(n),t=r[0],s=r[1],[t,(0,i.useCallback)(function(e){n&&s.set(e)},[n,s])]),y=w[0],T=w[1],P=function(){var e=null!=b?b:y;return f({value:e,tabValues:u})?e:null}();return(0,c.A)(function(){P&&v(P)},[P]),{selectedValue:g,selectValue:(0,i.useCallback)(function(e){if(!f({value:e,tabValues:u}))throw new Error("Can't select invalid tab value="+e);v(e),k(e),T(e)},[k,T,u]),tabValues:u}}var g=r(1062);const v="tabList__CuJ",j="tabItem_LNqP";var b=r(4848);function k(e){var n=e.className,r=e.block,i=e.selectedValue,s=e.selectValue,o=e.tabValues,c=[],l=(0,a.a_)().blockElementScrollPositionUntilNextRender,h=function(e){var n=e.currentTarget,r=c.indexOf(n),t=o[r].value;t!==i&&(l(n),s(t))},d=function(e){var n,r=null;switch(e.key){case"Enter":h(e);break;case"ArrowRight":var i,t=c.indexOf(e.currentTarget)+1;r=null!=(i=c[t])?i:c[0];break;case"ArrowLeft":var s,a=c.indexOf(e.currentTarget)-1;r=null!=(s=c[a])?s:c[c.length-1]}null==(n=r)||n.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.A)("tabs",{"tabs--block":r},n),children:o.map(function(e){var n=e.value,r=e.label,s=e.attributes;return(0,b.jsx)("li",Object.assign({role:"tab",tabIndex:i===n?0:-1,"aria-selected":i===n,ref:function(e){c.push(e)},onKeyDown:d,onClick:h},s,{className:(0,t.A)("tabs__item",j,null==s?void 0:s.className,{"tabs__item--active":i===n}),children:null!=r?r:n}),n)})})}function w(e){var n=e.lazy,r=e.children,s=e.selectedValue,a=(Array.isArray(r)?r:[r]).filter(Boolean);if(n){var o=a.find(function(e){return e.props.value===s});return o?(0,i.cloneElement)(o,{className:(0,t.A)("margin-top--md",o.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:a.map(function(e,n){return(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==s})})})}function y(e){var n=x(e);return(0,b.jsxs)("div",{className:(0,t.A)(s.G.tabs.container,"tabs-container",v),children:[(0,b.jsx)(k,Object.assign({},n,e)),(0,b.jsx)(w,Object.assign({},n,e))]})}function T(e){var n=(0,g.A)();return(0,b.jsx)(y,Object.assign({},e,{children:u(e.children)}),String(n))}},8453(e,n,r){r.d(n,{R:()=>a,x:()=>o});var i=r(6540);const t={},s=i.createContext(t);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);