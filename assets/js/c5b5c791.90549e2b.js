"use strict";(self.webpackChunkmif_docs=self.webpackChunkmif_docs||[]).push([[208],{7852(e,n,t){t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>m,frontMatter:()=>o,metadata:()=>r,toc:()=>u});const r=JSON.parse('{"id":"benchmarking/deepseek_r1_671b_on_amd_mi300x_gpus_maximum_throughput","title":"DeepSeek R1 671B on AMD MI300X GPUs: Maximum throughput","description":"This article presents the performance evaluation method and results of DeepSeek R1 671B inference on 5x AMD MI300X servers (40 GPUs in total).","source":"@site/versioned_docs/version-v0.0.0/benchmarking/deepseek_r1_671b_on_amd_mi300x_gpus_maximum_throughput.md","sourceDirName":"benchmarking","slug":"/benchmarking/deepseek_r1_671b_on_amd_mi300x_gpus_maximum_throughput","permalink":"/benchmarking/deepseek_r1_671b_on_amd_mi300x_gpus_maximum_throughput","draft":false,"unlisted":false,"tags":[],"version":"v0.0.0","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"DeepSeek R1 671B on AMD MI300X GPUs: Maximum throughput"},"sidebar":"docs","previous":{"title":"Benchmarking","permalink":"/benchmarking"},"next":{"title":"Performance with prefix cache- and load-aware routing","permalink":"/benchmarking/more_benchmarking_for_deepseek_r1_671b_on_amd_mi300x_gpus/performance_with_prefix_cache_and_load_aware_routing"}}');var s=t(4848),i=t(8453),a=t(9489),l=t(7227);const o={sidebar_position:1,title:"DeepSeek R1 671B on AMD MI300X GPUs: Maximum throughput"},c="DeepSeek R1 671B on AMD MI300X GPUs: Maximum throughput",d={},u=[{value:"Overview",id:"overview",level:2},{value:"Target environment and configuration",id:"target-environment-and-configuration",level:2},{value:"Deployment",id:"deployment",level:2},{value:"Benchmarking method",id:"benchmarking-method",level:2},{value:"Experimental results",id:"experimental-results",level:2},{value:"Appendix",id:"appendix",level:2},{value:"Experimental results for ISL=2,000 and OSL=100",id:"experimental-results-for-isl2000-and-osl100",level:3}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"deepseek-r1-671b-on-amd-mi300x-gpus-maximum-throughput",children:"DeepSeek R1 671B on AMD MI300X GPUs: Maximum throughput"})}),"\n",(0,s.jsxs)(n.p,{children:["This article presents the performance evaluation method and results of ",(0,s.jsx)(n.strong,{children:"DeepSeek R1 671B"})," inference on 5x AMD MI300X servers (40 GPUs in total)."]}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"The purpose of this benchmarking is to measure the maximum throughput (output tokens/sec) achievable when running distributed inference of the DeepSeek R1 671B model on a 5-node AMD MI300X GPU cluster. This metric directly determines the cost efficiency of inference service (tokens/$). This benchmarking demonstrates three key points:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["We built a distributed inference system operating at the AMD GPU cluster level ",(0,s.jsx)(n.strong,{children:"in real deployments"}),", which efficiently handles high-concurrency requests via prefill-decode disaggregation and expert parallelism."]}),"\n",(0,s.jsx)(n.li,{children:"MoAI Inference Framework delivers industry-leading throughput on AMD MI300X GPU clusters, which enables lower cost-per-token ($/token) configurations."}),"\n",(0,s.jsx)(n.li,{children:"MoAI Inference Framework achieves throughput on AMD MI300X GPU clusters that is on par with what is attainable on NVIDIA H100 GPU clusters."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The experimental methodology was largely designed by referring to the following report from the SGLang team, which measures the performance of PD disaggregation and expert parallelism on an NVIDIA H100 GPU cluster. The key difference is that, while the SGLang team measures prefill-only and decode-only performance separately, our benchmarking integrates prefill and decode instances and measures performance in an end-to-end inference environment, which more accurately reflects real-world achievable performance."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Reference: ",(0,s.jsx)(n.a,{href:"https://lmsys.org/blog/2025-05-05-large-scale-ep/",children:"Deploying DeepSeek with PD Disaggregation and Large-Scale Expert Parallelism on 96 H100 GPUs"})]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"target-environment-and-configuration",children:"Target environment and configuration"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Item"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"GPU servers"}),(0,s.jsx)(n.td,{children:"5x servers, each equipped with 8x AMD MI300X GPUs"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Networking"}),(0,s.jsx)(n.td,{children:"InfiniBand HDR"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Inference engine"}),(0,s.jsx)(n.td,{children:"Moreh vLLM (0.11.0rc2.moreh20251212)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"deepseek-ai/DeepSeek-R1"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"PD disaggregation"}),(0,s.jsx)(n.td,{children:"2x prefill, 3x decode instances"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Parallelization"}),(0,s.jsx)(n.td,{children:"EP=8 + DP=8"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"The specifications of each GPU server are as follows:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"CPU: 2x AMD EPYC 9474F 48-core 3.6 GHz"}),"\n",(0,s.jsx)(n.li,{children:"Main memory: 2,304 GB"}),"\n",(0,s.jsx)(n.li,{children:"GPU: 8x AMD Instinct MI300X OAM GPU 192 GB"}),"\n",(0,s.jsx)(n.li,{children:"Server: Gigabyte G593-ZX1-AAX1"}),"\n",(0,s.jsx)(n.li,{children:"Operating system: Ubuntu 22.04.4 LTS"}),"\n",(0,s.jsx)(n.li,{children:"ROCm version: 6.4.1"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"deployment",children:"Deployment"}),"\n",(0,s.jsxs)(n.p,{children:["Please make sure to install all ",(0,s.jsx)(n.a,{href:"/getting_started/prerequisites",children:"prerequisites"})," before starting this benchmarking. Also, please refer to the ",(0,s.jsx)(n.a,{href:"/getting_started/quickstart",children:"quickstart"})," to understand how to run the MoAI Inference Framework."]}),"\n",(0,s.jsxs)(n.p,{children:["In this benchmarking, you need to deploy the ",(0,s.jsx)(n.strong,{children:"Istio"})," gateway, the ",(0,s.jsx)(n.strong,{children:"Heimdall"})," scheduler configured to specify the basic routing strategy for PD disaggregation, and the ",(0,s.jsx)(n.strong,{children:"Odin"})," inference service configured to run two prefill instances and three decode instances across five GPU servers using optimized settings."]}),"\n",(0,s.jsxs)(n.p,{children:["First, you need to have a namespace for deploying and running the components of the MoAI Inference Framework. In this guide, we assume the namespace is named ",(0,s.jsx)(n.code,{children:"mif"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"kubectl create namespace mif\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"AWS credentials must be configured in this namespace to allow the container images of the MoAI Inference Framework to be downloaded"}),'. For details, refer to the "Amazon ECR token for Moreh\'s container image repository" section in the ',(0,s.jsx)(n.a,{href:"/getting_started/prerequisites",children:"prerequisites"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["Then, you can use the following configuration files for the components. Click to view their contents. ",(0,s.jsxs)(n.strong,{children:["You must store the DeepSeek-R1 model checkpoint on the host of every worker node and specify its path on line 19 of the ",(0,s.jsx)(n.code,{children:"inference-service-values.yaml"})," file"]}),". This path will be mounted to ",(0,s.jsx)(n.code,{children:"/app/model/DeepSeek-R1"})," inside the pod and used to run the Moreh vLLM server."]}),"\n",(0,s.jsxs)(a.A,{children:[(0,s.jsx)(l.A,{value:"istio",label:"Istio gateway configuration (gateway.yaml)",default:!0,children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:"gateway.yaml",children:"apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mif-gateway-infrastructure\n  namespace: mif\ndata:\n  service: |\n    spec:\n      type: ClusterIP\n  deployment: |\n    spec:\n      template:\n        metadata:\n          annotations:\n            proxy.istio.io/config: |\n              accessLogFile: /dev/stdout\n              accessLogEncoding: JSON\n        spec:\n          containers:\n            - name: istio-proxy\n              resources:\n                limits: null\n\n---\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: mif\n  namespace: mif\nspec:\n  gatewayClassName: istio\n  infrastructure:\n    parametersRef:\n      group: ''\n      kind: ConfigMap\n      name: mif-gateway-infrastructure\n  listeners:\n    - name: http\n      protocol: HTTP\n      port: 80\n      allowedRoutes:\n        namespaces:\n          from: All\n"})})}),(0,s.jsx)(l.A,{value:"heimdall",label:"Heimdall scheduler configuration (heimdall-values.yaml)",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:"heimdall-values.yaml",children:"global:\n  imagePullSecrets:\n    - name: moreh-registry\n\nconfig:\n  apiVersion: inference.networking.x-k8s.io/v1alpha1\n  kind: EndpointPickerConfig\n  plugins:\n    - type: pd-profile-handler\n    - type: prefill-filter\n    - type: decode-filter\n    - type: active-request-scorer\n      parameters:\n        requestTimeout: '20m'\n    - type: max-score-picker\n    - type: random-picker\n  schedulingProfiles:\n    - name: prefill\n      plugins:\n        - pluginRef: prefill-filter\n        - pluginRef: active-request-scorer\n          weight: 1\n        - pluginRef: max-score-picker\n    - name: decode\n      plugins:\n        - pluginRef: decode-filter\n        - pluginRef: active-request-scorer\n          weight: 1\n        - pluginRef: max-score-picker\n\ntolerations:\n  - key: amd.com/gpu\n    operator: Exists\n    effect: NoSchedule\n\ngateway:\n  name: mif\n  gatewayClassName: istio\n"})})}),(0,s.jsx)(l.A,{value:"odin",label:"Odin inference service configuration (inference-service-values.yaml)",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:"inference-service-values.yaml {19}",children:"global:\n  imagePullSecrets:\n    - name: moreh-registry\n\nextraVolumeMounts:\n  - name: shm\n    mountPath: /dev/shm\n  - name: dsr1\n    mountPath: /app/model/DeepSeek-R1\n    readOnly: false\n\nextraVolumes:\n  - name: shm\n    emptyDir:\n      medium: Memory\n      sizeLimit: 16Gi\n  - name: dsr1\n    hostPath:\n      path: /path/to/deepseek-r1\n\n_common: &common\n  image:\n    repository: 255250787067.dkr.ecr.ap-northeast-2.amazonaws.com/quickstart/moreh-vllm\n    tag: vllm_251212\n\n  updateStrategy:\n    type: Recreate\n\n  resources:\n    requests: &resources\n      amd.com/gpu: '8'\n      mellanox/hca: '1'\n    limits: *resources\n\n  tolerations:\n    - key: amd.com/gpu\n      operator: Exists\n      effect: NoSchedule\n\n  podMonitor:\n    labels:\n      prometheus-stack/prometheus: enabled\n\nextraEnvVars:\n  - name: UCX_IB_PCI_RELAXED_ORDERING\n    value: 'on'\n  - name: UCX_TLS\n    value: rocm_copy,rocm_ipc,self,sm,rc_x\n  - name: NCCL_IB_PCI_RELAXED_ORDERING\n    value: '1'\n  - name: NCCL_NET_GDR_LEVEL\n    value: '3'\n  - name: NCCL_MIN_NCHANNELS\n    value: '112'\n  - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS\n    value: '1'\n  - name: VLLM_ROCM_USE_AITER\n    value: '1'\n  - name: VLLM_ROCM_USE_AITER_FP8BMM\n    value: '0'\n  - name: VLLM_ALL2ALL_BACKEND\n    value: 'mori'\n  - name: VLLM_HTTP_TIMEOUT_KEEP_ALIVE\n    value: '1000000000'\n  - name: VLLM_NIXL_ABORT_REQUEST_TIMEOUT\n    value: '1000000000'\n  - name: VLLM_NIXL_SIDE_CHANNEL_HOST\n    valueFrom:\n      fieldRef:\n        fieldPath: status.podIP\n  - name: VLLM_LOG_STATS_INTERVAL\n    value: '10'\n  - name: VLLM_SERVERSIDE_LOGGING\n    value: '1'\n  - name: VLLM_SERVERSIDE_LOG_INTERVAL\n    value: '10'\n  - name: GLOO_SOCKET_IFNAME\n    value: ''\n  - name: NCCL_SOCKET_IFNAME\n    value: ''\n  - name: TP_SOCKET_IFNAME\n    value: ''\n\nproxy:\n  image:\n    tag: c8abd08\n\ndecode:\n  replicas: 3\n\n  <<: *common\n\n  parallelism:\n    data: 8\n\n  extraArgs:\n    - /app/model/DeepSeek-R1\n    - --served-model-name\n    - deepseek-ai/DeepSeek-R1\n    - --trust-remote-code\n    - --no-enable-prefix-caching\n    - --no-enable-chunked-prefill\n    - --enforce-eager\n    - --tensor-parallel-size\n    - '1'\n    - --enable-expert-parallel\n    - --max-model-len\n    - '8192'\n    - --max-num-seqs\n    - '2048'\n    - --kv-cache-dtype\n    - fp8_e4m3\n    - --quantization\n    - ds_fp8_per_token\n    - --block-size\n    - '16'\n    - --kv-transfer-config\n    - '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}'\n    - --disable-uvicorn-access-log\n    - --no-enable-log-requests\n    - --disable-log-stats\n    - --max-num-batched-token\n    - '16384'\n    - --gpu-memory-utilization\n    - '0.92'\n\n  extraEnvVars:\n    - name: VLLM_MOE_DP_CHUNK_SIZE\n      value: '512'\n    - name: VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\n      value: '512'\n    - name: VLLM_MORI_DISPATCH_BLK_NO\n      value: '128'\n    - name: VLLM_MORI_DISPATCH_WARP_PER_BLK\n      value: '16'\n    - name: VLLM_MORI_COMBINE_BLK_NO\n      value: '64'\n    - name: VLLM_MORI_COMBINE_WARP_PER_BLK\n      value: '8'\n    - name: VLLM_IS_DECODE_WORKER\n      value: 'decode'\n\nprefill:\n  replicas: 2\n\n  <<: *common\n  command:\n    - /bin/bash\n    - -lc\n\n  args:\n    - |\n      vllm serve \"/app/model/DeepSeek-R1\" \\\n        --served-model-name deepseek-ai/DeepSeek-R1 \\\n        --port 8000 \\\n        --trust-remote-code \\\n        --tensor-parallel-size 1 \\\n        --data-parallel-size 8 \\\n        --enable-expert-parallel \\\n        --no-enable-prefix-caching \\\n        --no-enable-chunked-prefill \\\n        --enforce-eager \\\n        --max-model-len 8192 \\\n        --max-num-seqs 2048 \\\n        --kv-cache-dtype fp8_e4m3 \\\n        --quantization ds_fp8_per_token \\\n        --block-size 16 \\\n        --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}' \\\n        --disable-uvicorn-access-log \\\n        --no-enable-log-requests \\\n        --disable-log-stats \\\n        --max-num-batched-token 64000 \\\n        --gpu-memory-utilization 0.92\n\n  extraEnvVars:\n    - name: VLLM_MOE_DP_CHUNK_SIZE\n      value: '4096'\n    - name: VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\n      value: '128'\n    - name: VLLM_MORI_DISPATCH_BLK_NO\n      value: '128'\n    - name: VLLM_MORI_DISPATCH_WARP_PER_BLK\n      value: '16'\n    - name: VLLM_MORI_COMBINE_BLK_NO\n      value: '64'\n    - name: VLLM_MORI_COMBINE_WARP_PER_BLK\n      value: '4'\n    - name: VLLM_IS_DECODE_WORKER\n      value: 'prefill'\n"})})})]}),"\n",(0,s.jsx)(n.p,{children:"Run the following commands to deploy and run the components."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Istio gateway:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"kubectl apply -f gateway.yaml\nkubectl get pod -n mif -l gateway.networking.k8s.io/gateway-name=mif\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",metastring:"Expected output",children:"NAME                         READY   STATUS    RESTARTS   AGE\nmif-istio-584474ddd9-rt9p9   1/1     Running   0          163m\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Heimdall scheduler:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"helm upgrade -i heimdall moreh/heimdall \\\n    --version v0.5.0 \\\n    -n mif \\\n    -f heimdall-values.yaml\nkubectl get all -n mif -l app.kubernetes.io/instance=heimdall\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",metastring:"Expected output",children:"NAME                            READY   STATUS    RESTARTS   AGE\npod/heimdall-5576d4f48b-bgn4c   1/1     Running   0          3d1h\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Odin inference service:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"helm upgrade -i inference-service moreh/inference-service \\\n    --version v0.6.1 \\\n    -n mif \\\n    -f inference-service-values.yaml\nkubectl get all -n mif -l app.kubernetes.io/instance=inference-service\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",metastring:"Expected output",children:"NAME                                             READY   STATUS    RESTARTS   AGE\npod/inference-service-decode-0-1                 1/1     Running   0          95s\npod/inference-service-decode-0-2                 1/1     Running   0          95s\npod/inference-service-decode-0-3                 1/1     Running   0          95s\npod/inference-service-decode-0-4                 1/1     Running   0          95s\npod/inference-service-decode-0-5                 1/1     Running   0          95s\npod/inference-service-decode-0-6                 1/1     Running   0          95s\npod/inference-service-decode-0-7                 1/1     Running   0          95s\npod/inference-service-decode-0-8                 1/1     Running   0          95s\npod/inference-service-decode-1-1                 1/1     Running   0          103s\npod/inference-service-decode-1-2                 1/1     Running   0          103s\npod/inference-service-decode-1-3                 1/1     Running   0          103s\npod/inference-service-decode-1-4                 1/1     Running   0          103s\npod/inference-service-decode-1-5                 1/1     Running   0          103s\npod/inference-service-decode-1-6                 1/1     Running   0          103s\npod/inference-service-decode-1-7                 1/1     Running   0          103s\npod/inference-service-decode-1-8                 1/1     Running   0          103s\npod/inference-service-decode-2-1                 1/1     Running   0          110s\npod/inference-service-decode-2-2                 1/1     Running   0          110s\npod/inference-service-decode-2-3                 1/1     Running   0          110s\npod/inference-service-decode-2-4                 1/1     Running   0          110s\npod/inference-service-decode-2-5                 1/1     Running   0          110s\npod/inference-service-decode-2-6                 1/1     Running   0          110s\npod/inference-service-decode-2-7                 1/1     Running   0          110s\npod/inference-service-decode-2-8                 1/1     Running   0          110s\npod/inference-service-prefill-648bfd7bd6-cthnv   1/1     Running   0          3m38s\npod/inference-service-prefill-648bfd7bd6-lz6km   1/1     Running   0          3m38s\n\nNAME                                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/inference-service-prefill   2/2     2            2           3m38s\n\nNAME                                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/inference-service-prefill-648bfd7bd6   2         2         2       3m38s\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"benchmarking-method",children:"Benchmarking method"}),"\n",(0,s.jsx)(n.p,{children:"We follow a commonly used approach for measuring the computational performance of inference servers. Multiple concurrent users send requests at a specific request-per-second (RPS) rate, each with a fixed input sequence length and output sequence length. The concurrency and RPS are determined empirically as high as possible within the limits of GPU memory capacity and without allowing requests to accumulate in the request queue of vLLM instances. We measure the response times of these requests and compute output tokens per second, total tokens per second, time to first token, and inter-token latency (also known as time per output token)."}),"\n",(0,s.jsxs)(n.p,{children:["We use the ",(0,s.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/cli/bench/serve/",children:"vLLM bench serve"})," tool to conduct experiments of this kind. However, this tool was originally designed to measure the performance of a single-GPU server, and several aspects of it are insufficient for evaluating the levels of throughput observed in our experiments \u2014 tens of thousands of tokens per second. Therefore, we implemented three additional features in the vLLM bench serve tool bundled with Moreh vLLM, to correctly measure performance in a distributed inference environment with very high throughput. See the modified version ",(0,s.jsx)(n.a,{href:"https://github.com/moreh-dev/vllm/tree/main/vllm/benchmarks",children:"here"}),"."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"--warmup-time"}),", ",(0,s.jsx)(n.code,{children:"--cooldown-time"}),": At the beginning of the experiment, before enough requests have accumulated, and near the end of the experiment, as computation winds down, the GPUs are not fully utilized. To reliably measure the maximum throughput achievable by the inference system, we enabled the tool to exclude requests from the initial (warm-up) and final (cool-down) phases from the performance measurement."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"--max-connections-per-worker"}),": We made the response times of individual requests be recorded across multiple threads; otherwise, information for some requests may be lost."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"--sharegpt-input-len"}),", ",(0,s.jsx)(n.code,{children:"--sharegpt-output-len"}),", ",(0,s.jsx)(n.code,{children:"--gutenberg-input-len"}),", ",(0,s.jsx)(n.code,{children:"--gutenberg-output-len"}),": To accurately measure the effect of EP load balancing, we used substrings of meaningful text from a real dataset, cut to the desired input sequence length, as prompts rather than meaningless random strings."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["In this benchmarking, we evaluate three different input/output sequence lengths (512/512, 1000/1000, and 2000/2000) and two different datasets (",(0,s.jsx)(n.a,{href:"https://www.kaggle.com/datasets/roschildrui/sharegpt-v3-unfiltered-cleaned-split",children:"ShareGPT"})," and ",(0,s.jsx)(n.a,{href:"https://huggingface.co/datasets/manu/project_gutenberg",children:"Gutenberg"}),"). To launch a new Moreh vLLM pod in a Kubernetes cluster, first create a ",(0,s.jsx)(n.code,{children:"benchmarking-client.yaml"})," file as follows. ",(0,s.jsx)(n.strong,{children:"Please modify the following items to match your system."})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"On lines 5, 15, 26 and 28, specify the name of the Kubernetes worker node on which the benchmarking pod will run."})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsxs)(n.strong,{children:["Store the ",(0,s.jsx)(n.code,{children:"ShareGPT_V3_unfiltered_cleaned_split.json"})," file and the ",(0,s.jsx)(n.code,{children:"project_gutenberg"})," directory on the host filesystem of that node, and specify their paths on lines 44 and 47."]})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:"benchmarking-client.yaml {5,15,26,28,44,47}",children:"apiVersion: v1\nkind: Pod\nmetadata:\n  annotations: {}\n  name: <clientHostname>\n  namespace: mif\nspec:\n  containers:\n    - args:\n        - infinity\n      command:\n        - sleep\n      image: 255250787067.dkr.ecr.ap-northeast-2.amazonaws.com/quickstart/moreh-vllm:vllm_251212\n      imagePullPolicy: IfNotPresent\n      name: <clientHostname>\n      resources: {}\n      volumeMounts:\n        - name: sharegpt-dataset\n          mountPath: '/app/dataset/ShareGPT_V3_unfiltered_cleaned_split.json'\n        - name: gutenberg-dataset\n          mountPath: '/app/dataset/project_gutenberg'\n      securityContext:\n        privileged: true\n  dnsPolicy: ClusterFirst\n  enableServiceLinks: true\n  nodeName: <clientHostname>\n  nodeSelector:\n    kubernetes.io/hostname: <clientHostname>\n  preemptionPolicy: PreemptLowerPriority\n  priority: 0\n  restartPolicy: Always\n  schedulerName: default-scheduler\n  securityContext: {}\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n  tolerations:\n    - effect: NoSchedule\n      key: amd.com/gpu\n      operator: Exists\n  volumes:\n    - name: sharegpt-dataset\n      hostPath:\n        path: /path/to/ShareGPT_V3_unfiltered_cleaned_split.json\n    - name: gutenberg-dataset\n      hostPath:\n        path: /path/to/project_gutenberg\n"})}),"\n",(0,s.jsx)(n.p,{children:"Run the following command to start the pod."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"kubectl -n mif apply -f benchmarking-client.yaml\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Inside the pod, you can run ",(0,s.jsx)(n.code,{children:"vllm bench serve"})," as follows. This is an example that uses an input sequence length of 512, an output sequence length of 512, and the ShareGPT dataset. ",(0,s.jsx)(n.strong,{children:"You may need to modify the host on line 6 depending on your Istio gateway address"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",metastring:"{6}",children:'vllm bench serve \\\n  --backend vllm \\\n  --model "deepseek-ai/DeepSeek-R1" \\\n  --metric-percentiles "1,10,25,50,75,90" \\\n  --percentile-metrics "itl,tps,ttft" \\\n  --host "mif-istio.mif.svc.cluster.local" \\\n  --port 80 \\\n  --num-prompts 32400 \\\n  --max-concurrency 10800 \\\n  --request-rate 140 \\\n  --ignore-eos \\\n  --ready-check-timeout-sec 0 \\\n  --max-connections-per-worker 1296 \\\n  --warmup-time 120.0 \\\n  --cooldown-time 70.0 \\\n  --dataset-name sharegpt \\\n  --dataset-path /app/dataset/ShareGPT_V3_unfiltered_cleaned_split.json \\\n  --sharegpt-input-len 512 \\\n  --sharegpt-output-len 512\n'})}),"\n",(0,s.jsx)(n.p,{children:"The followings are the actual commands used to run each experiment. You can click to view each command. For each experiment, the warm-up time and cool-down time were adjusted appropriately."}),"\n",(0,s.jsx)(n.p,{children:"==- (512, 512, ShareGPT)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'vllm bench serve \\\n  --backend vllm \\\n  --model "deepseek-ai/DeepSeek-R1" \\\n  --metric-percentiles "1,10,25,50,75,90" \\\n  --percentile-metrics "itl,tps,ttft" \\\n  --host "mif-istio.mif.svc.cluster.local" \\\n  --port 80 \\\n  --num-prompts 32400 \\\n  --max-concurrency 10800 \\\n  --request-rate 140 \\\n  --ignore-eos \\\n  --ready-check-timeout-sec 0 \\\n  --max-connections-per-worker 1296 \\\n  --warmup-time 120.0 \\\n  --cooldown-time 70.0 \\\n  --dataset-name sharegpt \\\n  --dataset-path /app/dataset/ShareGPT_V3_unfiltered_cleaned_split.json \\\n  --sharegpt-input-len 512 \\\n  --sharegpt-output-len 512\n'})}),"\n",(0,s.jsx)(n.p,{children:"==- (512, 512, Gutenberg)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'vllm bench serve \\\n  --backend vllm \\\n  --model "deepseek-ai/DeepSeek-R1" \\\n  --metric-percentiles "1,10,25,50,75,90" \\\n  --percentile-metrics "itl,tps,ttft" \\\n  --host "mif-istio.mif.svc.cluster.local" \\\n  --port 80 \\\n  --num-prompts 32400 \\\n  --max-concurrency 10800 \\\n  --request-rate 140 \\\n  --ignore-eos \\\n  --ready-check-timeout-sec 0 \\\n  --max-connections-per-worker 1296 \\\n  --warmup-time 130.0 \\\n  --cooldown-time 70.0 \\\n  --dataset-name gutenberg \\\n  --dataset-path /app/dataset/project_gutenberg \\\n  --gutenberg-input-len 512 \\\n  --gutenberg-output-len 512\n'})}),"\n",(0,s.jsx)(n.p,{children:"==- (1000, 1000, ShareGPT)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'vllm bench serve \\\n  --backend vllm \\\n  --model "deepseek-ai/DeepSeek-R1" \\\n  --metric-percentiles "1,10,25,50,75,90" \\\n  --percentile-metrics "itl,tps,ttft" \\\n  --host "mif-istio.mif.svc.cluster.local" \\\n  --port 80 \\\n  --num-prompts 32400 \\\n  --max-concurrency 10800 \\\n  --request-rate 80 \\\n  --ignore-eos \\\n  --ready-check-timeout-sec 0 \\\n  --max-connections-per-worker 1296 \\\n  --warmup-time 140.0 \\\n  --cooldown-time 110.0 \\\n  --dataset-name sharegpt \\\n  --dataset-path /app/dataset/ShareGPT_V3_unfiltered_cleaned_split.json \\\n  --sharegpt-input-len 1000 \\\n  --sharegpt-output-len 1000\n'})}),"\n",(0,s.jsx)(n.p,{children:"==- (1000, 1000, Gutenberg)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'vllm bench serve \\\n  --backend vllm \\\n  --model "deepseek-ai/DeepSeek-R1" \\\n  --metric-percentiles "10,25,50,75,90" \\\n  --percentile-metrics "itl,tps,ttft" \\\n  --host "mif-istio.mif.svc.cluster.local" \\\n  --port 80 \\\n  --num-prompts 32400 \\\n  --max-concurrency 10800 \\\n  --request-rate 80 \\\n  --ignore-eos \\\n  --ready-check-timeout-sec 0 \\\n  --max-connections-per-worker 1296 \\\n  --warmup-time 150.0 \\\n  --cooldown-time 120.0 \\\n  --dataset-name gutenberg \\\n  --dataset-path /app/dataset/project_gutenberg \\\n  --gutenberg-input-len 1000 \\\n  --gutenberg-output-len 1000\n'})}),"\n",(0,s.jsx)(n.p,{children:"==- (2000, 2000, ShareGPT)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'vllm bench serve \\\n  --backend vllm \\\n  --model "deepseek-ai/DeepSeek-R1" \\\n  --metric-percentiles "1,10,25,50,75,90" \\\n  --percentile-metrics "itl,tps,ttft" \\\n  --host "mif-istio.mif.svc.cluster.local" \\\n  --port 80 \\\n  --num-prompts 32400 \\\n  --max-concurrency 10800 \\\n  --request-rate 48 \\\n  --ignore-eos \\\n  --ready-check-timeout-sec 0 \\\n  --max-connections-per-worker 1296 \\\n  --warmup-time 250.0 \\\n  --cooldown-time 290.0 \\\n  --dataset-name sharegpt \\\n  --dataset-path /app/dataset/ShareGPT_V3_unfiltered_cleaned_split.json \\\n  --sharegpt-input-len 2000 \\\n  --sharegpt-output-len 2000\n'})}),"\n",(0,s.jsx)(n.p,{children:"==- (2000, 2000, Gutenberg)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'vllm bench serve \\\n  --backend vllm \\\n  --model "deepseek-ai/DeepSeek-R1" \\\n  --metric-percentiles "1,10,25,50,75,90" \\\n  --percentile-metrics "itl,tps,ttft" \\\n  --host "mif-istio.mif.svc.cluster.local" \\\n  --port 80 \\\n  --num-prompts 32400 \\\n  --max-concurrency 10800 \\\n  --request-rate 60 \\\n  --ignore-eos \\\n  --ready-check-timeout-sec 0 \\\n  --max-connections-per-worker 1296 \\\n  --warmup-time 260.0 \\\n  --cooldown-time 240.0 \\\n  --dataset-name gutenberg \\\n  --dataset-path /app/dataset/project_gutenberg \\\n  --gutenberg-input-len 2000 \\\n  --gutenberg-output-len 2000\n'})}),"\n",(0,s.jsx)(n.p,{children:"==="}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"experimental-results",children:"Experimental results"}),"\n",(0,s.jsxs)(n.p,{children:["The results are as follows. As mentioned earlier, the concurrency and RPS values were determined empirically and may vary depending on the system scale (the number of GPU nodes). We achieved 50,892-66,194 output tokens/sec across various configurations, which corresponds to ",(0,s.jsx)(n.strong,{children:"17,000-22,000 tokens/sec per decode node"}),"."]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Input sequence length"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Output sequence length"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Dataset"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"(Concurrency, RPS)"}),(0,s.jsx)(n.th,{style:{textAlign:"right"},children:"Output tokens/sec"}),(0,s.jsx)(n.th,{style:{textAlign:"right"},children:"Output tokens/sec per decode node"}),(0,s.jsx)(n.th,{style:{textAlign:"right"},children:"Total tokens/sec"}),(0,s.jsx)(n.th,{style:{textAlign:"right"},children:"Mean TTFT (ms)"}),(0,s.jsx)(n.th,{style:{textAlign:"right"},children:"Mean ITL (ms)"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"512"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"512"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"ShareGPT"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"(10800, 140)"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"66,194.80"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:(0,s.jsx)(n.strong,{children:"22,064.93"})}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"85,347.35"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"1,677.87"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"160.33"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"512"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"512"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Gutenberg"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"(10800, 140)"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"64,695.10"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:(0,s.jsx)(n.strong,{children:"21,565.03"})}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"79,432.24"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"1,774.90"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"164.32"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"1000"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"1000"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"ShareGPT"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"(10800, 80)"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"61,828.90"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:(0,s.jsx)(n.strong,{children:"20,609.63"})}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"94,103.87"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"1,802.87"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"172.16"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"1000"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"1000"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Gutenberg"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"(10800, 80)"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"61,418.55"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:(0,s.jsx)(n.strong,{children:"20,472.85"})}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"92,353.63"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"2,149.80"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"173.63"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"2000"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"2000"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"ShareGPT"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"(10800, 48)"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"51,187.87"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:(0,s.jsx)(n.strong,{children:"17,062.62"})}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"77,775.33"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"2,567.87"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"208.59"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"2000"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"2000"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Gutenberg"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"(10800, 60)"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"50,892.65"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:(0,s.jsx)(n.strong,{children:"16,964.22"})}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"76,739.86"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"5,586.34"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"208.76"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"Click to view raw benchmarking logs."}),"\n",(0,s.jsx)(n.p,{children:"==- (512, 512, ShareGPT)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"=============Serving Benchmark Result=============\nNumber of worker processes:              25\nSuccessful requests:                     4333\nMaximum request concurrency:             10800\nRequest rate configured (RPS):           140.00\nWarm-up Time:                            120.0\nCool-down Time:                          70.0\nBenchmark duration (s):                  115.83\nTotal input tokens:                      2218496\nTotal generated tokens:                  7667539\nOutput token throughput (tok/s):         66194.80\nTotal Token throughput (tok/s):          85347.35\n---------------Time to First Token----------------\nMean TTFT (ms):                          1677.87\nMedian TTFT (ms):                        1720.09\nP1 TTFT (ms):                            673.72\nP10 TTFT (ms):                           944.53\nP25 TTFT (ms):                           1177.09\nP50 TTFT (ms):                           1720.09\nP75 TTFT (ms):                           2117.51\nP90 TTFT (ms):                           2387.32\n---------------Inter-token Latency----------------\nMean ITL (ms):                           160.33\nMedian ITL (ms):                         158.98\nP1 ITL (ms):                             105.28\nP10 ITL (ms):                            139.59\nP25 ITL (ms):                            151.42\nP50 ITL (ms):                            158.98\nP75 ITL (ms):                            169.65\nP90 ITL (ms):                            184.02\n==================================================\n"})}),"\n",(0,s.jsx)(n.p,{children:"==- (512, 512, Gutenberg)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"=============Serving Benchmark Result=============\nNumber of worker processes:              25\nSuccessful requests:                     3186\nMaximum request concurrency:             10800\nRequest rate configured (RPS):           140.00\nWarm-up Time:                            130.0\nCool-down Time:                          70.0\nBenchmark duration (s):                  110.69\nTotal input tokens:                      1631232\nTotal generated tokens:                  7161008\nOutput token throughput (tok/s):         64695.10\nTotal Token throughput (tok/s):          79432.24\n---------------Time to First Token----------------\nMean TTFT (ms):                          1774.90\nMedian TTFT (ms):                        1795.76\nP1 TTFT (ms):                            775.35\nP10 TTFT (ms):                           877.82\nP25 TTFT (ms):                           1124.76\nP50 TTFT (ms):                           1795.76\nP75 TTFT (ms):                           2296.75\nP90 TTFT (ms):                           2685.10\n---------------Inter-token Latency----------------\nMean ITL (ms):                           164.32\nMedian ITL (ms):                         162.24\nP1 ITL (ms):                             106.99\nP10 ITL (ms):                            142.68\nP25 ITL (ms):                            154.22\nP50 ITL (ms):                            162.24\nP75 ITL (ms):                            174.62\nP90 ITL (ms):                            189.84\n==================================================\n"})}),"\n",(0,s.jsx)(n.p,{children:"==- (1000, 1000, ShareGPT)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"=============Serving Benchmark Result=============\nNumber of worker processes:              25\nSuccessful requests:                     11856\nMaximum request concurrency:             10800\nRequest rate configured (RPS):           80.00\nWarm-up Time:                            140.0\nCool-down Time:                          110.0\nBenchmark duration (s):                  367.34\nTotal input tokens:                      11856000\nTotal generated tokens:                  22712445\nOutput token throughput (tok/s):         61828.90\nTotal Token throughput (tok/s):          94103.87\n---------------Time to First Token----------------\nMean TTFT (ms):                          1802.87\nMedian TTFT (ms):                        1411.86\nP1 TTFT (ms):                            724.34\nP10 TTFT (ms):                           987.73\nP25 TTFT (ms):                           1059.29\nP50 TTFT (ms):                           1411.86\nP75 TTFT (ms):                           2221.47\nP90 TTFT (ms):                           3434.72\n---------------Inter-token Latency----------------\nMean ITL (ms):                           172.16\nMedian ITL (ms):                         169.69\nP1 ITL (ms):                             120.22\nP10 ITL (ms):                            157.36\nP25 ITL (ms):                            164.97\nP50 ITL (ms):                            169.69\nP75 ITL (ms):                            177.89\nP90 ITL (ms):                            192.85\n==================================================\n"})}),"\n",(0,s.jsx)(n.p,{children:"==- (1000, 1000, Gutenberg)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"=============Serving Benchmark Result=============\nNumber of worker processes:              25\nSuccessful requests:                     10931\nMaximum request concurrency:             10800\nRequest rate configured (RPS):           80.00\nWarm-up Time:                            150.0\nCool-down Time:                          120.0\nBenchmark duration (s):                  353.35\nTotal input tokens:                      10931000\nTotal generated tokens:                  21702425\nOutput token throughput (tok/s):         61418.55\nTotal Token throughput (tok/s):          92353.63\n---------------Time to First Token----------------\nMean TTFT (ms):                          2149.80\nMedian TTFT (ms):                        1910.21\nP10 TTFT (ms):                           1040.06\nP25 TTFT (ms):                           1374.56\nP50 TTFT (ms):                           1910.21\nP75 TTFT (ms):                           2759.64\nP90 TTFT (ms):                           3502.56\n---------------Inter-token Latency----------------\nMean ITL (ms):                           173.63\nMedian ITL (ms):                         171.10\nP10 ITL (ms):                            155.99\nP25 ITL (ms):                            165.99\nP50 ITL (ms):                            171.10\nP75 ITL (ms):                            180.85\nP90 ITL (ms):                            195.95\n==================================================\n"})}),"\n",(0,s.jsx)(n.p,{children:"==- (2000, 2000, ShareGPT)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"=============Serving Benchmark Result=============\nNumber of worker processes:              25\nSuccessful requests:                     11906\nMaximum request concurrency:             10800\nRequest rate configured (RPS):           48.00\nWarm-up Time:                            300.0\nCool-down Time:                          230.0\nBenchmark duration (s):                  895.61\nTotal input tokens:                      23812000\nTotal generated tokens:                  45844389\nOutput token throughput (tok/s):         51187.87\nTotal Token throughput (tok/s):          77775.33\n---------------Time to First Token----------------\nMean TTFT (ms):                          2567.87\nMedian TTFT (ms):                        2538.34\nP1 TTFT (ms):                            971.14\nP10 TTFT (ms):                           1213.06\nP25 TTFT (ms):                           1622.42\nP50 TTFT (ms):                           2538.34\nP75 TTFT (ms):                           3267.10\nP90 TTFT (ms):                           4126.80\n---------------Inter-token Latency----------------\nMean ITL (ms):                           208.59\nMedian ITL (ms):                         201.50\nP1 ITL (ms):                             140.72\nP10 ITL (ms):                            186.91\nP25 ITL (ms):                            195.65\nP50 ITL (ms):                            201.50\nP75 ITL (ms):                            217.87\nP90 ITL (ms):                            243.87\n==================================================\n"})}),"\n",(0,s.jsx)(n.p,{children:"==- (2000, 2000, Gutenberg)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"=============Serving Benchmark Result=============\nNumber of worker processes:              25\nSuccessful requests:                     12254\nMaximum request concurrency:             10800\nRequest rate configured (RPS):           60.00\nWarm-up Time:                            260.0\nCool-down Time:                          240.0\nBenchmark duration (s):                  948.19\nTotal input tokens:                      24508000\nTotal generated tokens:                  48255768\nOutput token throughput (tok/s):         50892.65\nTotal Token throughput (tok/s):          76739.86\n---------------Time to First Token----------------\nMean TTFT (ms):                          5586.34\nMedian TTFT (ms):                        5313.19\nP1 TTFT (ms):                            1017.56\nP10 TTFT (ms):                           1745.51\nP25 TTFT (ms):                           2823.02\nP50 TTFT (ms):                           5313.19\nP75 TTFT (ms):                           7612.50\nP90 TTFT (ms):                           10096.06\n---------------Inter-token Latency----------------\nMean ITL (ms):                           208.76\nMedian ITL (ms):                         201.13\nP1 ITL (ms):                             139.90\nP10 ITL (ms):                            187.16\nP25 ITL (ms):                            195.34\nP50 ITL (ms):                            201.13\nP75 ITL (ms):                            214.94\nP90 ITL (ms):                            245.98\n=================================================\n"})}),"\n",(0,s.jsx)(n.p,{children:"==="}),"\n",(0,s.jsx)(n.p,{children:"The followings are some publicly available performance numbers for comparison."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The SGLang team reported that, on a cluster of 12x H100 nodes (96x GPUs) \u2014 with 3 nodes used for prefill and 9 nodes for decode \u2014 they achieved a throughput of ",(0,s.jsx)(n.strong,{children:"22,300 output tokens/sec"})," per decode node under a configuration with an input sequence length of 2,000 and an output sequence length of 100. Note that this number does not represent end-to-end performance with actual PD disaggregation applied; rather, it measures partial performance with decoding-only execution. (",(0,s.jsx)(n.a,{href:"https://lmsys.org/blog/2025-05-05-large-scale-ep/",children:"Link"}),")"]}),"\n",(0,s.jsxs)(n.li,{children:["DeepSeek reported achieving ",(0,s.jsx)(n.strong,{children:"14,800 tokens/sec"})," per H800 decode node by applying PD disaggregation and expert parallelism. (",(0,s.jsx)(n.a,{href:"https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md",children:"Link"}),")"]}),"\n",(0,s.jsxs)(n.li,{children:["AMD reported achieving ",(0,s.jsx)(n.strong,{children:"up to 14,300 output tokens/sec"})," per MI300X decode node. This result was also measured under decoding-only execution. (",(0,s.jsx)(n.a,{href:"https://rocm.blogs.amd.com/software-tools-optimization/wide-ep-deepseek/README.html",children:"Link"}),")."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"In real production deployments, an appropriate trade-off between throughput and latency (inter-token latency and time to first token) must be chosen according to the service-level objectives (SLOs). As shorter latency targets are pursued, achievable throughput inevitably decreases. Nevertheless, meausring and comparing the maximum achievable throughput before applying SLO constraints is an important step in evaluating infrastructure efficiency. Our next benchmarking will examine how throughput varies across different ITL targets."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"appendix",children:"Appendix"}),"\n",(0,s.jsx)(n.h3,{id:"experimental-results-for-isl2000-and-osl100",children:"Experimental results for ISL=2,000 and OSL=100"}),"\n",(0,s.jsx)(n.p,{children:"An input sequence length of 2,000 and an output sequence length of 100 were first used by the SGLang team for their PD+EP performance evaluation. Since then, this configuration has been widely adopted to evaluate PD+EP performance of DeepSeek R1."}),"\n",(0,s.jsx)(n.p,{children:"First, please note that this configuration was proposed to measure prefill and decode throughput separately. Under the assumption that the input length is always 20x longer, a real inference system would require ~10x more prefill instances than decode instances. (In practice, real usage patterns differ from this assumption, and the number of decode instances typically exceeds that of prefill instances.) In small clusters, prefill inevitably becomes the overall performance bottleneck, making it impossible to accurately measuring the output tokens/sec that the GPU servers can actually deliver."}),"\n",(0,s.jsxs)(n.p,{children:["Despite this, by enabling prefix caching and having input sequences share a fixed set of prompts, we can design a scenario in which the prefill workload is significantly reduced and measure the resulting output tokens/sec. As a result, we achieved ",(0,s.jsx)(n.strong,{children:"~18,000 tokens/sec per decode node"}),"."]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Input sequence length"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Output sequence length"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Dataset"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"(Concurrency, RPS)"}),(0,s.jsx)(n.th,{style:{textAlign:"right"},children:"Output tokens/sec"}),(0,s.jsx)(n.th,{style:{textAlign:"right"},children:"Output tokens/sec per decode node"}),(0,s.jsx)(n.th,{style:{textAlign:"right"},children:"Mean ITL (ms)"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"2000"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"100"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Gutenberg"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"(10800, 1500)"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"53,776.62"}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:(0,s.jsx)(n.strong,{children:"17,925.54"})}),(0,s.jsx)(n.td,{style:{textAlign:"right"},children:"191.71"})]})})]}),"\n",(0,s.jsx)(n.p,{children:"Click to view the raw benchmarking log."}),"\n",(0,s.jsx)(n.p,{children:"==- (2000, 100, Gutenberg)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"'=============Serving Benchmark Result=============\nNumber of worker processes:              30\nSuccessful requests:                     185577\nMaximum request concurrency:             10800\nRequest rate configured (RPS):           1500.00\nWarm-up Time:                            30.0\nCool-down Time:                          20.0\nBenchmark duration (s):                  367.68\nTotal input tokens:                      371154000\nTotal generated tokens:                  19772555\nOutput token throughput (tok/s):         53776.62\nTotal Token throughput (tok/s):          1063226.66\n---------------Time to First Token----------------\nMean TTFT (ms):                          1079.36\nMedian TTFT (ms):                        979.51\nP10 TTFT (ms):                           832.05\nP25 TTFT (ms):                           897.71\nP50 TTFT (ms):                           979.51\nP75 TTFT (ms):                           1079.35\nP90 TTFT (ms):                           1223.28\n---------------Inter-token Latency----------------\nMean ITL (ms):                           191.71\nMedian ITL (ms):                         181.35\nP10 ITL (ms):                            166.87\nP25 ITL (ms):                            175.00\nP50 ITL (ms):                            181.35\nP75 ITL (ms):                            191.88\nP90 ITL (ms):                            259.46\n==================================================\n"})}),"\n",(0,s.jsx)(n.p,{children:"==="}),"\n",(0,s.jsxs)(n.p,{children:["We have also measured the performance of a decoding-only execution under the same configuration (ISL=2,000, OSL=100) and reported the results in a ",(0,s.jsx)(n.a,{href:"https://moreh.io/technical-report/21k-output-tokens-per-second-deepseek-inference-on-amd-instinct-mi300x-gpus-with-expert-parallelism-251113/",children:"technical report"}),". The maximum throughput achieved in this setting was 21,224 tokens/sec per decode node. This indicates that, in an end-to-end environment, MoAI Inference Framework is able to achieve ",(0,s.jsx)(n.strong,{children:"~85% of the peak decode performance"}),"."]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},7227(e,n,t){t.d(n,{A:()=>a});t(6540);var r=t(4164);const s="tabItem_Ymn6";var i=t(4848);function a(e){var n=e.children,t=e.hidden,a=e.className;return(0,i.jsx)("div",{role:"tabpanel",className:(0,r.A)(s,a),hidden:t,children:n})}},9489(e,n,t){t.d(n,{A:()=>_});var r=t(6540),s=t(4164),i=t(8630),a=t(4245),l=t(6347),o=t(6494),c=t(2814),d=t(5167),u=t(9900);function h(e){var n,t;return null!=(n=null==(t=r.Children.toArray(e).filter(function(e){return"\n"!==e}).map(function(e){if(!e||(0,r.isValidElement)(e)&&((n=e.props)&&"object"==typeof n&&"value"in n))return e;var n;throw new Error("Docusaurus error: Bad <Tabs> child <"+("string"==typeof e.type?e.type:e.type.name)+'>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.')}))?void 0:t.filter(Boolean))?n:[]}function m(e){var n=e.values,t=e.children;return(0,r.useMemo)(function(){var e=null!=n?n:function(e){return h(e).map(function(e){var n=e.props;return{value:n.value,label:n.label,attributes:n.attributes,default:n.default}})}(t);return function(e){var n=(0,d.XI)(e,function(e,n){return e.value===n.value});if(n.length>0)throw new Error('Docusaurus error: Duplicate values "'+n.map(function(e){return e.value}).join(", ")+'" found in <Tabs>. Every value needs to be unique.')}(e),e},[n,t])}function p(e){var n=e.value;return e.tabValues.some(function(e){return e.value===n})}function g(e){var n=e.queryString,t=void 0!==n&&n,s=e.groupId,i=(0,l.W6)(),a=function(e){var n=e.queryString,t=void 0!==n&&n,r=e.groupId;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!r)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return null!=r?r:null}({queryString:t,groupId:s});return[(0,c.aZ)(a),(0,r.useCallback)(function(e){if(a){var n=new URLSearchParams(i.location.search);n.set(a,e),i.replace(Object.assign({},i.location,{search:n.toString()}))}},[a,i])]}function x(e){var n,t,s,i,a=e.defaultValue,l=e.queryString,c=void 0!==l&&l,d=e.groupId,h=m(e),x=(0,r.useState)(function(){return function(e){var n,t=e.defaultValue,r=e.tabValues;if(0===r.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!p({value:t,tabValues:r}))throw new Error('Docusaurus error: The <Tabs> has a defaultValue "'+t+'" but none of its children has the corresponding value. Available values are: '+r.map(function(e){return e.value}).join(", ")+". If you intend to show no default tab, use defaultValue={null} instead.");return t}var s=null!=(n=r.find(function(e){return e.default}))?n:r[0];if(!s)throw new Error("Unexpected error: 0 tabValues");return s.value}({defaultValue:a,tabValues:h})}),f=x[0],T=x[1],v=g({queryString:c,groupId:d}),j=v[0],y=v[1],k=(n=function(e){return e?"docusaurus.tab."+e:null}({groupId:d}.groupId),t=(0,u.Dv)(n),s=t[0],i=t[1],[s,(0,r.useCallback)(function(e){n&&i.set(e)},[n,i])]),b=k[0],_=k[1],P=function(){var e=null!=j?j:b;return p({value:e,tabValues:h})?e:null}();return(0,o.A)(function(){P&&T(P)},[P]),{selectedValue:f,selectValue:(0,r.useCallback)(function(e){if(!p({value:e,tabValues:h}))throw new Error("Can't select invalid tab value="+e);T(e),y(e),_(e)},[y,_,h]),tabValues:h}}var f=t(1062);const T="tabList__CuJ",v="tabItem_LNqP";var j=t(4848);function y(e){var n=e.className,t=e.block,r=e.selectedValue,i=e.selectValue,l=e.tabValues,o=[],c=(0,a.a_)().blockElementScrollPositionUntilNextRender,d=function(e){var n=e.currentTarget,t=o.indexOf(n),s=l[t].value;s!==r&&(c(n),i(s))},u=function(e){var n,t=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":var r,s=o.indexOf(e.currentTarget)+1;t=null!=(r=o[s])?r:o[0];break;case"ArrowLeft":var i,a=o.indexOf(e.currentTarget)-1;t=null!=(i=o[a])?i:o[o.length-1]}null==(n=t)||n.focus()};return(0,j.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":t},n),children:l.map(function(e){var n=e.value,t=e.label,i=e.attributes;return(0,j.jsx)("li",Object.assign({role:"tab",tabIndex:r===n?0:-1,"aria-selected":r===n,ref:function(e){o.push(e)},onKeyDown:u,onClick:d},i,{className:(0,s.A)("tabs__item",v,null==i?void 0:i.className,{"tabs__item--active":r===n}),children:null!=t?t:n}),n)})})}function k(e){var n=e.lazy,t=e.children,i=e.selectedValue,a=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){var l=a.find(function(e){return e.props.value===i});return l?(0,r.cloneElement)(l,{className:(0,s.A)("margin-top--md",l.props.className)}):null}return(0,j.jsx)("div",{className:"margin-top--md",children:a.map(function(e,n){return(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==i})})})}function b(e){var n=x(e);return(0,j.jsxs)("div",{className:(0,s.A)(i.G.tabs.container,"tabs-container",T),children:[(0,j.jsx)(y,Object.assign({},n,e)),(0,j.jsx)(k,Object.assign({},n,e))]})}function _(e){var n=(0,f.A)();return(0,j.jsx)(b,Object.assign({},e,{children:h(e.children)}),String(n))}},8453(e,n,t){t.d(n,{R:()=>a,x:()=>l});var r=t(6540);const s={},i=r.createContext(s);function a(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);