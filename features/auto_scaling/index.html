<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-v0.0.0 docs-doc-page docs-doc-id-features/auto_scaling" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Auto-scaling | Moreh</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://test-docs.moreh.io/features/auto_scaling/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="v0.0.0"><meta data-rh="true" name="docusaurus_tag" content="docs-default-v0.0.0"><meta data-rh="true" name="docsearch:version" content="v0.0.0"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-v0.0.0"><meta data-rh="true" property="og:title" content="Auto-scaling | Moreh"><meta data-rh="true" name="description" content="The model inference endpoints provided by the MoAI Inference Framework are often just one of many functions running on the overall AI compute infrastructure. Therefore, it is essential to allocate the appropriate amount of GPU resources (to run the appropriate number of Pods) so that GPUs are not under-utilized while still handling all incoming traffic and meeting the defined service level objectives (SLOs)."><meta data-rh="true" property="og:description" content="The model inference endpoints provided by the MoAI Inference Framework are often just one of many functions running on the overall AI compute infrastructure. Therefore, it is essential to allocate the appropriate amount of GPU resources (to run the appropriate number of Pods) so that GPUs are not under-utilized while still handling all incoming traffic and meeting the defined service level objectives (SLOs)."><link data-rh="true" rel="icon" href="/moreh-icon.png"><link data-rh="true" rel="canonical" href="https://test-docs.moreh.io/features/auto_scaling/"><link data-rh="true" rel="alternate" href="https://test-docs.moreh.io/features/auto_scaling/" hreflang="en"><link data-rh="true" rel="alternate" href="https://test-docs.moreh.io/features/auto_scaling/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Features","item":"https://test-docs.moreh.io/features"},{"@type":"ListItem","position":2,"name":"Auto-scaling","item":"https://test-docs.moreh.io/features/auto_scaling"}]}</script><link rel="stylesheet" href="/assets/css/styles.596da9e5.css">
<script src="/assets/js/runtime~main.ee5c6e29.js" defer="defer"></script>
<script src="/assets/js/main.7dc2dd5f.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/moreh-logo.svg" alt="Moreh logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/moreh-logo-white.svg" alt="Moreh logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a aria-current="page" class="navbar__link active" aria-haspopup="true" aria-expanded="false" role="button" href="/features/auto_scaling/">v0.0.0</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/dev/reference/heimdall/api-reference/">Dev ðŸš§</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/features/auto_scaling/">v0.0.0</a></li></ul></div><a href="https://moreh.io/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Website<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><a href="https://github.com/moreh-dev/mif" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="dsla-search-wrapper"><div class="dsla-search-field" data-tags="default,docs-default-v0.0.0"></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/"><span title="Home" class="linkLabel_WmDU">Home</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/getting-started/"><span title="Getting Started" class="categoryLinkLabel_W154">Getting Started</span></a><button aria-label="Collapse sidebar category &#x27;Getting Started&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/getting_started/overview/"><span title="Overview" class="linkLabel_WmDU">Overview</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/getting_started/prerequisites/"><span title="Prerequisites" class="linkLabel_WmDU">Prerequisites</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/getting_started/quickstart/"><span title="Quickstart" class="linkLabel_WmDU">Quickstart</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/getting_started/monitoring/"><span title="Monitoring" class="linkLabel_WmDU">Monitoring</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/getting_started/supported_devices/"><span title="Supported devices" class="linkLabel_WmDU">Supported devices</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/benchmarking/"><span title="Benchmarking" class="categoryLinkLabel_W154">Benchmarking</span></a><button aria-label="Collapse sidebar category &#x27;Benchmarking&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/benchmarking/deepseek_r1_671b_on_amd_mi300x_gpus_maximum_throughput/"><span title="DeepSeek R1 671B on AMD MI300X GPUs: Maximum throughput" class="linkLabel_WmDU">DeepSeek R1 671B on AMD MI300X GPUs: Maximum throughput</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" tabindex="0" href="/benchmarking/more_benchmarking_for_deepseek_r1_671b_on_amd_mi300x_gpus/performance_with_prefix_cache_and_load_aware_routing/"><span title="More benchmarking for DeepSeek R1 671B on AMD MI300X GPUs" class="categoryLinkLabel_W154">More benchmarking for DeepSeek R1 671B on AMD MI300X GPUs</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/benchmarking/more_benchmarking_for_deepseek_r1_671b_on_amd_mi300x_gpus/performance_with_prefix_cache_and_load_aware_routing/"><span title="Performance with prefix cache- and load-aware routing" class="linkLabel_WmDU">Performance with prefix cache- and load-aware routing</span></a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/features/"><span title="Features" class="categoryLinkLabel_W154">Features</span></a><button aria-label="Collapse sidebar category &#x27;Features&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/features/preset/"><span title="Presets" class="linkLabel_WmDU">Presets</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/features/prefill_decode_disaggregation/"><span title="Prefill-decode disaggregation" class="linkLabel_WmDU">Prefill-decode disaggregation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/features/expert_parallelism/"><span title="Expert parallelism" class="linkLabel_WmDU">Expert parallelism</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/features/prefix_cache_aware_routing/"><span title="Prefix cache-aware routing" class="linkLabel_WmDU">Prefix cache-aware routing</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/features/load_aware_routing/"><span title="Load-aware routing" class="linkLabel_WmDU">Load-aware routing</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/features/auto_scaling/"><span title="Auto-scaling" class="linkLabel_WmDU">Auto-scaling</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/best-practices/"><span title="Best practices" class="categoryLinkLabel_W154">Best practices</span></a><button aria-label="Collapse sidebar category &#x27;Best practices&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/best_practices/resource_allocation/"><span title="Resource allocation" class="linkLabel_WmDU">Resource allocation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/best_practices/hf_model_management_with_pv/"><span title="Hugging Face model management with persistent volume" class="linkLabel_WmDU">Hugging Face model management with persistent volume</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/reference/"><span title="Reference" class="categoryLinkLabel_W154">Reference</span></a><button aria-label="Collapse sidebar category &#x27;Reference&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/reference/heimdall_scheduler/"><span title="Heimdall scheduler" class="linkLabel_WmDU">Heimdall scheduler</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/reference/odin_inference_service/"><span title="Odin inference service" class="linkLabel_WmDU">Odin inference service</span></a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/features/"><span>Features</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Auto-scaling</span></li></ul></nav><span class="theme-doc-version-badge badge badge--secondary">Version: v0.0.0</span><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Auto-scaling</h1></header>
<p>The model inference endpoints provided by the MoAI Inference Framework are often just one of many functions running on the overall AI compute infrastructure. Therefore, it is essential to allocate the appropriate amount of GPU resources (to run the appropriate number of Pods) so that GPUs are not under-utilized while still handling all incoming traffic and meeting the defined service level objectives (SLOs).</p>
<p>This is where auto-scaling comes into play. Instead of allocating all GPU resources from the start, the system begins with a small number of Pods, and adds more only when traffic increases or SLOs are at risk. Additionally, if traffic decreases, the number of Pods is reduced accordingly. It is also necessary to adjust not only the total number of Pods but also the number of Pods assigned to each disaggregated part (prefill, decode, a set of experts, etc.).</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="key-features">Key features<a href="#key-features" class="hash-link" aria-label="Direct link to Key features" title="Direct link to Key features" translate="no">â€‹</a></h2>
<ul>
<li class="">The framework can dynamically adjust the number of GPU resources (the number of Pods) according to the given SLOs and the current amount of traffic.</li>
<li class="">Users can manually configure auto-scaling rules using <a href="https://keda.sh/" target="_blank" rel="noopener noreferrer" class="">KEDA</a>.</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="manual-configuration-of-auto-scaling-rules">Manual configuration of auto-scaling rules<a href="#manual-configuration-of-auto-scaling-rules" class="hash-link" aria-label="Direct link to Manual configuration of auto-scaling rules" title="Direct link to Manual configuration of auto-scaling rules" translate="no">â€‹</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="installing-keda">Installing KEDA<a href="#installing-keda" class="hash-link" aria-label="Direct link to Installing KEDA" title="Direct link to Installing KEDA" translate="no">â€‹</a></h3>
<p>You can install KEDA as follows. See <a href="https://keda.sh/docs/deploy/" target="_blank" rel="noopener noreferrer" class="">KEDA / Deploying KEDA</a> for more details.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">helm repo add kedacore https://kedacore.github.io/charts</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">helm repo update kedacore</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">helm upgrade -i keda kedacore/keda \</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    --version 2.18.0 \</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    -n keda \</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    --create-namespace</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="enabling-auto-scaling-in-the-odin-inference-service">Enabling auto-scaling in the Odin inference service<a href="#enabling-auto-scaling-in-the-odin-inference-service" class="hash-link" aria-label="Direct link to Enabling auto-scaling in the Odin inference service" title="Direct link to Enabling auto-scaling in the Odin inference service" translate="no">â€‹</a></h3>
<p>To enable auto-scaling, set the <code>autoscaling.enabled</code> to <code>true</code> under each profile (<code>decode</code> and <code>prefill</code>) of the <strong>Odin</strong> inference service. If this is set to <code>false</code>, the number of replicas remains fixed as specified in the <code>replicas</code> field. However, if it is set to <code>true</code>, the number of replicas dynamically changes between <code>minReplicaCount</code> and <code>maxReplicaCount</code>. The following is an example of configuring auto-scaling for the prefill phase (and not for the decode phase).</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token punctuation" style="color:rgb(199, 146, 234)">...</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token key atrule">decode</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">replicas</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">4</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">autoscaling</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token key atrule">enabled</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token boolean important" style="color:rgb(255, 88, 116)">false</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token punctuation" style="color:rgb(199, 146, 234)">...</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token key atrule">prefill</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">autoscaling</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token key atrule">enabled</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token boolean important" style="color:rgb(255, 88, 116)">true</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token key atrule">minReplicaCount</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token key atrule">maxReplicaCount</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">6</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token key atrule">behavior</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">      </span><span class="token key atrule">scaleUp</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token key atrule">stabilizationWindowSeconds</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">100</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token key atrule">policies</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">          </span><span class="token punctuation" style="color:rgb(199, 146, 234)">-</span><span class="token plain"> </span><span class="token key atrule">type</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> Pods</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token key atrule">value</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token key atrule">periodSeconds</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">200</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">      </span><span class="token key atrule">scaleDown</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token key atrule">stabilizationWindowSeconds</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">200</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token key atrule">policies</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">          </span><span class="token punctuation" style="color:rgb(199, 146, 234)">-</span><span class="token plain"> </span><span class="token key atrule">type</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> Pods</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token key atrule">value</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token key atrule">periodSeconds</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">60</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token key atrule">triggers</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(199, 146, 234)">-</span><span class="token plain"> </span><span class="token key atrule">type</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> prometheus</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token key atrule">metricType</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> Value</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token key atrule">metadata</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">          </span><span class="token key atrule">serverAddress</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> http</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain">//prometheus</span><span class="token punctuation" style="color:rgb(199, 146, 234)">-</span><span class="token plain">operated.prometheus</span><span class="token punctuation" style="color:rgb(199, 146, 234)">-</span><span class="token plain">stack</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token number" style="color:rgb(247, 140, 108)">9090</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">          </span><span class="token key atrule">query</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> histogram_quantile(0.9</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> sum by(le) (rate(llm_time_to_first_token_seconds_bucket</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain">job=&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"> include &quot;common.names.namespace&quot; . </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token plain">/</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"> include &quot;inferenceService.prefill.fullname&quot; . </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token plain">&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token plain">2m</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain">)))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">          </span><span class="token key atrule">threshold</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;1&quot;</span><span class="token plain">  </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Scale up if the P90 TTFT exceeds 1 second</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token punctuation" style="color:rgb(199, 146, 234)">...</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="configuration-parameters">Configuration parameters<a href="#configuration-parameters" class="hash-link" aria-label="Direct link to Configuration parameters" title="Direct link to Configuration parameters" translate="no">â€‹</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="scaling-behavior">Scaling behavior<a href="#scaling-behavior" class="hash-link" aria-label="Direct link to Scaling behavior" title="Direct link to Scaling behavior" translate="no">â€‹</a></h4>
<p>The <code>behavior.scaleUp</code> and <code>behavior.scaleDown</code> section control how fast the system scales up and down, respectively. For more details, see the <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-autoscaling/853-configurable-hpa-scale-velocity/README.md" target="_blank" rel="noopener noreferrer" class="">Kubernetes HPA Scale Velocity KEP</a>.</p>
<ul>
<li class=""><code>stabilizationWindowSeconds</code>: specifies the duration of the time window the autoscaler considers when determining the target replica count.<!-- -->
<ul>
<li class="">When scaling up, the system picks the <strong>smallest</strong> replica count recommended during the window.<!-- -->
<ul>
<li class="">Set to 0 to respond immediately to load increases.</li>
<li class="">Use a non-zero value (e.g., 300 seconds) to prevent rapid scaling due to temporary traffic spikes.</li>
</ul>
</li>
<li class="">When scaling down, the system picks the <strong>largest</strong> replica count recommended during the window.<!-- -->
<ul>
<li class="">Set to 0 for immediate scale-down (not recommended for production).</li>
<li class="">Use a non-zero value (e.g., 300 seconds) to prevent rapid scaling due to temporary traffic drops.</li>
</ul>
</li>
</ul>
</li>
<li class=""><code>policies</code>: define the maximum rate at which replicas can be added or removed. Each item includes the following fields.<!-- -->
<ul>
<li class=""><code>type</code>: either <code>Pods</code> (absolute replica count) or <code>Percent</code> (percentage relative to the current replica count).</li>
<li class=""><code>value</code>: maximum number of pods or percentage that can be added or removed.</li>
<li class=""><code>periodSeconds</code>: the time period over which the policy applies.</li>
</ul>
</li>
<li class=""><code>selectPolicy</code>: defines how the system determines which policy to apply when multiple policies are specified.<!-- -->
<ul>
<li class="">Set to <code>Max</code> (default) to select the policy that allows the maximum change.</li>
<li class="">Set to <code>Min</code> to select the policy that allows the minimum change.</li>
<li class="">Set to <code>Disabled</code> to selectively disable scaling up or scaling down.</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="prometheus-metric-triggers">Prometheus metric triggers<a href="#prometheus-metric-triggers" class="hash-link" aria-label="Direct link to Prometheus metric triggers" title="Direct link to Prometheus metric triggers" translate="no">â€‹</a></h4>
<p>The <code>triggers</code> section defines the metrics that will be monitored to determine when to scale. Specifically, the MoAI Inference Framework uses KEDA&#x27;s Prometheus scaler. For more details, see <a href="https://keda.sh/docs/scalers/prometheus/" target="_blank" rel="noopener noreferrer" class="">KEDA / Prometheus Scaler</a>. Each trigger has the following fields.</p>
<ul>
<li class=""><code>type</code>: <code>prometheus</code></li>
<li class=""><code>metricType</code>: specifies how the metric value is interpreted. It can be either <code>Value</code> or <code>AverageValue</code>.<!-- -->
<ul>
<li class="">Set to <code>Value</code> to adjust the replica count so that <code>currentMetricValue</code> equals <code>threshold</code>.<!-- -->
<ul>
<li class=""><code>desiredReplicaCount = currentReplicaCount * (currentMetricValue / threshold)</code></li>
</ul>
</li>
<li class="">Set to <code>AverageValue</code> to set the number of replicas to <code>currentMetricValue / threshold</code>.<!-- -->
<ul>
<li class=""><code>desiredReplicaCount = currentMetricValue / threshold</code></li>
</ul>
</li>
</ul>
</li>
<li class=""><code>metadata</code>: defines the metric value and the threshold.<!-- -->
<ul>
<li class=""><code>serverAddress</code>: Prometheus server endpoint.</li>
<li class=""><code>query</code>: a PromQL query to calculate the metric value.</li>
<li class=""><code>threshold</code>: the target value that triggers scaling.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="available-metrics-for-auto-scaling">Available metrics for auto-scaling<a href="#available-metrics-for-auto-scaling" class="hash-link" aria-label="Direct link to Available metrics for auto-scaling" title="Direct link to Available metrics for auto-scaling" translate="no">â€‹</a></h3>
<p>The following metrics can be used to configure auto-scaling triggers. These metrics are originally exposed by vLLM with the <code>vllm:</code> prefix, but are relabeled with the <code>llm_</code> prefix in Prometheus through the PodMonitor configuration (for example, <code>vllm:time_to_first_token_seconds</code> becomes <code>llm_time_to_first_token_seconds</code>) to maintain compatibility with other inference engines such as SGLang.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="latency-metrics">Latency metrics<a href="#latency-metrics" class="hash-link" aria-label="Direct link to Latency metrics" title="Direct link to Latency metrics" translate="no">â€‹</a></h4>
<ul>
<li class=""><code>llm_time_to_first_token_seconds_bucket</code>: a histogram of time to first token (TTFT) in seconds, used to scale based on how quickly users receive the first token.<!-- -->
<ul>
<li class="">Example: scale up when the P90 TTFT exceeds 1 second.</li>
</ul>
</li>
</ul>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token key atrule">metricType</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> Value</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token key atrule">metadata</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">query</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> histogram_quantile(0.9</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> sum by(le) (rate(llm_time_to_first_token_seconds_bucket</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain">job=&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"> include &quot;common.names.namespace&quot; . </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token plain">/</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"> include &quot;inferenceService.prefill.fullname&quot; . </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token plain">&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token plain">2m</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain">)))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">threshold</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;1&#x27;</span><br></span></code></pre></div></div>
<ul>
<li class=""><code>llm_inter_token_latency_seconds_bucket</code>: a histogram of inter-token latency (ITL) in seconds, used to scale based on token generation speed.<!-- -->
<ul>
<li class="">Example: scale up when the P90 ITL exceeds 200 ms.</li>
</ul>
</li>
</ul>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token key atrule">metricType</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> Value</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token key atrule">metadata</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">query</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> histogram_quantile(0.9</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> sum by(le) (rate(llm_inter_token_latency_seconds_bucket</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain">job=&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"> include &quot;common.names.namespace&quot; . </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token plain">/</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"> include &quot;inferenceService.prefill.fullname&quot; . </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token plain">&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token plain">2m</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain">)))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">threshold</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;0.2&#x27;</span><br></span></code></pre></div></div>
<ul>
<li class=""><code>llm_e2e_request_latency_seconds_bucket</code>: a histogram of end-to-end request latency (E2EL) in seconds, used to scale based on total request processing time.<!-- -->
<ul>
<li class="">Example: scale up when the P95 E2EL exceeds 5 seconds.</li>
</ul>
</li>
</ul>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token key atrule">metricType</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> Value</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token key atrule">metadata</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">query</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> histogram_quantile(0.95</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> sum by(le) (rate(llm_e2e_request_latency_seconds_bucket</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain">job=&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"> include &quot;common.names.namespace&quot; . </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token plain">/</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"> include &quot;inferenceService.prefill.fullname&quot; . </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token plain">&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token plain">2m</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain">)))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">threshold</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;5&#x27;</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="queue-and-load-related-metrics">Queue and load-related metrics<a href="#queue-and-load-related-metrics" class="hash-link" aria-label="Direct link to Queue and load-related metrics" title="Direct link to Queue and load-related metrics" translate="no">â€‹</a></h4>
<ul>
<li class=""><code>llm_num_requests_waiting</code>: the number of requests waiting to be processed, used to scale up when too many requests are queued.</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="resource-related-metrics">Resource-related metrics<a href="#resource-related-metrics" class="hash-link" aria-label="Direct link to Resource-related metrics" title="Direct link to Resource-related metrics" translate="no">â€‹</a></h4>
<ul>
<li class=""><code>llm_kv_cache_usage_perc</code>: the KV cache usage (1 = 100% utilization), used to scale up when the cache is nearly full.</li>
</ul></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/features/load_aware_routing/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Load-aware routing</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/best-practices/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Best practices</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#key-features" class="table-of-contents__link toc-highlight">Key features</a></li><li><a href="#manual-configuration-of-auto-scaling-rules" class="table-of-contents__link toc-highlight">Manual configuration of auto-scaling rules</a><ul><li><a href="#installing-keda" class="table-of-contents__link toc-highlight">Installing KEDA</a></li><li><a href="#enabling-auto-scaling-in-the-odin-inference-service" class="table-of-contents__link toc-highlight">Enabling auto-scaling in the Odin inference service</a></li><li><a href="#configuration-parameters" class="table-of-contents__link toc-highlight">Configuration parameters</a></li><li><a href="#available-metrics-for-auto-scaling" class="table-of-contents__link toc-highlight">Available metrics for auto-scaling</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Â© Copyright 2026 Moreh, Inc. All rights reserved.</div></div></div></footer></div>
</body>
</html>